<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="color-scheme" content="light" />
    <title>FrankenSQLite Spec Evolution</title>

    <!-- OpenGraph -->
    <meta property="og:title" content="FrankenSQLite Spec Evolution" />
    <meta property="og:description" content="Interactive visualization of 137 commits across 12 deep sessions building a 10,791-line comprehensive database specification. MVCC page-level versioning, RaptorQ erasure coding, SSI concurrency control." />
    <meta property="og:image" content="https://frankensqlite.com/og-image.png" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta property="og:image:type" content="image/png" />
    <meta property="og:url" content="https://frankensqlite.com" />
    <meta property="og:type" content="website" />
    <meta property="og:site_name" content="FrankenSQLite" />

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="FrankenSQLite Spec Evolution" />
    <meta name="twitter:description" content="137 commits · 10,791 lines · 12 sessions of deep architectural design for a clean-room Rust reimplementation of SQLite with MVCC." />
    <meta name="twitter:image" content="https://frankensqlite.com/twitter-image.png" />

    <!-- General meta -->
    <meta name="description" content="Interactive visualization tracking the evolution of the FrankenSQLite specification across 137 commits — from initial architecture through MVCC, RaptorQ erasure coding, and SSI concurrency control." />

    <!-- Typography -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Fraunces:opsz,wght@9..144,350..900&family=Manrope:wght@200..800&display=swap"
      rel="stylesheet"
    />

    <!-- Tailwind (CDN) -->
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
      tailwind.config = {
        theme: {
          extend: {
            fontFamily: {
              display: ["Fraunces", "ui-serif", "Georgia", "serif"],
              sans: ["Manrope", "ui-sans-serif", "system-ui", "sans-serif"],
              mono: [
                "ui-monospace",
                "SFMono-Regular",
                "SF Mono",
                "Menlo",
                "Monaco",
                "Consolas",
                "Liberation Mono",
                "Courier New",
                "monospace",
              ],
            },
            boxShadow: {
              glow: "0 0 0 1px rgba(15, 23, 42, 0.06), 0 10px 30px rgba(2, 6, 23, 0.10)",
              glowLg:
                "0 0 0 1px rgba(15, 23, 42, 0.06), 0 24px 60px rgba(2, 6, 23, 0.16)",
            },
          },
        },
      };
    </script>

    <!-- Code highlighting -->
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css"
    />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/diff2html/bundles/css/diff2html.min.css"
    />

    <style>
      :root {
        /* Surfaces */
        --bg0: #fbfbfe;
        --bg1: #f6f7ff;
        --card: rgba(255, 255, 255, 0.82);
        --card2: rgba(255, 255, 255, 0.7);
        --stroke: rgba(2, 6, 23, 0.10);
        --stroke2: rgba(2, 6, 23, 0.07);

        /* Ink */
        --ink0: #0b1220;
        --ink1: rgba(2, 6, 23, 0.80);
        --ink2: rgba(2, 6, 23, 0.62);
        --ink3: rgba(2, 6, 23, 0.46);

        /* Brand-ish gradient */
        --a0: rgba(37, 99, 235, 0.14);
        --a1: rgba(219, 39, 119, 0.10);
        --a2: rgba(20, 184, 166, 0.10);
        --a3: rgba(217, 119, 6, 0.10);

        /* Category colors (10 buckets) */
        --c1: #2563eb; /* logic/math */
        --c2: #d97706; /* sqlite legacy */
        --c3: #059669; /* asupersync */
        --c4: #dc2626; /* architecture mistakes */
        --c5: #64748b; /* scrivening */
        --c6: #db2777; /* background/context */
        --c7: #0ea5e9; /* standard eng perf */
        --c8: #7c3aed; /* alien math */
        --c9: #16a34a; /* clarification */
        --c10: #0f172a; /* other */

        --ring: rgba(37, 99, 235, 0.35);
      }

      html,
      body {
        height: 100%;
      }

      body {
        font-family: Manrope, system-ui, -apple-system, Segoe UI, sans-serif;
        color: var(--ink0);
        background: radial-gradient(1200px 700px at 10% -10%, var(--a0), transparent 60%),
          radial-gradient(900px 600px at 95% 5%, var(--a1), transparent 58%),
          radial-gradient(1100px 700px at 40% 110%, var(--a2), transparent 60%),
          radial-gradient(1100px 650px at 100% 85%, var(--a3), transparent 56%),
          linear-gradient(180deg, var(--bg0), var(--bg1));
        background-attachment: fixed;
      }

      /* Subtle noise */
      body::before {
        content: "";
        position: fixed;
        inset: 0;
        pointer-events: none;
        opacity: 0.035;
        mix-blend-mode: multiply;
        background-image: url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='320' height='320'%3E%3Cfilter id='n'%3E%3CfeTurbulence type='fractalNoise' baseFrequency='.9' numOctaves='3' stitchTiles='stitch'/%3E%3C/filter%3E%3Crect width='320' height='320' filter='url(%23n)' opacity='.35'/%3E%3C/svg%3E");
      }

      .glass {
        background: var(--card);
        border: 1px solid var(--stroke);
        backdrop-filter: blur(14px);
        -webkit-backdrop-filter: blur(14px);
      }

      .glass-2 {
        background: var(--card2);
        border: 1px solid var(--stroke2);
        backdrop-filter: blur(14px);
        -webkit-backdrop-filter: blur(14px);
      }

      .focus-ring:focus {
        outline: none;
        box-shadow: 0 0 0 4px var(--ring);
      }

      @media (prefers-reduced-motion: reduce) {
        * {
          animation: none !important;
          transition: none !important;
          scroll-behavior: auto !important;
        }
      }

      .enter {
        animation: enter 700ms cubic-bezier(0.2, 1, 0.2, 1) both;
      }
      @keyframes enter {
        from {
          opacity: 0;
          transform: translateY(10px);
        }
        to {
          opacity: 1;
          transform: translateY(0);
        }
      }

      .chip {
        border: 1px solid var(--stroke2);
        background: rgba(255, 255, 255, 0.7);
      }

      .codebox {
        background: rgba(15, 23, 42, 0.03);
        border: 1px solid rgba(2, 6, 23, 0.10);
      }

      .mono {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono",
          "Courier New", monospace;
      }

      /* Mobile bottom sheet */
      .sheet {
        transform: translateY(12px);
        opacity: 0;
        transition: transform 200ms ease, opacity 200ms ease;
        will-change: transform, opacity;
      }
      .sheet.open {
        transform: translateY(0);
        opacity: 1;
      }

      /* ECharts container should not shrink weirdly */
      .chart {
        min-height: 280px;
        contain: content;
      }

      @media (max-width: 639px) {
        .glass,
        .glass-2 {
          backdrop-filter: blur(8px);
          -webkit-backdrop-filter: blur(8px);
        }
        .chart {
          min-height: 320px;
        }
        #docMain:not(.hidden) {
          display: flex;
          flex-direction: column;
        }
        #docMain > aside {
          order: -1;
        }
      }

      #commitList {
        contain: content;
      }

      /* Section summary heading highlight */
      @keyframes section-flash {
        0% { background-color: rgba(59, 130, 246, 0.25); }
        100% { background-color: transparent; }
      }
      .section-highlight {
        animation: section-flash 1.2s ease-out;
      }

      /* Section summary table */
      #sectionTable th {
        cursor: pointer;
        user-select: none;
      }
      #sectionTable th:hover {
        color: var(--ink0);
      }
      #sectionTable .sparkline-bar {
        display: inline-block;
        min-width: 2px;
        border-radius: 1px;
        vertical-align: middle;
      }

      /* Markdown rendering (hand-rolled; avoids tailwind/typography plugin dependency) */
      .md {
        color: rgba(2, 6, 23, 0.82);
        font-size: 15px;
        line-height: 1.75;
      }
      .md h1,
      .md h2,
      .md h3,
      .md h4 {
        font-family: Fraunces, ui-serif, Georgia, serif;
        color: rgba(2, 6, 23, 0.92);
        letter-spacing: -0.01em;
      }
      .md h1 {
        font-size: 28px;
        line-height: 1.15;
        margin: 18px 0 10px;
      }
      .md h2 {
        font-size: 22px;
        line-height: 1.2;
        margin: 18px 0 8px;
      }
      .md h3 {
        font-size: 18px;
        line-height: 1.25;
        margin: 14px 0 6px;
      }
      .md p {
        margin: 10px 0;
      }
      .md ul,
      .md ol {
        margin: 10px 0 10px 18px;
      }
      .md li {
        margin: 4px 0;
      }
      .md code {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono",
          "Courier New", monospace;
        font-size: 0.92em;
        padding: 0.12em 0.36em;
        border: 1px solid rgba(2, 6, 23, 0.10);
        border-radius: 10px;
        background: rgba(15, 23, 42, 0.04);
      }
      .md pre code {
        display: block;
        padding: 0;
        border: none;
        background: transparent;
        font-size: 12px;
        line-height: 1.6;
      }
      .md pre {
        margin: 12px 0;
        padding: 12px 14px;
        border-radius: 18px;
        overflow: auto;
        border: 1px solid rgba(2, 6, 23, 0.10);
        background: rgba(15, 23, 42, 0.03);
      }
      .md a {
        color: rgb(37, 99, 235);
        text-decoration: none;
      }
      .md a:hover {
        text-decoration: underline;
      }
      .md table {
        width: 100%;
        border-collapse: collapse;
        margin: 12px 0;
        font-size: 13px;
      }
      .md th,
      .md td {
        border: 1px solid rgba(2, 6, 23, 0.10);
        padding: 8px 10px;
        vertical-align: top;
      }
      .md th {
        background: rgba(15, 23, 42, 0.03);
        font-weight: 800;
      }
      .md blockquote {
        margin: 12px 0;
        padding: 10px 12px;
        border-left: 3px solid rgba(37, 99, 235, 0.35);
        background: rgba(37, 99, 235, 0.06);
        border-radius: 14px;
      }

      /* Bottom timeline dock */
      .dock {
        background: rgba(255, 255, 255, 0.78);
        border-top: 1px solid rgba(2, 6, 23, 0.10);
        backdrop-filter: blur(16px);
        -webkit-backdrop-filter: blur(16px);
      }
      .dock-canvas {
        width: 100%;
        height: 40px;
        display: block;
      }
      .dock-slider {
        width: 100%;
      }

      /* A/B Compare Typeahead Pickers (bd-24q.1) */
      .ab-picker {
        position: relative;
        display: inline-flex;
      }
      .ab-picker-btn {
        display: inline-flex;
        align-items: center;
        gap: 4px;
        padding: 4px 10px;
        border: 1px solid rgba(59, 130, 246, 0.3);
        border-radius: 8px;
        background: white;
        font-size: 11px;
        font-weight: 600;
        color: #0f172a;
        cursor: pointer;
        max-width: 240px;
        overflow: hidden;
        text-overflow: ellipsis;
        white-space: nowrap;
        transition: border-color 150ms, box-shadow 150ms;
      }
      .ab-picker-btn:hover { border-color: rgba(59, 130, 246, 0.5); }
      .ab-picker-btn:focus { outline: none; box-shadow: 0 0 0 3px rgba(59, 130, 246, 0.25); }
      .ab-picker-dropdown {
        position: absolute;
        top: 100%;
        left: 0;
        z-index: 60;
        margin-top: 4px;
        width: 340px;
        max-height: 300px;
        border: 1px solid rgba(2, 6, 23, 0.12);
        border-radius: 14px;
        background: white;
        box-shadow: 0 10px 30px rgba(2, 6, 23, 0.16);
        display: flex;
        flex-direction: column;
        overflow: hidden;
      }
      .ab-picker-search {
        width: 100%;
        border: none;
        border-bottom: 1px solid rgba(2, 6, 23, 0.08);
        padding: 8px 12px;
        font-size: 12px;
        outline: none;
        background: transparent;
      }
      .ab-picker-search::placeholder { color: rgba(2, 6, 23, 0.4); }
      .ab-picker-list {
        flex: 1;
        overflow-y: auto;
        padding: 4px;
      }
      .ab-picker-item {
        display: flex;
        align-items: baseline;
        gap: 6px;
        padding: 6px 8px;
        border-radius: 8px;
        font-size: 11px;
        cursor: pointer;
        transition: background 100ms;
      }
      .ab-picker-item:hover,
      .ab-picker-item.keyboard-active { background: rgba(59, 130, 246, 0.08); }
      .ab-picker-item.selected { background: rgba(59, 130, 246, 0.12); font-weight: 700; }
      .ab-picker-item .idx {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace;
        color: rgba(2, 6, 23, 0.35);
        font-size: 10px;
        min-width: 22px;
        flex-shrink: 0;
      }
      .ab-picker-item .hash {
        font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace;
        color: rgba(2, 6, 23, 0.5);
        font-size: 10px;
        flex-shrink: 0;
      }
      .ab-picker-item .subject {
        color: #0f172a;
        overflow: hidden;
        text-overflow: ellipsis;
        white-space: nowrap;
        flex: 1;
        min-width: 0;
      }
      @media (max-width: 639px) {
        .ab-picker-dropdown {
          position: fixed;
          inset: 0;
          width: 100%;
          max-height: 100%;
          border-radius: 0;
          margin-top: 0;
          z-index: 100;
        }
        .ab-picker-search {
          padding: 14px 16px;
          font-size: 16px;
        }
        .ab-picker-item {
          padding: 12px 16px;
          font-size: 13px;
        }
        .ab-picker-item .idx { font-size: 11px; }
        .ab-picker-item .hash { font-size: 11px; }
        .ab-picker-dropdown::before {
          content: "";
          display: block;
          height: 4px;
          width: 36px;
          margin: 8px auto 4px;
          border-radius: 2px;
          background: rgba(2, 6, 23, 0.15);
        }
      }
    </style>
  </head>

  <body>
    <div class="mx-auto max-w-[1200px] px-4 pb-28 pt-10 sm:px-6 lg:px-8">
      <!-- Top -->
      <header class="enter">
        <div
          class="glass shadow-glow rounded-3xl px-5 py-6 sm:px-7 sm:py-7 lg:px-10 lg:py-10"
        >
          <div class="flex flex-col gap-6 lg:flex-row lg:items-end lg:justify-between">
            <div class="max-w-[70ch]">
              <div class="flex flex-wrap items-center gap-2">
                <span
                  class="chip inline-flex items-center gap-2 rounded-full px-3 py-1 text-xs font-semibold text-slate-700"
                >
                  <span
                    class="inline-block h-2 w-2 rounded-full"
                    style="background: linear-gradient(90deg, var(--c1), var(--c6))"
                  ></span>
                  Single-file visualization
                </span>
                <span
                  id="metaSpan"
                  class="chip inline-flex items-center rounded-full px-3 py-1 text-xs font-semibold text-slate-700"
                ></span>
              </div>

              <h1 class="mt-4 font-display text-4xl leading-[1.05] sm:text-5xl">
                Evolution of the FrankenSQLite Spec
              </h1>
              <p class="mt-4 text-base leading-relaxed text-slate-700 sm:text-lg">
                A commit-by-commit atlas of how
                <span class="mono">COMPREHENSIVE_SPEC_FOR_FRANKENSQLITE_V1.md</span>
                changed from inception. Every change is bucketed into 10 categories with
                multi-label support, confidence, and diff excerpts.
              </p>
            </div>

            <div class="flex flex-col gap-3 sm:flex-row sm:flex-wrap sm:items-center lg:justify-end">
              <button
                id="btnFilters"
                class="focus-ring inline-flex items-center justify-center gap-2 rounded-2xl border border-slate-900/10 bg-white/60 px-4 py-2.5 text-sm font-semibold text-slate-900 shadow-sm transition hover:bg-white"
                type="button"
              >
                Filters
              </button>
              <a
                id="btnOpenSpec"
                class="focus-ring inline-flex items-center justify-center gap-2 rounded-2xl border border-slate-900/10 bg-white/60 px-4 py-2.5 text-sm font-semibold text-slate-900 shadow-sm transition hover:bg-white"
                href="COMPREHENSIVE_SPEC_FOR_FRANKENSQLITE_V1.md"
              >
                Open Spec
              </a>
              <button
                id="btnGalaxy"
                class="focus-ring inline-flex items-center justify-center gap-2 rounded-2xl bg-slate-900 px-4 py-2.5 text-sm font-semibold text-white shadow-sm transition hover:bg-slate-800"
                type="button"
              >
                Galaxy Brain
              </button>
            </div>
          </div>

          <div class="mt-7 grid grid-cols-2 gap-3 sm:grid-cols-3 md:grid-cols-5">
            <div class="glass-2 rounded-2xl p-4 shadow-sm">
              <div class="text-xs font-semibold text-slate-500">Commits</div>
              <div id="kpiCommits" class="mt-2 text-2xl font-bold text-slate-900">-</div>
            </div>
            <div class="glass-2 rounded-2xl p-4 shadow-sm">
              <div class="text-xs font-semibold text-slate-500">Change Groups</div>
              <div id="kpiGroups" class="mt-2 text-2xl font-bold text-slate-900">-</div>
            </div>
            <div class="glass-2 rounded-2xl p-4 shadow-sm">
              <div class="text-xs font-semibold text-slate-500">Lines Changed</div>
              <div id="kpiLines" class="mt-2 text-2xl font-bold text-slate-900">-</div>
            </div>
            <div class="glass-2 rounded-2xl p-4 shadow-sm">
              <div class="text-xs font-semibold text-slate-500">Primary Mode</div>
              <div id="kpiMode" class="mt-2 text-2xl font-bold text-slate-900">-</div>
            </div>
            <div class="glass-2 rounded-2xl p-4 shadow-sm">
              <div class="text-xs font-semibold text-slate-500">Data Integrity</div>
              <div id="kpiIntegrity" class="mt-2 text-2xl font-bold text-slate-900">-</div>
            </div>
          </div>
        </div>
      </header>

      <!-- Main layout -->
      <div class="mt-8 grid grid-cols-1 gap-6 lg:grid-cols-[360px_1fr]">
        <!-- Sidebar (desktop) -->
        <aside class="enter hidden lg:block">
          <div class="glass shadow-glow rounded-3xl p-5">
            <div class="flex items-center justify-between">
              <div class="text-sm font-bold text-slate-900">Filters</div>
              <button
                id="btnReset"
                class="focus-ring rounded-xl border border-slate-900/10 bg-white/60 px-3 py-1.5 text-xs font-semibold text-slate-900 hover:bg-white"
                type="button"
              >
                Reset
              </button>
            </div>

            <div class="mt-4">
              <label class="text-xs font-semibold text-slate-600">Search</label>
              <input
                id="q"
                class="focus-ring mt-2 w-full rounded-2xl border border-slate-900/10 bg-white/70 px-3.5 py-2.5 text-sm text-slate-900 placeholder:text-slate-400"
                placeholder="commit, section, keyword..."
              />
            </div>

            <div class="mt-4">
              <div class="flex items-center justify-between">
                <label class="text-xs font-semibold text-slate-600">Min Impact</label>
                <div id="impactLabel" class="mono text-xs text-slate-500">-</div>
              </div>
              <input id="impact" class="mt-2 w-full" type="range" min="0" max="200" value="0" />
            </div>

            <div class="mt-5">
              <div class="flex items-center justify-between">
                <label class="text-xs font-semibold text-slate-600">Bucket Mode</label>
                <div class="mono text-xs text-slate-500" id="bucketModeLabel">primary</div>
              </div>
              <div class="mt-2 grid grid-cols-2 gap-2">
                <button
                  id="modePrimary"
                  class="focus-ring rounded-2xl border border-slate-900/10 bg-slate-900 px-3 py-2 text-xs font-semibold text-white"
                  type="button"
                >
                  Primary (disjoint)
                </button>
                <button
                  id="modeMulti"
                  class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-xs font-semibold text-slate-900"
                  type="button"
                >
                  Multi-label
                </button>
              </div>
              <p class="mt-2 text-xs leading-relaxed text-slate-500">
                Multi-label counts a change group in every bucket it matches. Primary assigns each group
                exactly one bucket for clean stacks.
              </p>
            </div>

            <div class="mt-5">
              <label class="text-xs font-semibold text-slate-600">Buckets</label>
              <div id="bucketToggles" class="mt-2 grid grid-cols-1 gap-2"></div>
            </div>

            <div class="mt-6">
              <label class="text-xs font-semibold text-slate-600">Quick Views</label>
              <div class="mt-2 grid grid-cols-1 gap-2">
                <button
                  id="viewTimeline"
                  class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-left text-xs font-semibold text-slate-900 hover:bg-white"
                  type="button"
                >
                  Timeline + Categories
                </button>
                <button
                  id="viewCommits"
                  class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-left text-xs font-semibold text-slate-900 hover:bg-white"
                  type="button"
                >
                  Commit Explorer
                </button>
                <button
                  id="viewAlien"
                  class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-left text-xs font-semibold text-slate-900 hover:bg-white"
                  type="button"
                >
                  Alien Telemetry (BOCPD)
                </button>
              </div>
            </div>
          </div>
        </aside>

        <!-- Content -->
        <main class="enter">
          <!-- Timeline -->
          <section id="sectionTimeline" class="glass shadow-glow rounded-3xl p-5">
            <div class="flex flex-col gap-3 sm:flex-row sm:items-end sm:justify-between">
              <div>
                <div class="text-sm font-bold text-slate-900">Timeline</div>
                <div class="mt-1 text-xs text-slate-500">
                  Dot = commit. Size = lines changed. Color = primary bucket.
                </div>
              </div>
              <div class="flex flex-wrap items-center gap-2">
                <div class="chip rounded-2xl px-3 py-1.5 text-xs font-semibold text-slate-700">
                  <span class="mono" id="rangeLabel">-</span>
                </div>
                <a
                  id="btnOpenRepo"
                  class="focus-ring chip inline-flex items-center gap-2 rounded-2xl px-3 py-1.5 text-xs font-semibold text-slate-700 hover:bg-white"
                  href="https://github.com/Dicklesworthstone/frankensqlite"
                  target="_blank"
                  rel="noreferrer"
                >
                  GitHub
                </a>
              </div>
            </div>

            <div id="timelineChart" class="chart mt-4 w-full"></div>
          </section>

          <section class="mt-6 grid grid-cols-1 gap-6 xl:grid-cols-2">
            <section class="glass shadow-glow rounded-3xl p-5">
              <div class="flex flex-col gap-3 sm:flex-row sm:items-end sm:justify-between">
                <div>
                  <div class="text-sm font-bold text-slate-900">Buckets Over Time</div>
                  <div class="mt-1 text-xs text-slate-500">
                    Stacked totals across time bins (or by commit). Use this for day/hour/15m/5m
                    density.
                  </div>
                </div>
                <div class="flex flex-wrap items-center gap-2">
                  <label class="chip inline-flex items-center gap-2 rounded-2xl px-3 py-1.5 text-xs font-semibold text-slate-700">
                    Resolution
                    <select id="stackResolution" class="ml-1 rounded-xl border border-slate-900/10 bg-white/70 px-2 py-1 text-xs">
                      <option value="commit">Commit</option>
                      <option value="day">Day</option>
                      <option value="hour">Hour</option>
                      <option value="15m">15 min</option>
                      <option value="5m">5 min</option>
                    </select>
                  </label>
                  <label class="chip inline-flex items-center gap-2 rounded-2xl px-3 py-1.5 text-xs font-semibold text-slate-700">
                    Metric
                    <select id="stackMetric" class="ml-1 rounded-xl border border-slate-900/10 bg-white/70 px-2 py-1 text-xs">
                      <option value="groups">Change groups</option>
                      <option value="lines">Lines changed (approx)</option>
                      <option value="tokens">Tokens changed (approx)</option>
                      <option value="lev">Levenshtein (hunks)</option>
                    </select>
                  </label>
                  <label class="chip inline-flex items-center gap-2 rounded-2xl px-3 py-1.5 text-xs font-semibold text-slate-700">
                    Timezone
                    <select id="stackTimezone" class="ml-1 rounded-xl border border-slate-900/10 bg-white/70 px-2 py-1 text-xs">
                      <option value="local">Local</option>
                      <option value="utc">UTC</option>
                    </select>
                  </label>
                </div>
              </div>
              <div id="stackChart" class="chart mt-4 w-full"></div>
            </section>

            <section class="glass shadow-glow rounded-3xl p-5">
              <div class="flex items-end justify-between">
                <div>
                  <div class="text-sm font-bold text-slate-900">Bucket Mix</div>
                  <div class="mt-1 text-xs text-slate-500">Distribution in current filter.</div>
                </div>
              </div>
              <div id="donutChart" class="chart mt-4 w-full"></div>
            </section>
          </section>

          <!-- Doc evolution -->
          <section id="sectionDoc" class="glass shadow-glow mt-6 rounded-3xl p-5">
            <div class="flex flex-col gap-3 sm:flex-row sm:items-end sm:justify-between">
              <div>
                <div class="text-sm font-bold text-slate-900">Document Evolution</div>
                <div class="mt-1 text-xs text-slate-500">
                  Scrub the timeline dock to step through history. Inspect the rendered spec and the
                  unified diff (with token + Levenshtein metrics).
                </div>
              </div>
              <div class="flex flex-wrap items-center gap-2">
                <button
                  id="docTabSpec"
                  class="focus-ring rounded-2xl bg-slate-900 px-3 py-2 text-xs font-semibold text-white hover:bg-slate-800"
                  type="button"
                >
                  Spec
                </button>
                <button
                  id="docTabDiff"
                  class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-xs font-semibold text-slate-900 hover:bg-white"
                  type="button"
                >
                  Diff
                </button>
                <button
                  id="docTabMetrics"
                  class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-xs font-semibold text-slate-900 hover:bg-white"
                  type="button"
                >
                  Metrics
                </button>
                <button
                  id="docTabSections"
                  class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-xs font-semibold text-slate-900 hover:bg-white"
                  type="button"
                >
                  Sections
                </button>
                <span class="relative ml-1">
                  <button
                    id="btnCopyLink"
                    class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-xs font-semibold text-slate-900 hover:bg-white"
                    type="button"
                    title="Copy a shareable permalink to the current view"
                  >Copy Link</button>
                  <button
                    id="btnShareHelp"
                    class="focus-ring ml-1 rounded-full border border-slate-900/10 bg-white/70 px-2 py-1 text-[10px] font-bold text-slate-500 hover:bg-white hover:text-slate-900"
                    type="button"
                    title="URL parameter reference"
                  >?</button>
                  <div
                    id="shareHelpPopover"
                    class="hidden absolute right-0 top-full z-50 mt-2 w-72 rounded-2xl border border-slate-900/10 bg-white p-4 shadow-glowLg text-left"
                  >
                    <div class="text-xs font-bold text-slate-900 mb-2">Permalink URL Parameters</div>
                    <table class="w-full text-[11px] text-slate-700">
                      <thead><tr class="border-b border-slate-200"><th class="pb-1 text-left font-semibold">Param</th><th class="pb-1 text-left font-semibold">Values</th><th class="pb-1 text-left font-semibold">Default</th></tr></thead>
                      <tbody>
                        <tr><td class="py-0.5 font-mono">v</td><td>Schema version</td><td>1</td></tr>
                        <tr><td class="py-0.5 font-mono">c</td><td>Commit index (0-based)</td><td>latest</td></tr>
                        <tr><td class="py-0.5 font-mono">t</td><td>spec | diff | metrics</td><td>spec</td></tr>
                        <tr><td class="py-0.5 font-mono">raw</td><td>1 = raw markdown</td><td>rendered</td></tr>
                        <tr><td class="py-0.5 font-mono">dm</td><td>pretty | raw</td><td>pretty</td></tr>
                        <tr><td class="py-0.5 font-mono">q</td><td>Search query</td><td>(empty)</td></tr>
                        <tr><td class="py-0.5 font-mono">mi</td><td>Min impact (lines)</td><td>0</td></tr>
                        <tr><td class="py-0.5 font-mono">bm</td><td>primary | multi</td><td>primary</td></tr>
                        <tr><td class="py-0.5 font-mono">b</td><td>Bucket IDs (1-10, comma-sep)</td><td>all</td></tr>
                        <tr><td class="py-0.5 font-mono">cmp</td><td>1 = A/B compare mode</td><td>off</td></tr>
                        <tr><td class="py-0.5 font-mono">ca</td><td>Compare "A" commit index</td><td>0</td></tr>
                        <tr><td class="py-0.5 font-mono">cb</td><td>Compare "B" commit index</td><td>0</td></tr>
                        <tr><td class="py-0.5 font-mono">dl</td><td>side-by-side | line-by-line</td><td>side-by-side</td></tr>
                      </tbody>
                    </table>
                    <div class="mt-2 text-[10px] text-slate-500">Default values are omitted for minimal URLs. Invalid values are clamped. Canonical key order: v, c, t, raw, dm, cmp, ca, cb, dl, q, mi, bm, b.</div>
                  </div>
                </span>
              </div>
            </div>

            <div id="docLoading" class="mt-4 rounded-3xl border border-slate-900/10 bg-white/60 p-4 text-sm text-slate-700">
              Loading spec evolution dataset... (local gzip JSON; no GitHub API)
            </div>

            <div id="docMain" class="mt-4 hidden grid-cols-1 gap-4 lg:grid lg:grid-cols-[1fr_360px]">
              <section class="glass-2 rounded-3xl p-4 lg:p-5">
                <div class="flex flex-col gap-2 sm:flex-row sm:items-center sm:justify-between">
                  <div class="min-w-0">
                    <div class="text-xs font-semibold text-slate-500">Selected commit</div>
                    <div id="docCommitTitle" class="mt-1 truncate text-sm font-semibold text-slate-900">-</div>
                  </div>
                  <div class="flex shrink-0 flex-wrap items-center gap-2">
                    <span id="docCommitMeta" class="chip mono inline-flex items-center rounded-full px-2.5 py-1 text-[11px] text-slate-700">-</span>
                    <a
                      id="docCommitLink"
                      class="focus-ring chip inline-flex items-center gap-2 rounded-2xl px-3 py-2 text-xs font-semibold text-slate-700 hover:bg-white"
                      href="#"
                      target="_blank"
                      rel="noreferrer"
                    >
                      Open commit
                    </a>
                  </div>
                </div>

                <div class="mt-4">
                  <div id="docSpecView" class="hidden">
                    <div class="flex items-center justify-between">
                      <div class="text-xs font-semibold text-slate-600">Rendered Markdown</div>
                      <div class="flex items-center gap-2">
                        <button
                          id="btnMiniMapToggle"
                          class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-1.5 text-[11px] font-semibold text-slate-900 hover:bg-white"
                          type="button"
                          title="Toggle heading mini-map"
                        >
                          Outline
                        </button>
                        <button
                          id="btnRawToggle"
                          class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-1.5 text-[11px] font-semibold text-slate-900 hover:bg-white"
                          type="button"
                        >
                          Toggle Raw
                        </button>
                      </div>
                    </div>
                    <div class="mt-3 flex gap-3">
                      <nav
                        id="miniMap"
                        class="hidden w-56 shrink-0 flex flex-col rounded-2xl border border-slate-900/10 bg-white/70"
                        style="max-height: 70vh"
                        aria-label="Document outline"
                      >
                        <div class="shrink-0 border-b border-slate-900/10 px-3 py-2">
                          <input
                            id="miniMapSearch"
                            type="text"
                            class="w-full rounded-lg border border-slate-900/10 bg-white/80 px-2 py-1 text-[11px] text-slate-700 placeholder:text-slate-400 focus:border-slate-400 focus:outline-none"
                            placeholder="Filter headings..."
                            autocomplete="off"
                          />
                        </div>
                        <div id="miniMapItems" class="flex-1 overflow-y-auto p-3" role="tree" tabindex="0"></div>
                      </nav>
                      <div id="docRendered" class="md min-w-0 flex-1 max-h-[70vh] overflow-auto rounded-2xl border border-slate-900/10 bg-white/70 p-4"></div>
                    </div>
                    <pre id="docRaw" class="codebox mono mt-3 hidden max-h-[70vh] overflow-auto rounded-2xl p-4 text-[11px] leading-relaxed text-slate-900"></pre>
                  </div>

                  <div id="docDiffView" class="hidden">
                    <div class="flex items-center justify-between flex-wrap gap-2">
                      <div class="flex items-center gap-2">
                        <div id="diffLabel" class="text-xs font-semibold text-slate-600">Diff (parent → selected)</div>
                        <button
                          id="btnCompareToggle"
                          class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-1.5 text-[11px] font-semibold text-slate-900 hover:bg-white"
                          type="button"
                          title="Toggle A/B compare mode"
                        >A/B Compare</button>
                      </div>
                      <div class="flex items-center gap-2">
                        <button
                          id="btnDiffLayout"
                          class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-1.5 text-[11px] font-semibold text-slate-900 hover:bg-white hidden"
                          type="button"
                          title="Toggle side-by-side / unified layout"
                        >Side-by-Side</button>
                        <button
                          id="btnPrettyDiff"
                          class="focus-ring rounded-xl bg-slate-900 px-3 py-1.5 text-[11px] font-semibold text-white hover:bg-slate-800"
                          type="button"
                        >
                          Pretty
                        </button>
                        <button
                          id="btnRawDiff"
                          class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-1.5 text-[11px] font-semibold text-slate-900 hover:bg-white"
                          type="button"
                        >
                          Raw
                        </button>
                      </div>
                    </div>
                    <!-- A/B Compare commit pickers with typeahead (bd-24q.1) -->
                    <div id="abCompareBar" class="hidden mt-2 flex items-center gap-2 flex-wrap rounded-xl border border-blue-200 bg-blue-50/60 px-3 py-2">
                      <label class="text-[11px] font-semibold text-blue-700">A:</label>
                      <div class="ab-picker" id="pickerA">
                        <button type="button" class="ab-picker-btn" id="pickerABtn" title="Select commit A">Select...</button>
                        <div class="ab-picker-dropdown hidden" id="pickerADropdown">
                          <input type="text" class="ab-picker-search" id="pickerASearch" placeholder="Search by subject, hash, or #index..." autocomplete="off" />
                          <div class="ab-picker-list" id="pickerAList"></div>
                        </div>
                      </div>
                      <button id="btnSwapAB" class="focus-ring rounded-lg border border-blue-200 bg-white px-2 py-1 text-[11px] font-semibold text-blue-700 hover:bg-blue-100" type="button" title="Swap A and B">⇄</button>
                      <label class="text-[11px] font-semibold text-blue-700">B:</label>
                      <div class="ab-picker" id="pickerB">
                        <button type="button" class="ab-picker-btn" id="pickerBBtn" title="Select commit B">Select...</button>
                        <div class="ab-picker-dropdown hidden" id="pickerBDropdown">
                          <input type="text" class="ab-picker-search" id="pickerBSearch" placeholder="Search by subject, hash, or #index..." autocomplete="off" />
                          <div class="ab-picker-list" id="pickerBList"></div>
                        </div>
                      </div>
                      <button id="btnResetAB" class="focus-ring rounded-lg border border-blue-200 bg-white px-2 py-1 text-[11px] font-semibold text-blue-700 hover:bg-blue-100" type="button" title="Reset to current commit">Reset</button>
                      <div id="abDiffLoading" class="hidden text-[11px] text-blue-600 ml-2">Computing diff…</div>
                    </div>
                    <!-- A/B Compare metric chips -->
                    <div id="abMetricsBar" class="hidden mt-2 flex items-center gap-2 flex-wrap">
                      <span id="abmLines" class="chip mono rounded-full px-2.5 py-1 text-[11px] text-blue-800 bg-blue-100 border border-blue-200"></span>
                      <span id="abmTokens" class="chip mono rounded-full px-2.5 py-1 text-[11px] text-blue-800 bg-blue-100 border border-blue-200"></span>
                      <span id="abmLev" class="chip mono rounded-full px-2.5 py-1 text-[11px] text-blue-800 bg-blue-100 border border-blue-200"></span>
                      <span id="abmHunks" class="chip mono rounded-full px-2.5 py-1 text-[11px] text-blue-800 bg-blue-100 border border-blue-200"></span>
                      <span id="abmBytes" class="chip mono rounded-full px-2.5 py-1 text-[11px] text-blue-800 bg-blue-100 border border-blue-200"></span>
                    </div>
                    <div id="diffPretty" class="mt-3 max-h-[70vh] overflow-auto rounded-2xl border border-slate-900/10 bg-white/70 p-2"></div>
                    <pre id="diffRaw" class="codebox mono mt-3 hidden max-h-[70vh] overflow-auto rounded-2xl p-4 text-[11px] leading-relaxed text-slate-900"></pre>
                  </div>

                  <div id="docMetricsView" class="hidden">
                    <div class="text-xs font-semibold text-slate-600">Computed metrics</div>
                    <div class="mt-3 grid grid-cols-2 gap-3 sm:grid-cols-4">
                      <div class="glass-2 rounded-2xl p-4 shadow-sm">
                        <div class="text-[11px] font-semibold text-slate-500">Tokens touched</div>
                        <div id="mTokens" class="mt-2 text-lg font-bold text-slate-900">-</div>
                      </div>
                      <div class="glass-2 rounded-2xl p-4 shadow-sm">
                        <div class="text-[11px] font-semibold text-slate-500">Levenshtein</div>
                        <div id="mLev" class="mt-2 text-lg font-bold text-slate-900">-</div>
                      </div>
                      <div class="glass-2 rounded-2xl p-4 shadow-sm">
                        <div class="text-[11px] font-semibold text-slate-500">Hunks</div>
                        <div id="mHunks" class="mt-2 text-lg font-bold text-slate-900">-</div>
                      </div>
                      <div class="glass-2 rounded-2xl p-4 shadow-sm">
                        <div class="text-[11px] font-semibold text-slate-500">Bytes touched</div>
                        <div id="mBytes" class="mt-2 text-lg font-bold text-slate-900">-</div>
                      </div>
                    </div>
                    <div class="mt-4">
                      <div class="grid grid-cols-1 gap-2 sm:grid-cols-2">
                        <button
                          id="btnComputeAll"
                          class="focus-ring w-full rounded-2xl bg-slate-900 px-4 py-2.5 text-sm font-semibold text-white hover:bg-slate-800"
                          type="button"
                        >
                          Compute token + Levenshtein metrics for all commits
                        </button>
                        <button
                          id="btnCancelCompute"
                          class="focus-ring hidden w-full rounded-2xl border border-rose-300 bg-rose-50 px-4 py-2.5 text-sm font-semibold text-rose-700 hover:bg-rose-100"
                          type="button"
                        >
                          Cancel running compute
                        </button>
                      </div>
                      <div class="mt-2 text-xs text-slate-500">
                        This can take a bit on large diffs. The UI stays responsive; charts update
                        as metrics arrive.
                      </div>
                      <div id="workerStatus" class="mt-2 text-xs text-slate-500"></div>
                      <div id="computeProgress" class="mt-2 hidden text-xs font-semibold text-slate-700"></div>
                    </div>
                  </div>

                  <div id="docSectionsView" class="hidden">
                    <div class="flex items-center justify-between">
                      <div class="text-xs font-semibold text-slate-600">Per-Section Change Summary</div>
                      <div class="flex items-center gap-2">
                        <input
                          id="sectionFilter"
                          type="text"
                          placeholder="Filter headings..."
                          class="focus-ring w-40 rounded-xl border border-slate-900/10 bg-white/70 px-3 py-1.5 text-[11px] text-slate-700 placeholder-slate-400"
                        />
                      </div>
                    </div>
                    <div id="sectionTableWrap" class="mt-3 max-h-[70vh] overflow-auto rounded-2xl border border-slate-900/10 bg-white/70">
                      <table id="sectionTable" class="w-full text-[11px]">
                        <thead class="sticky top-0 z-10 bg-white/95 backdrop-blur">
                          <tr class="border-b border-slate-200">
                            <th class="px-3 py-2 text-left font-semibold text-slate-500" data-sort="name">Section</th>
                            <th class="px-3 py-2 text-right font-semibold text-slate-500 w-16" data-sort="add">+Lines</th>
                            <th class="px-3 py-2 text-right font-semibold text-slate-500 w-16" data-sort="del">-Lines</th>
                            <th class="px-3 py-2 text-right font-semibold text-slate-500 w-20" data-sort="tokens">Tokens</th>
                            <th class="px-3 py-2 text-right font-semibold text-slate-500 w-20" data-sort="impact">Impact</th>
                          </tr>
                        </thead>
                        <tbody id="sectionTableBody"></tbody>
                      </table>
                    </div>
                    <div id="sectionEmpty" class="hidden mt-3 text-xs text-slate-400 italic p-3">No section changes for this commit.</div>
                    <button id="btnOpenSectionSheet" class="mt-3 w-full sm:hidden focus-ring rounded-2xl bg-slate-900 px-4 py-2.5 text-sm font-semibold text-white hover:bg-slate-800" type="button">View sections (mobile)</button>
                  </div>
                </div>
              </section>

              <aside class="glass-2 rounded-3xl p-4 lg:p-5">
                <div class="text-xs font-semibold text-slate-600">Change summary</div>
                <div id="docSummary" class="mt-3 space-y-2 text-sm text-slate-700"></div>
                <div class="mt-4 border-t border-slate-900/10 pt-4">
                  <div class="text-xs font-semibold text-slate-600">Patch notes</div>
                  <div class="mt-2 text-xs leading-relaxed text-slate-600">
                    Distances are computed per-hunk between removed and added blocks (byte-level
                    Levenshtein in WASM), then summed.
                  </div>
                </div>
              </aside>
            </div>
          </section>

          <!-- Commit explorer -->
          <section id="sectionCommits" class="glass shadow-glow mt-6 rounded-3xl p-5">
            <div class="flex flex-col gap-3 sm:flex-row sm:items-end sm:justify-between">
              <div>
                <div class="text-sm font-bold text-slate-900">Commit Explorer</div>
                <div class="mt-1 text-xs text-slate-500">
                  Click a commit to expand its change groups and evidence excerpts.
                </div>
              </div>
              <div class="flex items-center gap-2">
                <div class="chip rounded-2xl px-3 py-1.5 text-xs font-semibold text-slate-700">
                  Showing <span id="showingCount" class="mono">-</span>
                </div>
              </div>
            </div>

            <div id="commitList" class="mt-4 grid grid-cols-1 gap-3"></div>
          </section>

          <!-- Alien telemetry -->
          <section id="sectionAlien" class="glass shadow-glow mt-6 rounded-3xl p-5">
            <div class="flex flex-col gap-3 sm:flex-row sm:items-end sm:justify-between">
              <div>
                <div class="text-sm font-bold text-slate-900">Alien Telemetry</div>
                <div class="mt-1 text-xs text-slate-500">
                  BOCPD change-point detection over commit impact (lines changed).
                </div>
              </div>
              <div class="chip rounded-2xl px-3 py-1.5 text-xs font-semibold text-slate-700">
                Hazard <span id="hazardLabel" class="mono">-</span>
              </div>
            </div>

            <div class="mt-4 grid grid-cols-1 gap-4 lg:grid-cols-[1fr_360px]">
              <div>
                <div id="bocpdChart" class="chart w-full"></div>
              </div>
              <div class="glass-2 rounded-3xl p-5">
                <div class="text-xs font-semibold text-slate-600">Galaxy Brain Card</div>
                <div class="mt-3 codebox rounded-2xl p-3">
                  <div class="mono text-[11px] leading-relaxed text-slate-900">
                    P(r_t | x_{1:t}) ∝ Σ_{r_{t-1}} P(x_t | r_t, …) · P(r_t | r_{t-1}) · P(r_{t-1} |
                    x_{1:t-1})
                  </div>
                </div>
                <p class="mt-3 text-xs leading-relaxed text-slate-600">
                  This page runs a tiny BOCPD model locally on the commit impact series (add+del).
                  With only 101 points, it is fast and deterministic. Change points are used only as
                  a lens, not a claim of truth.
                </p>
                <div class="mt-4">
                  <label class="text-xs font-semibold text-slate-600">Hazard H</label>
                  <input
                    id="hazard"
                    class="mt-2 w-full"
                    type="range"
                    min="0.01"
                    max="0.30"
                    step="0.01"
                    value="0.10"
                  />
                </div>
              </div>
            </div>
          </section>
        </main>
      </div>
    </div>

    <!-- Mobile filter sheet -->
    <div id="overlay" class="fixed inset-0 z-40 hidden bg-slate-900/30 backdrop-blur-sm"></div>
    <div
      id="sheet"
      class="sheet fixed bottom-0 left-0 right-0 z-50 hidden rounded-t-3xl border border-slate-900/10 bg-white/90 p-5 shadow-2xl backdrop-blur-xl"
    >
      <div class="flex items-center justify-between">
        <div class="text-sm font-bold text-slate-900">Filters</div>
        <button
          id="btnCloseSheet"
          class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-1.5 text-xs font-semibold text-slate-900 hover:bg-white"
          type="button"
        >
          Close
        </button>
      </div>
      <div class="mt-4">
        <label class="text-xs font-semibold text-slate-600">Search</label>
        <input
          id="qMobile"
          class="focus-ring mt-2 w-full rounded-2xl border border-slate-900/10 bg-white/70 px-3.5 py-2.5 text-sm text-slate-900 placeholder:text-slate-400"
          placeholder="commit, section, keyword..."
        />
      </div>
      <div class="mt-4">
        <div class="flex items-center justify-between">
          <label class="text-xs font-semibold text-slate-600">Min Impact</label>
          <div id="impactLabelMobile" class="mono text-xs text-slate-500">-</div>
        </div>
        <input id="impactMobile" class="mt-2 w-full" type="range" min="0" max="200" value="0" />
      </div>
      <div class="mt-5">
        <label class="text-xs font-semibold text-slate-600">Bucket Mode</label>
        <div class="mt-2 grid grid-cols-2 gap-2">
          <button
            id="modePrimaryMobile"
            class="focus-ring rounded-2xl border border-slate-900/10 bg-slate-900 px-3 py-2 text-xs font-semibold text-white"
            type="button"
          >
            Primary
          </button>
          <button
            id="modeMultiMobile"
            class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-xs font-semibold text-slate-900"
            type="button"
          >
            Multi-label
          </button>
        </div>
      </div>
      <div class="mt-5">
        <label class="text-xs font-semibold text-slate-600">Buckets</label>
        <div id="bucketTogglesMobile" class="mt-2 grid grid-cols-1 gap-2"></div>
      </div>
      <div class="mt-6 grid grid-cols-2 gap-2">
        <button
          id="btnResetMobile"
          class="focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-xs font-semibold text-slate-900 hover:bg-white"
          type="button"
        >
          Reset
        </button>
        <button
          id="btnApplyMobile"
          class="focus-ring rounded-2xl bg-slate-900 px-3 py-2 text-xs font-semibold text-white hover:bg-slate-800"
          type="button"
        >
          Apply
        </button>
      </div>
    </div>

    <!-- Mobile section summary sheet (bd-24q.8.3) -->
    <div id="sectionSheetOverlay" class="fixed inset-0 z-40 hidden bg-slate-900/30 backdrop-blur-sm"></div>
    <div
      id="sectionSheet"
      class="sheet fixed bottom-0 left-0 right-0 z-50 hidden rounded-t-3xl border border-slate-900/10 bg-white/90 p-5 shadow-2xl backdrop-blur-xl"
      style="max-height: 75vh"
    >
      <div class="flex items-center justify-between">
        <div class="text-sm font-bold text-slate-900">Section Changes</div>
        <button
          id="btnCloseSectionSheet"
          class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-1.5 text-xs font-semibold text-slate-900 hover:bg-white"
          type="button"
        >Close</button>
      </div>
      <input
        id="sectionFilterMobile"
        type="text"
        placeholder="Filter headings..."
        class="focus-ring mt-3 w-full rounded-2xl border border-slate-900/10 bg-white/70 px-3.5 py-2.5 text-sm text-slate-900 placeholder:text-slate-400"
      />
      <div id="sectionSheetList" class="mt-3 overflow-auto space-y-1" style="max-height: 55vh"></div>
    </div>

    <!-- Timeline dock -->
    <div id="dock" class="dock fixed inset-x-0 bottom-0 z-30 px-4 py-3">
      <div class="mx-auto max-w-[1200px]">
        <div class="flex items-center justify-between gap-3">
          <div class="min-w-0 flex items-center gap-2">
            <button
              id="dockCollapseToggle"
              class="sm:hidden focus-ring inline-flex h-6 w-6 items-center justify-center rounded-lg border border-slate-900/10 bg-white/70 text-[10px] font-bold text-slate-600 hover:bg-white"
              type="button"
              aria-label="Toggle dock"
            >&#9650;</button>
            <div>
              <div class="text-[11px] font-semibold text-slate-500">Timeline scrubber</div>
              <div id="dockTitle" class="mt-0.5 truncate text-xs font-semibold text-slate-900">-</div>
            </div>
          </div>
          <div class="shrink-0 flex items-center gap-2">
            <button
              id="dockPrev"
              class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-2.5 text-[11px] font-semibold text-slate-900 hover:bg-white"
              type="button"
              aria-label="Previous commit"
            >
              Prev
            </button>
            <button
              id="dockPlayPause"
              class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-2.5 text-[11px] font-semibold text-slate-900 hover:bg-white min-w-[3.5rem]"
              type="button"
              aria-label="Play or pause timeline playback"
            >
              &#9654;
            </button>
            <button
              id="dockNext"
              class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-3 py-2.5 text-[11px] font-semibold text-slate-900 hover:bg-white"
              type="button"
              aria-label="Next commit"
            >
              Next
            </button>
            <select
              id="dockSpeed"
              class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-2 py-2 text-[11px] font-semibold text-slate-900 hover:bg-white cursor-pointer"
              aria-label="Playback speed"
            >
              <option value="0.25">0.25x</option>
              <option value="0.5">0.5x</option>
              <option value="1" selected>1x</option>
              <option value="2">2x</option>
              <option value="4">4x</option>
            </select>
            <button
              id="dockLoop"
              class="focus-ring rounded-xl border border-slate-900/10 bg-white/70 px-2.5 py-2.5 text-[11px] font-semibold text-slate-500 hover:bg-white"
              type="button"
              aria-label="Toggle loop playback"
              title="Loop"
            >
              &#x1F501;
            </button>
          </div>
        </div>

        <div id="dockBody">
          <canvas
            id="dockCanvas"
            class="dock-canvas mt-2 rounded-2xl border border-slate-900/10 bg-white/70"
          ></canvas>
          <input id="dockSlider" class="dock-slider mt-2" type="range" min="0" max="1" value="0" />
          <div class="mt-1 flex items-center justify-between text-[11px] text-slate-500">
            <div id="dockLeftLabel" class="mono">-</div>
            <div id="dockRightLabel" class="mono">-</div>
          </div>
        </div>
      </div>
    </div>

    <!-- Dependencies -->
    <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/dayjs@1.11.10/dayjs.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/dayjs@1.11.10/plugin/utc.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/pako@2.1.0/dist/pako.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/markdown-it@14.1.0/dist/markdown-it.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/dompurify@3.1.0/dist/purify.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/diff2html/bundles/js/diff2html.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/diff@7.0.0/dist/diff.min.js"></script>

    <script>
      // Extend dayjs with UTC plugin for timezone-aware binning.
      if (window.dayjs_plugin_utc) dayjs.extend(window.dayjs_plugin_utc);

      // requestIdleCallback polyfill (Safari < 16.4)
      if (typeof requestIdleCallback === "undefined") {
        window.requestIdleCallback = function (cb) {
          const start = Date.now();
          return setTimeout(() => {
            cb({
              didTimeout: false,
              timeRemaining: () => Math.max(0, 50 - (Date.now() - start)),
            });
          }, 1);
        };
        window.cancelIdleCallback = function (id) {
          clearTimeout(id);
        };
      }

      // -----------------------------
      // Data: commit metadata + stats
      // -----------------------------

      const COMMIT_LOG_RAW = String.raw`c08f1602d03b1833a4f91c8f77347f8f196bac9d|c08f160|2026-02-07T01:17:42-05:00|Dicklesworthstone|Add comprehensive specification documents (8,628 + 1,206 lines)
f9d88aa96f50893f8531373d0734597ebae3c592|f9d88aa|2026-02-07T01:43:43-05:00|Dicklesworthstone|Promote SSI to V1, add intent logs, shard lock tables, expand specs
76eef31be20a6faedc62e7fecce88e78652ebdcf|76eef31|2026-02-07T02:04:31-05:00|Dicklesworthstone|docs: reconcile v1 specs (RaptorQ/SSI/K semantics)
9800b17df4a56c2dc065cf566c2810d4ed2e576c|9800b17|2026-02-07T02:45:02-05:00|Dicklesworthstone|Spec V1.3: scope doctrine, ECS substrate, multi-process MVCC, encryption
79018f161649e6d7977bc74eb849a0e22eceb286|79018f1|2026-02-07T02:56:16-05:00|Dicklesworthstone|Spec: revise WAL frame layout for true compatibility mode
5ad34871f7242de61378843c6c1e8311e35d9fa3|5ad3487|2026-02-07T03:14:56-05:00|Dicklesworthstone|Spec V1.4: Codex synthesis — RaptorQ everywhere, WAL sidecar overhaul, ECS layout, replication
7b2c677cf61adda977e71524b59d7ec234137962|7b2c677|2026-02-07T03:16:08-05:00|Dicklesworthstone|Spec V1.5: alien-artifact discipline — decision-theoretic SSI, BOCPD, monitoring stack, native mode
bf0426417685504bb2b2f5acfc4de2c2f087ef8b|bf04264|2026-02-07T03:31:40-05:00|Dicklesworthstone|Spec V1.6a: RaptorQ-native SSI witness plane — cross-process, distributed, proof-carrying
05cf07847753f444b9f74dfd0279ef96c3ff86b9|05cf078|2026-02-07T03:32:45-05:00|Dicklesworthstone|Spec V1.6b: alien-artifact discipline — formal derivations for every tunable parameter
e8ddf469d8ab8ff8eefbae92b24453e06fcf4627|e8ddf46|2026-02-07T03:33:30-05:00|Dicklesworthstone|Spec V1.6c: performance optimizations — arena allocators, CAR cache, per-invariant monitoring
480c10086fb73630744364f5fb34966de2572dc1|480c100|2026-02-07T03:35:03-05:00|Dicklesworthstone|Spec V1.6d: SSI transaction struct — TxnToken edges, eager abort, terminology scoping
0313678eff44b70cd4ea40cabc2f2222cac2ba93|0313678|2026-02-07T03:38:40-05:00|Dicklesworthstone|Spec V1.6e: SSI detection algorithm — proof-carrying commit with eager abort
b97b1abe4727a738197180c5639738c1da2f5311|b97b1ab|2026-02-07T03:39:32-05:00|Dicklesworthstone|Spec V1.6f: fixup — remove 3-way merge artifacts, restore canonical state
643c89c1c941d1e47cb6a2574e01fd7556f622ec|643c89c|2026-02-07T03:42:22-05:00|Dicklesworthstone|Spec V1.6g: final reconciliation — canonical SSI detection algorithm
0404e42c9dbdc677da9b8a5731d29290ef9f3b26|0404e42|2026-02-07T03:50:09-05:00|Dicklesworthstone|Spec V1.6h: native mode commit protocol — SSI witness-plane integration
2a6353d2f77a050454d2a6fd4b3fd6c8363064e9|2a6353d|2026-02-07T04:08:59-05:00|Dicklesworthstone|spec: align asupersync integration + preserve harness canon
3253050f2835fb305bd115bd9f5939b72cd46503|3253050|2026-02-07T04:30:26-05:00|Dicklesworthstone|docs/spec: schema epoch + align encryption
06dfe9a709d61fc899e00ee33d54dc15d003bb48|06dfe9a|2026-02-07T04:43:05-05:00|Dicklesworthstone|docs/spec: seal internal traits + clarify extensions
63f6057e8705b7105837952bc8ea977ad6f34da6|63f6057|2026-02-07T04:44:32-05:00|Dicklesworthstone|docs/spec: key ARC pages by commit_seq
2cff3a0287f1de30d2fd1185c376ac81913922c9|2cff3a0|2026-02-07T04:45:11-05:00|Dicklesworthstone|docs/spec: clarify INV-1 monitor name
4ba1ee3bce5eef69acf5e24c61e0fe5482662a79|4ba1ee3|2026-02-07T04:54:23-05:00|Dicklesworthstone|docs/spec: tighten glossary + fix merge pseudocode types
72eb835e83063d8fe19c1288a8c4624438473b2e|72eb835|2026-02-07T11:07:04-05:00|Dicklesworthstone|spec: fix cross-process cleanup + lock table semantics
0b543b5b587d72a5531a37324ad31fab0157b6ed|0b543b5|2026-02-07T11:21:20-05:00|Dicklesworthstone|docs/spec: deepen asupersync integration (regions/cancel/supervision)
d7b38efedc6f77029ce412c1eea2d0197b8b8fc6|d7b38ef|2026-02-07T11:50:08-05:00|Dicklesworthstone|Spec V1.7: deep audit fixes for Section 5 (MVCC Formal Model)
322af1706478c8da09b09af4927e6a011041c5a6|322af17|2026-02-07T11:53:09-05:00|Dicklesworthstone|Spec V1.7a + README: deep audit fixes (MVCC model alignment, SSI/ordering)
1a672dd545885d61c2531bf2be178d0edb4a5065|1a672dd|2026-02-07T12:09:27-05:00|Dicklesworthstone|Spec V1.7b: deep audit fixes for Section 7 (Checksums and Integrity)
a19a3acb957598db6e11ece9ab199ae8f34eb88e|a19a3ac|2026-02-07T12:10:44-05:00|Dicklesworthstone|docs/spec: deepen asupersync integration + witness-plane rigor
d0fda0186411a7f75bd73771f484259c29d7921a|d0fda01|2026-02-07T12:11:01-05:00|Dicklesworthstone|docs/spec: minor wording tweak
b8344dba9b295b31b843807f80c6ec10e601459b|b8344db|2026-02-07T12:11:40-05:00|Dicklesworthstone|docs/spec: tighten crate boundaries + WAL notes
5e2344ce7b5640aaad2fb48bed85838f9c9ecfef|5e2344c|2026-02-07T12:12:14-05:00|Dicklesworthstone|docs/spec: add WAL checksum algorithm + refine repair tail bounds
713e6dc4f8b202870b16e154a597bcd735d5c05b|713e6dc|2026-02-07T12:12:40-05:00|Dicklesworthstone|docs/spec: bump document version
e2dc2232567e901ce8f7192303c9da013e4c8d7c|e2dc223|2026-02-07T12:13:08-05:00|Dicklesworthstone|docs/spec: clarify fragmentation byte + add SQLite varint encoding
3c5d5a177c8fbd78fd6f11f32a02e4c3ada9e172|3c5d5a1|2026-02-07T12:13:41-05:00|Dicklesworthstone|docs/spec: fix index local-payload math + clarify affinity coercion
714570a488c9e93f3349ba527e70e744570555e9|714570a|2026-02-07T12:14:00-05:00|Dicklesworthstone|docs/spec: expand pointer-map entry type semantics
893691fc58cca08e1fba3a25a5ddf520624d6860|893691f|2026-02-07T12:14:48-05:00|Dicklesworthstone|docs/spec: add SSI invariants + correct e-process example + fix index max_local
a9aff80df7c0d04f465db40b74165a2e75ae5e29|a9aff80|2026-02-07T12:15:31-05:00|Dicklesworthstone|docs/spec: tighten budget algebra wording + fix BOCPD recursion
80bc4494162ff420cdb48c5122324d9bdc8d328b|80bc449|2026-02-07T12:16:49-05:00|Dicklesworthstone|docs/spec: refine public API facade + type-erased function state factories
a5a85e64c6356176ad86795d69e1293bd76f2601|a5a85e6|2026-02-07T12:17:16-05:00|Dicklesworthstone|docs/spec: thread Cx into rollback + clarify recursive CTE UNION semantics
7c93dbb54e2d939f820d97a469bf204829ce97c8|7c93dbb|2026-02-07T12:20:46-05:00|Dicklesworthstone|Spec audit round 2: WAL checksum inversion, WAL-index hash, ARC claim
07236c61ff86a87f22bb1aa6fd26885eb527d268|07236c6|2026-02-07T12:23:21-05:00|Dicklesworthstone|docs/spec: fix WAL checksum endianness + clarify WindowsVfs + layering notes
d9146f771541d7e134f540e11016df82cc6e17b9|d9146f7|2026-02-07T12:28:33-05:00|Dicklesworthstone|spec: harden marker stream + compaction + RaptorQ overhead
30619b34ea187d682283f8cd137d8d30e306e6bd|30619b3|2026-02-07T12:28:52-05:00|Dicklesworthstone|docs/spec: clarify R*-tree geopoly wording
7e9cace601485722e7b31411ad74796c0685bd05|7e9cace|2026-02-07T12:29:17-05:00|Dicklesworthstone|docs/spec: refine Zipf conflict probability
ff937f3d678f88eb855051dbbd32e8e5046bea0f|ff937f3|2026-02-07T12:29:51-05:00|Dicklesworthstone|docs/spec: tighten MVCC + merge modeling notes
61ee3e03f4cd5e44b189da55bb70e91cdba63a29|61ee3e0|2026-02-07T12:30:34-05:00|Dicklesworthstone|docs/spec: harden ARC eviction failure path
a90a3794616e12b083f88276e3cb47a045f3ac10|a90a379|2026-02-07T12:32:18-05:00|Dicklesworthstone|Spec V1.7c: deep audit fixes for Section 6 (Buffer Pool: ARC Cache)
246102e6b4266fa15000580a3d016d76bcb1c801|246102e|2026-02-07T12:35:11-05:00|Dicklesworthstone|Spec audit round 3: fix operator precedence tables (§10.2 and §12.15)
5ea1b6fb02945e4bc147c37ae7d054d92fdd9045|5ea1b6f|2026-02-07T12:44:12-05:00|Dicklesworthstone|Spec audit round 4: SSI pseudocode, precedence, RETURNING, merge terminology
f1f25abe3b40ad0fb8704eaee310aee17de78d25|f1f25ab|2026-02-07T12:46:31-05:00|Dicklesworthstone|Spec: harden write-merge (no raw XOR on SQLite pages)
9725f136b2450c6411f5735a1575a068f2aacc0b|9725f13|2026-02-07T12:47:33-05:00|Dicklesworthstone|Spec: clarify cross-process ordering + file format edge cases
56742886aae65bdca3a34acda8d677b52a0c241d|5674288|2026-02-07T12:47:51-05:00|Dicklesworthstone|Spec: witness hot-plane epoch advancement safety
71a41750d478e8ecad031a2f490e84a4424a839e|71a4175|2026-02-07T12:48:33-05:00|Dicklesworthstone|Spec: publish commit_seq only after durable marker
9fa8f7b6082f164d4cb1026296a360cfbafe1f73|9fa8f7b|2026-02-07T12:48:54-05:00|Dicklesworthstone|Spec: make commit_seq allocation marker-tip-derived
86d63af874d7a3dc7d522d47bf5f004f12913adb|86d63af|2026-02-07T12:49:41-05:00|Dicklesworthstone|Spec: compatibility mode legacy writer exclusion + WAL-index bridge
23f575f7d4ee20399865e33abca1f93e60eda2d7|23f575f|2026-02-07T12:49:56-05:00|Dicklesworthstone|Spec: Acquire snapshot commit_seq; tighten V1 rebase scope
6228e273d3dce6ca67c97fd43afb9018d19c6f5f|6228e27|2026-02-07T12:53:57-05:00|Dicklesworthstone|spec: fix native commit_seq gaps + witness epoch race
2e3ea218a908f4a22791b4c8b30a0dbf746f867b|2e3ea21|2026-02-07T12:58:54-05:00|Dicklesworthstone|Spec audit round 5: SharedPageLockTable key-stability, marker encoding, legacy writer exclusion
d05de23ea89da924e86232c8da519b6b58b4a0d5|d05de23|2026-02-07T12:59:44-05:00|Dicklesworthstone|Spec audit round 5b: align §5.6.3.1 rebuild prose with tombstone removal + header notes
fa1830e4593854a492d78bec24da1db90351c2f0|fa1830e|2026-02-07T13:00:08-05:00|Dicklesworthstone|Spec audit round 5c: permeation map, fragment limit, cell payload refs
70436b5c2c04e42c0aaf4b776a8f779416abdeff|70436b5|2026-02-07T13:00:29-05:00|Dicklesworthstone|Spec audit: WAL index byte order note + header layout corrections
a4e773481bd417530fe83f33868ef6ce5656e933|a4e7734|2026-02-07T13:02:16-05:00|Dicklesworthstone|Spec: add VFS shm API + clarify commit marker record
09c095901ddb1e5db47f55ddf5c616b5d51bf74e|09c0959|2026-02-07T13:02:34-05:00|Dicklesworthstone|Spec: checkpoint writer trait + replication gate
40a2ac7739778b30cb11129a782d8746bd13e6ed|40a2ac7|2026-02-07T13:03:14-05:00|Dicklesworthstone|Spec: bump audit footer to v1.18
40c6c3d152cf7d86104ad9ca19fbb850d6fdf343|40c6c3d|2026-02-07T13:04:47-05:00|Dicklesworthstone|Spec: lock-table acquire returns BUSY on insert race
8512e62aed8835864a8800ed895c91a8773dc996|8512e62|2026-02-07T13:08:08-05:00|Dicklesworthstone|spec: move compat page-FEC to sidecar + tighten UDP bounds
e31897fd92088e5305735d080e224a23b7315a11|e31897f|2026-02-07T13:08:45-05:00|Dicklesworthstone|spec: clarify checkpoint chunk MTU rationale
b03db7bc0b8a2705fe6fda19f5f149452755284c|b03db7b|2026-02-07T13:10:18-05:00|Dicklesworthstone|Spec V1.7d: deep audit fixes for Section 12 (SQL Coverage)
dd190e4f7f24aa2a8b7bb1bbc7020aead6d014c0|dd190e4|2026-02-07T13:23:44-05:00|Dicklesworthstone|Spec audit round 5+: fix GF(256) worked example, WAL checksum naming, MMR, IBLT
a664265e3f5876177b7c1410dad7119e01e7f9e9|a664265|2026-02-07T13:30:10-05:00|Dicklesworthstone|Spec+VFS: SAFE write-merge proofs + policy controller
599cafb3f29c52dd2a44f27d5632032c54a34a25|599cafb|2026-02-07T13:30:47-05:00|Dicklesworthstone|Spec: clarify forward target + symbol auth + epoch helpers
e42a43de57ff5d4ee616cd2ec3b54a0db249a3e6|e42a43d|2026-02-07T13:31:08-05:00|Dicklesworthstone|Spec: symbol auth master key derivation + VdbeOp p5 note
f158b44a5bd96ef34b378b58f61399568424f6d4|f158b44|2026-02-07T13:31:30-05:00|Dicklesworthstone|Spec: symbol auth wording + planner cost note
3cf0f13571c101725f9ef34ce2476c90478ff07d|3cf0f13|2026-02-07T13:35:09-05:00|Dicklesworthstone|Spec V1.7e: deep audit fix for Section 3 (RaptorQ MTU/sub-blocking)
0cb22962ec87cb29a29a44acfc5ec14588aadb23|0cb2296|2026-02-07T13:40:27-05:00|Dicklesworthstone|Spec: WAL lock refs + row-value expr
1680d6931f5773673fae021b7cc646e3f3faab44|1680d69|2026-02-07T13:45:48-05:00|Dicklesworthstone|Spec V1.7f: deep audit fixes for Section 4 (Asupersync e-process math)
1e2aae9940969190c6b9bebe94c94749eee91626|1e2aae9|2026-02-07T13:50:03-05:00|Dicklesworthstone|Spec: add UpdateExpression to write-merge system (§5.10)
da22f479dc2f8d1e98294750b741b3cce168fd19|da22f47|2026-02-07T13:55:11-05:00|Dicklesworthstone|Spec: formal durability+policy hardening
c25f0d00238fc36f355f818f571abc3d4a1198df|c25f0d0|2026-02-07T13:56:03-05:00|Dicklesworthstone|Spec: UpdateExpression rebase index regen
02e48a41810bdb2ad471926d9728a9a9af12b155|02e48a4|2026-02-07T14:00:52-05:00|Dicklesworthstone|Spec: fix maxLocal example + add encryption plan gates
bcd893da081fd15b1614a05c4d8b593f55cf7011|bcd893d|2026-02-07T14:16:38-05:00|Dicklesworthstone|Spec V1.7g: Section 9 deep audit — BtreeCursorOps, shm_map safety, opt-level fix
6da51572d2b5b76eb9a37ffa99da76bfb850cd83|6da5157|2026-02-07T14:16:48-05:00|Dicklesworthstone|Spec: deep audit fixes for §5.6.2 + §7.11
22e75e65b831adda852df3a11628e793aaa9e7a9|22e75e6|2026-02-07T14:27:26-05:00|Dicklesworthstone|planning: close obsolete beads; align spec/docs
2f0970b9a5d8fc755269ba848169f5a1369ec6a6|2f0970b|2026-02-07T14:27:39-05:00|Dicklesworthstone|Spec V1.7h: Section 10 deep audit — lexer DQS, cost model, UPDATE trace
5cc32a6bc813ff59db231f29db5b95a13ea94704|5cc32a6|2026-02-07T14:28:53-05:00|Dicklesworthstone|spec: tighten asupersync net + scheduler lane requirements
dc92e549d05ac8dc0c23937d6e6dfa874729fdd4|dc92e54|2026-02-07T14:31:37-05:00|Dicklesworthstone|spec: fix MVCC version-chain delta encoding
2433562451b188c9233f6fd83a9ef7573ce48f97|2433562|2026-02-07T14:33:19-05:00|Dicklesworthstone|spec: add per-source validation for WAL-FEC recovery
82dfd4bf99283e5ab283073651787b3c7975561f|82dfd4b|2026-02-07T14:33:56-05:00|Dicklesworthstone|spec: clarify WAL-FEC repair semantics + WAL checksums
ab20d7fac6f83ac527475927aa53b204c11ed7c8|ab20d7f|2026-02-07T14:34:24-05:00|Dicklesworthstone|spec: add ARC p-update online-learning note
96f32f3174453e68960d849672cb4b7a8d33d1c7|96f32f3|2026-02-07T14:34:56-05:00|Dicklesworthstone|spec: lock-table rebuild busy-wait rule + ARC p-update research note
f37158f2468145272817fd65794d78be539be19b|f37158f|2026-02-07T14:35:20-05:00|Dicklesworthstone|spec: fix UpdateExpression rebase step numbering
19fa01f6d66c19892c634a02da44f7f536b29e43|19fa01f|2026-02-07T14:42:37-05:00|Dicklesworthstone|Spec V1.7i: Section 13 deep audit — strftime specifiers, aggregate ORDER BY
a3e7ae521e6a0dcb3cf9489ac5a67b0f62aad526|a3e7ae5|2026-02-07T14:45:48-05:00|Dicklesworthstone|Spec V1.7j: Section 14 deep audit — FTS5 NOT is binary-only
56a4e91425fa5cba9aecf08f786691ef03563750|56a4e91|2026-02-07T14:46:20-05:00|Dicklesworthstone|spec: define replication changeset_bytes encoding
d2a49862864dff9799e236d2cf81b51ed268f93f|d2a4986|2026-02-07T14:48:36-05:00|Dicklesworthstone|spec: tighten crash/interop guidance (TxnSlot cleaning, WAL marks, ARC, triggers)
859e81752809cd343b00ad6499c159d2f341cc46|859e817|2026-02-07T14:50:40-05:00|Dicklesworthstone|spec: fully reset TxnSlot state/mode during cleanup
be3d256b12679a32f2fb4e44eeb82e513607b284|be3d256|2026-02-07T15:03:41-05:00|Dicklesworthstone|spec: make .db-fec checkpoint-owned + add per-source validation
65ab2f709f724750878f2cb5d045649bf0c53e89|65ab2f7|2026-02-07T15:14:48-05:00|Dicklesworthstone|spec: add auto-tuning defaults and scaling knobs
63ee097eb68a72ac23effab3979978adafb5b411|63ee097|2026-02-07T15:15:18-05:00|Dicklesworthstone|spec: fix §18 probabilistic conflict model — 10x arithmetic error + missing W²
29f7ebe942a43891f2a55fd998d02c37411efc9f|29f7ebe|2026-02-07T15:27:04-05:00|Dicklesworthstone|spec: harden rebase + safe SHM + skew-aware conflicts
6b0c12fc40c5759f5d53028ba692e8aaff80fc92|6b0c12f|2026-02-07T15:30:14-05:00|Dicklesworthstone|spec: fix §16 Phase 7 join ordering — beam search, not exhaustive
b181b6d148e02862b195824020954abadae8de88|b181b6d|2026-02-07T15:31:48-05:00|Dicklesworthstone|spec: fix §8.3 planner join ordering — beam search, not exhaustive
d302b391af5c2932c9097dd85923a8535593e45e|d302b39|2026-02-07T15:44:17-05:00|Dicklesworthstone|mvcc/spec: witness hot-index sizing manifest
017745631c79f0c9061e4fdba8d0ce09ecc6c86d|0177456|2026-02-07T15:44:51-05:00|Dicklesworthstone|spec: clarify Zipf write-set skew section
5dae90d79aef7d70300fe53a17f9e40dba24b309|5dae90d|2026-02-07T15:45:46-05:00|Dicklesworthstone|spec: tighten Zipf s_hat guidance
ca60e008352585f2501ca3f81337849396b4d140|ca60e00|2026-02-07T15:47:59-05:00|Dicklesworthstone|spec: define .db-fec physical layout + crash-consistent update
30203fb1f91acfb45c6085432a66029896e26e66|30203fb|2026-02-07T15:50:22-05:00|Dicklesworthstone|spec: reserve TxnId sentinels + guard allocation
75ac25db5853747e3f56877c943d51ce8d649a5e|75ac25d|2026-02-07T16:07:05-05:00|Dicklesworthstone|spec: harden TxnId alloc + replication changeset id + ARC singleflight
ec9adc1a8f61663d224399711d165a7dd623b919|ec9adc1|2026-02-07T16:13:12-05:00|Dicklesworthstone|spec: fix TxnId monotonicity note + clarify P_eff
e80fdde018b3ce359f49271f082366bbad19f928|e80fdde|2026-02-07T16:14:32-05:00|Dicklesworthstone|spec: deterministic RaptorQ seed for ChangesetId
fa25db0b24e84470e8271309370e0f8093463ddd|fa25db0|2026-02-07T16:21:05-05:00|Dicklesworthstone|spec: adopt NGQP beam search for V1 join ordering
1d8bbfb40fa7a1fe90e710b593dfa2b3e9113c2d|1d8bbfb|2026-02-07T16:22:50-05:00|Dicklesworthstone|spec: add TxnId CAS abort path and correct beam search complexity
4432a3daa2f875bc2d3fe47caf783b3eb286ebfe|4432a3d|2026-02-07T16:25:52-05:00|Dicklesworthstone|spec: conformance mode matrix; bump asupersync
aa8e81601a80b6910a3791490d07c5691b517850|aa8e816|2026-02-07T16:28:25-05:00|Dicklesworthstone|spec: tighten serialized FCW + schema_epoch open + rebase read footprint
0a8d8676ac74f994535eceaa1c6c47cb17868038|0a8d867|2026-02-07T16:38:09-05:00|Dicklesworthstone|spec: fix TxnSlot cleanup crash-safety and reconcile lock/VFS semantics
3d568547fd6b8036481df0d73a2c60b5bf30d5a7|3d56854|2026-02-07T16:39:53-05:00|Dicklesworthstone|spec: fix Vfs trait formatting and cleanup_txn_id comment
4c07e10c56ee30db5d03453237ae6c29c31a7cf0|4c07e10|2026-02-07T16:41:55-05:00|Dicklesworthstone|spec: clarify TxnSlot cleanup_txn_id + fix Vfs trait formatting
df0313b00f2854929c376434c56129ef6e2740e0|df0313b|2026-02-07T16:42:11-05:00|Dicklesworthstone|spec: fix ARC/CAR comment indentation
97df1f07893def20701570db65418ff75ed45656|97df1f0|2026-02-07T16:42:40-05:00|Dicklesworthstone|spec: clarify zero-copy terminology
bbc4a3114572200d7a8a674eb3a4e430ae1d0b47|bbc4a31|2026-02-07T16:45:48-05:00|Dicklesworthstone|spec: define canonical AAD encoding for page encryption
4363f50065239e47d56f12250d933f5a1ab75a00|4363f50|2026-02-07T16:51:53-05:00|Dicklesworthstone|spec: add critical implementation controls checklist
d9021cffb65692624f3990ae2544a96ae0c07021|d9021cf|2026-02-07T16:52:38-05:00|Dicklesworthstone|spec: clarify rebase rowid reuse + DatabaseId encoding
29107df6b155bb20934ef369b934699191076b32|29107df|2026-02-07T17:00:26-05:00|Dicklesworthstone|spec: harden TxnSlot cleanup and epoch reset semantics
f708f338cb6e435db3822e2aa81b2785b51bf39b|f708f33|2026-02-07T17:01:38-05:00|Dicklesworthstone|spec: clarify pipelined durability and compatibility spill semantics
a71e1d95715c1190335e8738413382b31b6d167c|a71e1d9|2026-02-07T17:07:05-05:00|Dicklesworthstone|spec: harden ECS root update; snapshot slot tid; clarify ESCAPE parsing
120eee252b4caa7fe6389602bd2c8a316c6cee2a|120eee2|2026-02-07T17:08:11-05:00|Dicklesworthstone|spec: strengthen WAL-FEC per-source validation hash to xxh3_128
	975f65c78a5745424665a95f0c222287306c9dd5|975f65c|2026-02-07T17:08:59-05:00|Dicklesworthstone|spec: clarify GF(256) elimination note; bound delta reconstruction cost
	24b6f60e9e751b699cb232759d85a06c42792019|24b6f60|2026-02-07T17:15:05-05:00|Dicklesworthstone|spec: fix GC scheduling cross-reference
	80decf6b8ba71dd4331f559a08ee0fba3fbdf4bb|80decf6|2026-02-07T17:28:31-05:00|Dicklesworthstone|spec: clarify db-fec generation digest + ESI terminology
	7cc726325faa5cad71d2132480cbb68fb046563d|7cc7263|2026-02-07T18:11:25-05:00|Dicklesworthstone|spec: harden SHM snapshot seqlock + compat db-fec freshness
	9ad50ae2a2c7fce76a75d1200e22374eb729914d|9ad50ae|2026-02-07T18:15:58-05:00|Dicklesworthstone|spec: define SHM snapshot seqlock + coordinator IPC
	19106d19e531ea75918241b2858024c34a73f037|19106d1|2026-02-07T18:20:51-05:00|Dicklesworthstone|spec: harden MVCC TxnSlot protocol, write_page idempotency, and SHM layout
	7313951174e317f889851f696de264b546e4b54e|7313951|2026-02-07T18:21:25-05:00|Dicklesworthstone|spec: define wire payload schemas, RowId allocator state, and CommitRequest type
	d329df055a1a13459d0f507bc222b05525bb7522|d329df0|2026-02-07T18:28:22-05:00|Dicklesworthstone|spec: fix snapshot seqlock + shm invariants
	351c282e9a9a2ee118496651cef7a4b33cf309f2|351c282|2026-02-07T18:41:22-05:00|Dicklesworthstone|spec: fix TxnSlot acquire pseudocode + add spec viz wasm
	b1c1e72d9331952247c804af6589fe3b349d0b8a|b1c1e72|2026-02-07T18:58:48-05:00|Dicklesworthstone|spec: tighten coordinator IPC framing
	e60049751d7e238e46e45ce8009d9e9f053a41c2|e600497|2026-02-07T19:15:51-05:00|Dicklesworthstone|spec: harden claiming liveness and IPC ordering
	6d5d36a1380d3b4b4b6ddba71828825ef942975b|6d5d36a|2026-02-07T19:16:12-05:00|Dicklesworthstone|spec: update Round 16 audit notes`;
      const COMMIT_STATS_RAW = String.raw`c08f1602d03b1833a4f91c8f77347f8f196bac9d|8628|0
f9d88aa96f50893f8531373d0734597ebae3c592|431|76
76eef31be20a6faedc62e7fecce88e78652ebdcf|242|116
9800b17df4a56c2dc065cf566c2810d4ed2e576c|537|74
79018f161649e6d7977bc74eb849a0e22eceb286|246|151
5ad34871f7242de61378843c6c1e8311e35d9fa3|262|45
7b2c677cf61adda977e71524b59d7ec234137962|425|36
bf0426417685504bb2b2f5acfc4de2c2f087ef8b|290|71
05cf07847753f444b9f74dfd0279ef96c3ff86b9|217|31
e8ddf469d8ab8ff8eefbae92b24453e06fcf4627|141|20
480c10086fb73630744364f5fb34966de2572dc1|60|48
0313678eff44b70cd4ea40cabc2f2222cac2ba93|169|73
b97b1abe4727a738197180c5639738c1da2f5311|79|170
643c89c1c941d1e47cb6a2574e01fd7556f622ec|37|59
0404e42c9dbdc677da9b8a5731d29290ef9f3b26|33|16
2a6353d2f77a050454d2a6fd4b3fd6c8363064e9|643|381
3253050f2835fb305bd115bd9f5939b72cd46503|404|86
06dfe9a709d61fc899e00ee33d54dc15d003bb48|318|424
63f6057e8705b7105837952bc8ea977ad6f34da6|14|14
2cff3a0287f1de30d2fd1185c376ac81913922c9|1|1
4ba1ee3bce5eef69acf5e24c61e0fe5482662a79|18|16
72eb835e83063d8fe19c1288a8c4624438473b2e|56|16
0b543b5b587d72a5531a37324ad31fab0157b6ed|472|64
d7b38efedc6f77029ce412c1eea2d0197b8b8fc6|78|39
322af1706478c8da09b09af4927e6a011041c5a6|21|1
1a672dd545885d61c2531bf2be178d0edb4a5065|449|51
a19a3acb957598db6e11ece9ab199ae8f34eb88e|47|21
d0fda0186411a7f75bd73771f484259c29d7921a|6|3
b8344dba9b295b31b843807f80c6ec10e601459b|6|2
5e2344ce7b5640aaad2fb48bed85838f9c9ecfef|81|11
713e6dc4f8b202870b16e154a597bcd735d5c05b|18|1
e2dc2232567e901ce8f7192303c9da013e4c8d7c|40|3
3c5d5a177c8fbd78fd6f11f32a02e4c3ada9e172|18|7
714570a488c9e93f3349ba527e70e744570555e9|16|8
893691fc58cca08e1fba3a25a5ddf520624d6860|26|14
a9aff80df7c0d04f465db40b74165a2e75ae5e29|14|10
80bc4494162ff420cdb48c5122324d9bdc8d328b|33|5
a5a85e64c6356176ad86795d69e1293bd76f2601|7|5
7c93dbb54e2d939f820d97a469bf204829ce97c8|89|34
07236c61ff86a87f22bb1aa6fd26885eb527d268|43|41
d9146f771541d7e134f540e11016df82cc6e17b9|264|207
30619b34ea187d682283f8cd137d8d30e306e6bd|16|8
7e9cace601485722e7b31411ad74796c0685bd05|17|3
ff937f3d678f88eb855051dbbd32e8e5046bea0f|29|6
61ee3e03f4cd5e44b189da55bb70e91cdba63a29|15|2
a90a3794616e12b083f88276e3cb47a045f3ac10|32|8
246102e6b4266fa15000580a3d016d76bcb1c801|336|117
5ea1b6fb02945e4bc147c37ae7d054d92fdd9045|90|26
f1f25abe3b40ad0fb8704eaee310aee17de78d25|96|14
9725f136b2450c6411f5735a1575a068f2aacc0b|66|48
56742886aae65bdca3a34acda8d677b52a0c241d|14|3
71a41750d478e8ecad031a2f490e84a4424a839e|5|1
9fa8f7b6082f164d4cb1026296a360cfbafe1f73|8|3
86d63af874d7a3dc7d522d47bf5f004f12913adb|99|2
23f575f7d4ee20399865e33abca1f93e60eda2d7|17|1
6228e273d3dce6ca67c97fd43afb9018d19c6f5f|81|5
2e3ea218a908f4a22791b4c8b30a0dbf746f867b|130|93
d05de23ea89da924e86232c8da519b6b58b4a0d5|15|8
fa1830e4593854a492d78bec24da1db90351c2f0|8|7
70436b5c2c04e42c0aaf4b776a8f779416abdeff|9|3
a4e773481bd417530fe83f33868ef6ce5656e933|98|11
09c095901ddb1e5db47f55ddf5c616b5d51bf74e|18|0
40a2ac7739778b30cb11129a782d8746bd13e6ed|1|1
40c6c3d152cf7d86104ad9ca19fbb850d6fdf343|3|2
8512e62aed8835864a8800ed895c91a8773dc996|57|27
e31897fd92088e5305735d080e224a23b7315a11|1|1
b03db7bc0b8a2705fe6fda19f5f149452755284c|18|8
dd190e4f7f24aa2a8b7bb1bbc7020aead6d014c0|478|55
a664265e3f5876177b7c1410dad7119e01e7f9e9|410|93
599cafb3f29c52dd2a44f27d5632032c54a34a25|70|12
e42a43de57ff5d4ee616cd2ec3b54a0db249a3e6|15|5
f158b44a5bd96ef34b378b58f61399568424f6d4|14|6
3cf0f13571c101725f9ef34ce2476c90478ff07d|22|17
0cb22962ec87cb29a29a44acfc5ec14588aadb23|80|16
1680d6931f5773673fae021b7cc646e3f3faab44|47|28
1e2aae9940969190c6b9bebe94c94749eee91626|228|38
da22f479dc2f8d1e98294750b741b3cce168fd19|373|88
c25f0d00238fc36f355f818f571abc3d4a1198df|27|7
02e48a41810bdb2ad471926d9728a9a9af12b155|11|4
bcd893da081fd15b1614a05c4d8b593f55cf7011|379|134
6da51572d2b5b76eb9a37ffa99da76bfb850cd83|4|4
22e75e65b831adda852df3a11628e793aaa9e7a9|214|60
2f0970b9a5d8fc755269ba848169f5a1369ec6a6|8|6
5cc32a6bc813ff59db231f29db5b95a13ea94704|56|1
dc92e549d05ac8dc0c23937d6e6dfa874729fdd4|17|15
2433562451b188c9233f6fd83a9ef7573ce48f97|24|5
82dfd4bf99283e5ab283073651787b3c7975561f|30|5
ab20d7fac6f83ac527475927aa53b204c11ed7c8|26|1
96f32f3174453e68960d849672cb4b7a8d33d1c7|18|6
f37158f2468145272817fd65794d78be539be19b|9|1
19fa01f6d66c19892c634a02da44f7f536b29e43|24|8
a3e7ae521e6a0dcb3cf9489ac5a67b0f62aad526|133|68
56a4e91425fa5cba9aecf08f786691ef03563750|26|0
d2a49862864dff9799e236d2cf81b51ed268f93f|79|28
859e81752809cd343b00ad6499c159d2f341cc46|6|1
be3d256b12679a32f2fb4e44eeb82e513607b284|176|9
65ab2f709f724750878f2cb5d045649bf0c53e89|267|56
63ee097eb68a72ac23effab3979978adafb5b411|10|0
29f7ebe942a43891f2a55fd998d02c37411efc9f|262|65
6b0c12fc40c5759f5d53028ba692e8aaff80fc92|3|1
b181b6d148e02862b195824020954abadae8de88|2|2
d302b391af5c2932c9097dd85923a8535593e45e|229|45
017745631c79f0c9061e4fdba8d0ce09ecc6c86d|12|2
5dae90d79aef7d70300fe53a17f9e40dba24b309|5|1
ca60e008352585f2501ca3f81337849396b4d140|51|0
30203fb1f91acfb45c6085432a66029896e26e66|6|2
75ac25db5853747e3f56877c943d51ce8d649a5e|116|51
ec9adc1a8f61663d224399711d165a7dd623b919|16|8
e80fdde018b3ce359f49271f082366bbad19f928|12|3
fa25db0b24e84470e8271309370e0f8093463ddd|78|34
1d8bbfb40fa7a1fe90e710b593dfa2b3e9113c2d|6|2
4432a3daa2f875bc2d3fe47caf783b3eb286ebfe|119|26
aa8e81601a80b6910a3791490d07c5691b517850|45|20
0a8d8676ac74f994535eceaa1c6c47cb17868038|261|135
3d568547fd6b8036481df0d73a2c60b5bf30d5a7|16|16
4c07e10c56ee30db5d03453237ae6c29c31a7cf0|78|57
df0313b00f2854929c376434c56129ef6e2740e0|5|5
97df1f07893def20701570db65418ff75ed45656|4|0
bbc4a3114572200d7a8a674eb3a4e430ae1d0b47|3|0
4363f50065239e47d56f12250d933f5a1ab75a00|44|2
d9021cffb65692624f3990ae2544a96ae0c07021|5|4
29107df6b155bb20934ef369b934699191076b32|109|166
f708f338cb6e435db3822e2aa81b2785b51bf39b|242|100
a71e1d95715c1190335e8738413382b31b6d167c|178|105
	120eee252b4caa7fe6389602bd2c8a316c6cee2a|6|6
	975f65c78a5745424665a95f0c222287306c9dd5|17|2
	24b6f60e9e751b699cb232759d85a06c42792019|3|3
	80decf6b8ba71dd4331f559a08ee0fba3fbdf4bb|15|10
	7cc726325faa5cad71d2132480cbb68fb046563d|781|148
	9ad50ae2a2c7fce76a75d1200e22374eb729914d|160|23
	19106d19e531ea75918241b2858024c34a73f037|87|21
	7313951174e317f889851f696de264b546e4b54e|183|20
	d329df055a1a13459d0f507bc222b05525bb7522|126|43
	351c282e9a9a2ee118496651cef7a4b33cf309f2|50|45
	b1c1e72d9331952247c804af6589fe3b349d0b8a|56|11
	e60049751d7e238e46e45ce8009d9e9f053a41c2|81|25
	6d5d36a1380d3b4b4b6ddba71828825ef942975b|1|1`;

      // -----------------------------
      // Data: manual classification
      // -----------------------------

      const CLASS_EARLY = [
  {
    "commit": "c08f1602d03b1833a4f91c8f77347f8f196bac9d",
    "change_groups": [
      {
        "summary": "Introduced the comprehensive V1 specification as a new, authoritative single-source document covering architecture, MVCC, RaptorQ, file format, SQL coverage, and testing.",
        "categories": [
          6,
          4,
          7,
          8,
          9
        ],
        "primary_category": 6,
        "confidence": 0.62,
        "evidence": [
          "new file mode 100644",
          "+# COMPREHENSIVE SPECIFICATION FOR FRANKENSQLITE V1"
        ],
        "changed_headings": [
          "0. How to Read This Document",
          "1. Project Identity"
        ]
      }
    ]
  },
  {
    "commit": "f9d88aa96f50893f8531373d0734597ebae3c592",
    "change_groups": [
      {
        "summary": "Promoted SSI to V1 concurrent mode and expanded MVCC structures with SIREAD tracking, sharded lock tables, and intent logs for deterministic rebase and merge policy.",
        "categories": [
          1,
          4,
          7,
          8
        ],
        "primary_category": 4,
        "confidence": 0.72,
        "evidence": [
          "+**Layer 2 (Ship in V1): MVCC concurrent mode with SSI (Serializable by Default).**",
          "+in_flight       : RoaringBitmap"
        ],
        "changed_headings": [
          "2.4 The Solution: Layered Isolation",
          "5. MVCC Formal Model",
          "5.10 Algebraic Write Merging and Intent Logs"
        ]
      },
      {
        "summary": "Added explicit crash model and formalized Compatibility vs Native operating modes with durable marker rules.",
        "categories": [
          4,
          6,
          7
        ],
        "primary_category": 4,
        "confidence": 0.64,
        "evidence": [
          "+### 7.9 Crash Model (Explicit Contract):",
          "+### 7.10 Two Operating Modes"
        ],
        "changed_headings": [
          "7.9 Crash Model (Explicit Contract)",
          "7.10 Two Operating Modes"
        ]
      },
      {
        "summary": "Threaded `Cx` through storage and pager traits to make I/O and blocking operations cancelable and deadline-aware.",
        "categories": [
          7,
          3
        ],
        "primary_category": 7,
        "confidence": 0.58,
        "evidence": [
          "+fn open(&self, cx: &Cx, path: Option<&Path>, flags: VfsOpenFlags)",
          "+fn commit(&self, cx: &Cx, txn: Transaction) -> Result<()>"
        ],
        "changed_headings": [
          "9. Trait Hierarchy",
          "9.1 Storage Traits"
        ]
      }
    ]
  },
  {
    "commit": "76eef31be20a6faedc62e7fecce88e78652ebdcf",
    "change_groups": [
      {
        "summary": "Reframed RaptorQ decoding guarantees to avoid hard-coded probabilities and require recoverable failure handling with monitoring.",
        "categories": [
          8,
          1,
          9
        ],
        "primary_category": 8,
        "confidence": 0.67,
        "evidence": [
          "+**Decoding Failure Behavior (Normative):**",
          "+Correctness MUST NOT depend on decoding succeeding with exactly `K` symbols."
        ],
        "changed_headings": [
          "3.2 How RaptorQ Works (Essential Understanding)"
        ]
      },
      {
        "summary": "Corrected SQLite legacy behaviors and details (WAL reader limit and integer overflow semantics).",
        "categories": [
          2,
          1
        ],
        "primary_category": 2,
        "confidence": 0.6,
        "evidence": [
          "+caps the number of simultaneously active reader locks via `WAL_NREADER`",
          "+promote to REAL (floating point) rather than wrapping."
        ],
        "changed_headings": [
          "2.1 The Problem"
        ]
      },
      {
        "summary": "Aligned asupersync module paths and API examples; clarified feature-flag placement and harness notes.",
        "categories": [
          3,
          5,
          9
        ],
        "primary_category": 3,
        "confidence": 0.58,
        "evidence": [
          "+src/raptorq/gf256.rs        -- GF(256) arithmetic",
          "+Feature flags MUST live on a real package manifest"
        ],
        "changed_headings": [
          "3.3 Asupersync's RaptorQ Implementation",
          "8.5 Feature Flags"
        ]
      }
    ]
  },
  {
    "commit": "9800b17df4a56c2dc065cf566c2810d4ed2e576c",
    "change_groups": [
      {
        "summary": "Added non-negotiable scope doctrine, normative language rules, and a formal glossary to eliminate V1-scope escape hatches.",
        "categories": [
          6,
          5,
          9
        ],
        "primary_category": 6,
        "confidence": 0.63,
        "evidence": [
          "+### 0.1 Non-Negotiable Scope Doctrine",
          "+### 0.2 Normative Language",
          "+### 0.3 Glossary"
        ],
        "changed_headings": [
          "0.1 Non-Negotiable Scope Doctrine",
          "0.2 Normative Language",
          "0.3 Glossary"
        ]
      },
      {
        "summary": "Specified ECS substrate details and made erasure-coded page storage fully in-scope.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.6,
        "evidence": [
          "+### 3.5 ECS: The Erasure-Coded Stream Substrate",
          "+#### 3.5.1 ObjectId: Content-Addressed Identity"
        ],
        "changed_headings": [
          "3.5 ECS: The Erasure-Coded Stream Substrate"
        ]
      },
      {
        "summary": "Introduced multi-process MVCC shared-memory coordination and cross-process lock table protocol.",
        "categories": [
          7,
          4
        ],
        "primary_category": 7,
        "confidence": 0.6,
        "evidence": [
          "+#### 5.6.1 Shared-Memory Coordination Region",
          "+#### 5.6.3 Cross-Process Page Lock Table"
        ],
        "changed_headings": [
          "5.6.1 Shared-Memory Coordination Region",
          "5.6.3 Cross-Process Page Lock Table"
        ]
      }
    ]
  },
  {
    "commit": "79018f161649e6d7977bc74eb849a0e22eceb286",
    "change_groups": [
      {
        "summary": "Corrected WAL compatibility by moving RaptorQ repair data to a `.wal-fec` sidecar; standard WAL frames remain byte-for-byte SQLite-compatible.",
        "categories": [
          2,
          7
        ],
        "primary_category": 2,
        "confidence": 0.7,
        "evidence": [
          "+Standard SQLite WAL frames are exactly 24 bytes (header) + page_size (data).",
          "+Instead, we use a **sidecar file** (`.wal-fec`) to store repair symbols."
        ],
        "changed_headings": [
          "Concrete WAL Commit Frame Layout (Compatibility Mode)"
        ]
      }
    ]
  },
  {
    "commit": "5ad34871f7242de61378843c6c1e8311e35d9fa3",
    "change_groups": [
      {
        "summary": "Added RaptorQ Everywhere doctrine, expanded glossary/TOC, and tightened mechanical sympathy constraints.",
        "categories": [
          6,
          4,
          9
        ],
        "primary_category": 6,
        "confidence": 0.62,
        "evidence": [
          "+### 0.4 What \"RaptorQ Everywhere\" Means (No Weasel Words)",
          "+| **CommitSeq** | Monotonically increasing `u64` commit sequence number"
        ],
        "changed_headings": [
          "0.4 What \"RaptorQ Everywhere\" Means"
        ]
      },
      {
        "summary": "Overhauled WAL-FEC sidecar model and ECS/replication architecture to unify repair symbols and replication into ECS objects.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.64,
        "evidence": [
          "+.wal contains ONLY standard SQLite WAL frames (source symbols);",
          "+### 3.4.7 Replication Architecture"
        ],
        "changed_headings": [
          "3.4.7 Replication Architecture"
        ]
      }
    ]
  },
  {
    "commit": "7b2c677cf61adda977e71524b59d7ec234137962",
    "change_groups": [
      {
        "summary": "Applied alien-artifact discipline: decision-theoretic SSI policy, VOI-driven granularity investment, BOCPD monitoring, and formal durability theorems.",
        "categories": [
          8,
          1
        ],
        "primary_category": 8,
        "confidence": 0.66,
        "evidence": [
          "+**Decision-Theoretic SSI Abort Policy (Alien-Artifact Discipline).**",
          "+### 4.8 Bayesian Online Change-Point Detection (BOCPD)"
        ],
        "changed_headings": [
          "4.8 Bayesian Online Change-Point Detection (BOCPD)",
          "5.7 SSI"
        ]
      },
      {
        "summary": "Added engineering guardrails: memory accounting, hash-tier strategy, and native-mode commit/recovery protocol details.",
        "categories": [
          7,
          4
        ],
        "primary_category": 7,
        "confidence": 0.56,
        "evidence": [
          "+### 6.9 Memory Accounting",
          "+### 7.11 Native Mode Commit Protocol"
        ],
        "changed_headings": [
          "6.9 Memory Accounting",
          "7.11 Native Mode Commit Protocol"
        ]
      }
    ]
  },
  {
    "commit": "bf0426417685504bb2b2f5acfc4de2c2f087ef8b",
    "change_groups": [
      {
        "summary": "Replaced ephemeral SIREAD tables with a RaptorQ-native SSI witness plane (hot+cold planes, durable ECS evidence, cross-process support).",
        "categories": [
          4,
          7,
          8
        ],
        "primary_category": 4,
        "confidence": 0.68,
        "evidence": [
          "+| **WitnessKey** | The canonical key-space for SSI read/write evidence",
          "+SSIWitnessPlane := (see Section 5.6.4)"
        ],
        "changed_headings": [
          "5.6.4 SSI Witness Plane"
        ]
      }
    ]
  },
  {
    "commit": "05cf07847753f444b9f74dfd0279ef96c3ff86b9",
    "change_groups": [
      {
        "summary": "Replaced magic-number defaults with formal derivations and cost models for tunable parameters (delta threshold, BOCPD priors, GC scheduling, etc.).",
        "categories": [
          8,
          1
        ],
        "primary_category": 8,
        "confidence": 0.7,
        "evidence": [
          "+// COST MODEL (Extreme Optimization Discipline):",
          "+P(r_t | x_{1:t}) proportional to sum_{r_{t-1}} ..."
        ],
        "changed_headings": [
          "3.4.4 Delta Threshold",
          "4.8 BOCPD",
          "5.6.5 GC Coordination"
        ]
      }
    ]
  },
  {
    "commit": "e8ddf469d8ab8ff8eefbae92b24453e06fcf4627",
    "change_groups": [
      {
        "summary": "Introduced performance-oriented data structures (VersionArena, AppendOnlyVec, CAR cache) and per-invariant monitoring calibration.",
        "categories": [
          7,
          8
        ],
        "primary_category": 7,
        "confidence": 0.63,
        "evidence": [
          "+VersionArena",
          "+CommitLog := AppendOnlyVec<CommitRecord>",
          "+CAR (Clock with Adaptive Replacement)"
        ],
        "changed_headings": [
          "5.5 MVCC Data Structures",
          "6. ARC Cache",
          "4.3 E-process Calibration"
        ]
      }
    ]
  },
  {
    "commit": "480c10086fb73630744364f5fb34966de2572dc1",
    "change_groups": [
      {
        "summary": "Updated SSI transaction struct to use TxnToken edges and added eager abort semantics plus witness-plane-aware commit-time detection pseudocode.",
        "categories": [
          4,
          1
        ],
        "primary_category": 4,
        "confidence": 0.62,
        "evidence": [
          "+marked_for_abort : bool",
          "+**Commit-time detection + proof emission pseudocode (witness plane):**"
        ],
        "changed_headings": [
          "5.7 SSI Algorithm"
        ]
      }
    ]
  },
  {
    "commit": "0313678eff44b70cd4ea40cabc2f2222cac2ba93",
    "change_groups": [
      {
        "summary": "Clarified SSI detection algorithm as a two-pass pivot/completion check with eager abort marking.",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.6,
        "evidence": [
          "+The on_commit(T) pseudocode specifies the two-pass check:",
          "+on_commit(T):"
        ],
        "changed_headings": [
          "5.7 SSI Algorithm"
        ]
      },
      {
        "summary": "Adjusted object identity and Cx capability descriptions (ObjectId truncation and richer capability mechanics).",
        "categories": [
          4,
          3,
          9
        ],
        "primary_category": 4,
        "confidence": 0.52,
        "evidence": [
          "+| **ObjectId** | Content-addressed identifier: 128-bit `Trunc128(BLAKE3(...))`",
          "+**Cooperative cancellation + progress checkpoints**"
        ],
        "changed_headings": [
          "0.3 Glossary",
          "4.2 Asupersync Integration"
        ]
      }
    ]
  },
  {
    "commit": "b97b1abe4727a738197180c5639738c1da2f5311",
    "change_groups": [
      {
        "summary": "Removed merge artifacts by restoring canonical ObjectId size and concise Cx glossary/API examples.",
        "categories": [
          5,
          9
        ],
        "primary_category": 5,
        "confidence": 0.66,
        "evidence": [
          "-| **ObjectId** | Content-addressed identifier: 128-bit `Trunc128(...)",
          "+| **ObjectId** | Content-addressed identifier: `BLAKE3(canonical_encoding(object))`, 32 bytes."
        ],
        "changed_headings": [
          "0.3 Glossary"
        ]
      }
    ]
  },
  {
    "commit": "643c89c1c941d1e47cb6a2574e01fd7556f622ec",
    "change_groups": [
      {
        "summary": "Restored canonical SSI detection algorithm and in-process edge representation after merge artifacts.",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.64,
        "evidence": [
          "+has_incoming_rw : bool",
          "+**Detection algorithm pseudocode:**"
        ],
        "changed_headings": [
          "5.7 SSI Algorithm"
        ]
      }
    ]
  },
  {
    "commit": "0404e42c9dbdc677da9b8a5731d29290ef9f3b26",
    "change_groups": [
      {
        "summary": "Integrated SSI witness-plane evidence into native-mode commit protocol, adding CommitProof and new ordering constraints.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.63,
        "evidence": [
          "+`proof_object_id`",
          "+**Critical ordering:** Marker publication MUST happen AFTER capsule durability and AFTER `CommitProof` durability is satisfied."
        ],
        "changed_headings": [
          "7.11 Native Mode Commit Protocol"
        ]
      }
    ]
  },
  {
    "commit": "2a6353d2f77a050454d2a6fd4b3fd6c8363064e9",
    "change_groups": [
      {
        "summary": "Aligned asupersync integration terminology, witness-plane naming, and ObjectId truncation details.",
        "categories": [
          3,
          9,
          4
        ],
        "primary_category": 3,
        "confidence": 0.6,
        "evidence": [
          "+| **ObjectId** | Content-addressed identifier: `Trunc128(BLAKE3(\"fsqlite:ecs:v1\" || canonical_object_header || payload_hash))`",
          "+| **SIREAD witness (legacy term)** | ... represented by `ReadWitness` objects"
        ],
        "changed_headings": [
          "0.3 Glossary"
        ]
      }
    ]
  },
  {
    "commit": "3253050f2835fb305bd115bd9f5939b72cd46503",
    "change_groups": [
      {
        "summary": "Added systematic symbol fast-path flags and tiered storage semantics for ECS objects.",
        "categories": [
          7,
          4
        ],
        "primary_category": 7,
        "confidence": 0.6,
        "evidence": [
          "+flags       : u8,           -- bitflags (see below)",
          "+**Systematic read fast path (hybrid decode):**"
        ],
        "changed_headings": [
          "3.5.2 Symbol Record Envelope",
          "3.5.11 Tiered Storage (\"Bottomless\", Native Mode)"
        ]
      },
      {
        "summary": "Introduced SchemaEpoch into MVCC snapshot to forbid intent replay across schema changes.",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.56,
        "evidence": [
          "+SchemaEpoch := u64",
          "+schema_epoch    : SchemaEpoch"
        ],
        "changed_headings": [
          "5.1 Core Types"
        ]
      }
    ]
  },
  {
    "commit": "06dfe9a709d61fc899e00ee33d54dc15d003bb48",
    "change_groups": [
      {
        "summary": "Updated core MVCC types to CommitSeq-based snapshots and formalized SSI witness plane in place of SIREAD tables.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.62,
        "evidence": [
          "+CommitSeq   := u64",
          "+SSIWitnessPlane := (see Section 5.6.4)"
        ],
        "changed_headings": [
          "5.1 Core Types"
        ]
      }
    ]
  },
  {
    "commit": "63f6057e8705b7105837952bc8ea977ad6f34da6",
    "change_groups": [
      {
        "summary": "Changed MVCC ARC cache keys from TxnId to CommitSeq and updated invariants accordingly.",
        "categories": [
          7,
          1
        ],
        "primary_category": 7,
        "confidence": 0.62,
        "evidence": [
          "-Standard ARC keys on page number. Our variant keys on `(PageNumber, TxnId)`",
          "+Standard ARC keys on page number. Our variant keys on `(PageNumber, CommitSeq)`"
        ],
        "changed_headings": [
          "6.2 MVCC-Aware ARC Data Structures"
        ]
      }
    ]
  },
  {
    "commit": "2cff3a0287f1de30d2fd1185c376ac81913922c9",
    "change_groups": [
      {
        "summary": "Clarified the INV-1 monitor label to include CommitSeq monotonicity.",
        "categories": [
          9,
          5
        ],
        "primary_category": 9,
        "confidence": 0.8,
        "evidence": [
          "+EProcess::new(\"INV-1: TxnId/CommitSeq Monotonicity\", EProcessConfig {"
        ],
        "changed_headings": [
          "4.3 E-process Monitoring"
        ]
      }
    ]
  },
  {
    "commit": "4ba1ee3bce5eef69acf5e24c61e0fe5482662a79",
    "change_groups": [
      {
        "summary": "Tightened glossary entries and fixed algebraic merge pseudocode types plus MVCC module descriptions.",
        "categories": [
          9,
          5,
          1
        ],
        "primary_category": 9,
        "confidence": 0.62,
        "evidence": [
          "+| **CommitMarker** | The durable \"this commit exists\" record in Native mode: `(commit_seq, commit_time_unix_ns, capsule_object_id, proof_object_id, prev_marker, integrity_hash)`.",
          "+page_T2_committed: &PageData"
        ],
        "changed_headings": [
          "0.3 Glossary",
          "5.10 Algebraic Write Merge"
        ]
      }
    ]
  },
  {
    "commit": "72eb835e83063d8fe19c1288a8c4624438473b2e",
    "change_groups": [
      {
        "summary": "Corrected cross-process TxnSlot cleanup to defend against PID reuse and formalized lock-table tombstone semantics.",
        "categories": [
          7,
          1
        ],
        "primary_category": 7,
        "confidence": 0.65,
        "evidence": [
          "+pid_birth       : AtomicU64",
          "+TOMBSTONE := 0xFFFF_FFFF is reserved."
        ],
        "changed_headings": [
          "5.6.2 TxnSlot",
          "5.6.3 Cross-Process Page Lock Table"
        ]
      }
    ]
  },
  {
    "commit": "0b543b5b587d72a5531a37324ad31fab0157b6ed",
    "change_groups": [
      {
        "summary": "Expanded asupersync integration glossary with Budget/Outcome/Region semantics and structured concurrency behavior.",
        "categories": [
          3,
          6
        ],
        "primary_category": 3,
        "confidence": 0.6,
        "evidence": [
          "+| **Budget** | Asupersync resource budget (product semiring) carried by `Cx`",
          "+| **Region** | Asupersync structured concurrency scope"
        ],
        "changed_headings": [
          "0.3 Glossary"
        ]
      },
      {
        "summary": "Corrected RaptorQ failure probability claims and clarified rules of thumb using RFC 6330 Annex B data.",
        "categories": [
          1,
          8
        ],
        "primary_category": 1,
        "confidence": 0.6,
        "evidence": [
          "+**Caution on failure probability claims:**",
          "+Decoding with **exactly K** received symbols: ~99% success (P_fail < 0.01)."
        ],
        "changed_headings": [
          "3.1 RaptorQ Foundation"
        ]
      }
    ]
  },
  {
    "commit": "d7b38efedc6f77029ce412c1eea2d0197b8b8fc6",
    "change_groups": [
      {
        "summary": "Deep audit fixes to MVCC formal model: CommitLog definition, SSI inequality correction, padding alignment, and cost model adjustments.",
        "categories": [
          1,
          7
        ],
        "primary_category": 1,
        "confidence": 0.66,
        "evidence": [
          "+CommitLog := AppendOnlyVec<CommitRecord>",
          "+_padding        : [u8; 64]"
        ],
        "changed_headings": [
          "5.1 Core Types",
          "5.6 Shared Memory"
        ]
      }
    ]
  },
  {
    "commit": "322af1706478c8da09b09af4927e6a011041c5a6",
    "change_groups": [
      {
        "summary": "Added normative memory-ordering rules for CommitSeq visibility and clarified SSI pivot abort overapproximation.",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.64,
        "evidence": [
          "+MUST be incremented with `Release` ordering AFTER all version chain updates",
          "+**Note (deliberate overapproximation):**"
        ],
        "changed_headings": [
          "5.4 Visibility",
          "5.7 SSI Algorithm"
        ]
      }
    ]
  },
  {
    "commit": "1a672dd545885d61c2531bf2be178d0edb4a5065",
    "change_groups": [
      {
        "summary": "Corrected checksum/format details: WAL magic endianness, integrity check scoping, and symbol overhead interpretation.",
        "categories": [
          2,
          1
        ],
        "primary_category": 2,
        "confidence": 0.62,
        "evidence": [
          "+0x377F0682 (bit 0 = 0) = little-endian word order",
          "+loss_fraction_max approx max(0, (R - slack) / (K_source + R))"
        ],
        "changed_headings": [
          "11. Checksums and Integrity"
        ]
      },
      {
        "summary": "Added family-wise error control for multiple e-process monitors using e-value aggregation.",
        "categories": [
          8
        ],
        "primary_category": 8,
        "confidence": 0.56,
        "evidence": [
          "+E_global(t) := prod_i E_i(t)^{w_i}",
          "+sum_i alpha_i <= alpha_total"
        ],
        "changed_headings": [
          "4.3 E-process Monitoring"
        ]
      }
    ]
  },
  {
    "commit": "a19a3acb957598db6e11ece9ab199ae8f34eb88e",
    "change_groups": [
      {
        "summary": "Fixed crate layering to avoid dependency cycles and clarified mvcc/wal relationships.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.6,
        "evidence": [
          "+fsqlite-mvcc moved from Layer 6 to Layer 3.",
          "+Checkpoint now receives `&dyn CheckpointPageWriter` at runtime"
        ],
        "changed_headings": [
          "8. Architecture: Crate Map and Dependencies"
        ]
      },
      {
        "summary": "Corrected SQLite expression precedence and VDBE p5 flag semantics.",
        "categories": [
          2
        ],
        "primary_category": 2,
        "confidence": 0.58,
        "evidence": [
          "+**IMPORTANT:** `NOT` is NOT at the same precedence as unary `~`/`+`/`-`.",
          "+upper 8 bits MUST be zero."
        ],
        "changed_headings": [
          "12. SQL Coverage",
          "VDBE Opcode Layout"
        ]
      }
    ]
  },
  {
    "commit": "d0fda0186411a7f75bd73771f484259c29d7921a",
    "change_groups": [
      {
        "summary": "Minor wording adjustments clarifying wal/pager dependency and header naming.",
        "categories": [
          5,
          9
        ],
        "primary_category": 5,
        "confidence": 0.72,
        "evidence": [
          "+Does NOT depend on `fsqlite-pager`",
          "+| `sqliteInt.h` | Main internal header"
        ],
        "changed_headings": [
          "Crate Descriptions",
          "C SQLite Reference Extraction"
        ]
      }
    ]
  },
  {
    "commit": "b8344dba9b295b31b843807f80c6ec10e601459b",
    "change_groups": [
      {
        "summary": "Clarified crate boundaries and added WAL header notes for SQLite format.",
        "categories": [
          4,
          2
        ],
        "primary_category": 4,
        "confidence": 0.6,
        "evidence": [
          "+does not depend on `fsqlite-wal`.",
          "+Format version: 3007000 (constant for all WAL1 databases;"
        ],
        "changed_headings": [
          "Crate Descriptions",
          "WAL Header"
        ]
      }
    ]
  },
  {
    "commit": "5e2344ce7b5640aaad2fb48bed85838f9c9ecfef",
    "change_groups": [
      {
        "summary": "Specified the exact SQLite WAL checksum algorithm and clarified wal-index layout details.",
        "categories": [
          2
        ],
        "primary_category": 2,
        "confidence": 0.64,
        "evidence": [
          "+fn wal_checksum(data: &[u8], mut s0: u32, mut s1: u32, native: bool) -> (u32, u32)",
          "+WalIndexHdr (first copy):"
        ],
        "changed_headings": [
          "11.9.1 WAL Checksum Algorithm",
          "11.10 WAL Index (wal-index / SHM)"
        ]
      },
      {
        "summary": "Refined repair tail bounds with concrete orders of magnitude for small K.",
        "categories": [
          8,
          1
        ],
        "primary_category": 8,
        "confidence": 0.54,
        "evidence": [
          "+K=4, R=1 (n=5): P(loss) approx C(5,2) p^2 approx 1e-7"
        ],
        "changed_headings": [
          "Durability Bound"
        ]
      }
    ]
  },
  {
    "commit": "713e6dc4f8b202870b16e154a597bcd735d5c05b",
    "change_groups": [
      {
        "summary": "Added explicit SQLite page header field layout description.",
        "categories": [
          2,
          6
        ],
        "primary_category": 2,
        "confidence": 0.62,
        "evidence": [
          "+**Page header field layout:**",
          "+Offset  Size  Field"
        ],
        "changed_headings": [
          "B-tree Page Header"
        ]
      },
      {
        "summary": "Updated document version metadata.",
        "categories": [
          5
        ],
        "primary_category": 5,
        "confidence": 0.6,
        "evidence": [
          "-*Document version: 1.8",
          "+*Document version: 1.9"
        ],
        "changed_headings": []
      }
    ]
  },
  {
    "commit": "e2dc2232567e901ce8f7192303c9da013e4c8d7c",
    "change_groups": [
      {
        "summary": "Clarified fragmentation byte meaning and added explicit SQLite varint encoding spec.",
        "categories": [
          2,
          9
        ],
        "primary_category": 2,
        "confidence": 0.65,
        "evidence": [
          "+### 11.2.1 Varint Encoding",
          "+minimum freeblock size is 4 bytes, so gaps of 1-3 bytes cannot be tracked"
        ],
        "changed_headings": [
          "11.2.1 Varint Encoding"
        ]
      }
    ]
  },
  {
    "commit": "3c5d5a177c8fbd78fd6f11f32a02e4c3ada9e172",
    "change_groups": [
      {
        "summary": "Fixed index local-payload math and clarified SQLite comparison affinity coercion rules.",
        "categories": [
          2,
          1
        ],
        "primary_category": 2,
        "confidence": 0.64,
        "evidence": [
          "+index max_local = 1001.",
          "+**Comparison affinity rules** (applied before comparison; determines which operand gets type coercion)"
        ],
        "changed_headings": [
          "11.3 Cell Formats",
          "12. SQL Coverage"
        ]
      }
    ]
  },
  {
    "commit": "714570a488c9e93f3349ba527e70e744570555e9",
    "change_groups": [
      {
        "summary": "Documented SQLite pointer-map entry type semantics with explicit codes.",
        "categories": [
          2
        ],
        "primary_category": 2,
        "confidence": 0.6,
        "evidence": [
          "+1 = PTRMAP_ROOTPAGE",
          "+5 = PTRMAP_BTREE"
        ],
        "changed_headings": [
          "Pointer Map"
        ]
      },
      {
        "summary": "Swapped raw pread usage for safe `FileExt::read_exact_at` in page read example.",
        "categories": [
          7,
          5
        ],
        "primary_category": 7,
        "confidence": 0.56,
        "evidence": [
          "+FileExt::read_exact_at is safe Rust; no `unsafe` needed.",
          "+async fn read_page(cx: &Cx, pool: &PageBufPool, file: &Arc<File>, offset: u64)"
        ],
        "changed_headings": [
          "VFS / Pager Example"
        ]
      }
    ]
  },
  {
    "commit": "893691fc58cca08e1fba3a25a5ddf520624d6860",
    "change_groups": [
      {
        "summary": "Added SSI-related invariants and clarified e-process usage versus hard invariants.",
        "categories": [
          8,
          1
        ],
        "primary_category": 8,
        "confidence": 0.6,
        "evidence": [
          "+**INV-5 (Snapshot Stability)**",
          "+**Recommendation:** Use `debug_assert!` for INV-1 through INV-7"
        ],
        "changed_headings": [
          "4.3 E-process Monitoring"
        ]
      },
      {
        "summary": "Corrected index max_local formula for 4096-byte pages.",
        "categories": [
          2
        ],
        "primary_category": 2,
        "confidence": 0.58,
        "evidence": [
          "+index max_local = 1002.",
          "+`4084 * 64 / 255 - 23` = `1025 - 23` = 1002"
        ],
        "changed_headings": [
          "11.3 Cell Formats"
        ]
      }
    ]
  },
  {
    "commit": "a9aff80df7c0d04f465db40b74165a2e75ae5e29",
    "change_groups": [
      {
        "summary": "Corrected budget algebra terminology and fixed BOCPD recursion by marginalizing over prior run length.",
        "categories": [
          1,
          8
        ],
        "primary_category": 1,
        "confidence": 0.64,
        "evidence": [
          "+product meet-semilattice",
          "+P(r_t | x_{1:t}) proportional to sum_{r_{t-1}} ..."
        ],
        "changed_headings": [
          "0.3 Glossary",
          "4.8 BOCPD"
        ]
      },
      {
        "summary": "Threaded `Cx` through VfsFile methods to make file operations cancelable/deadline-aware.",
        "categories": [
          3,
          7
        ],
        "primary_category": 7,
        "confidence": 0.6,
        "evidence": [
          "+fn read(&mut self, cx: &Cx, buf: &mut [u8], offset: u64) -> Result<usize>;",
          "+fn sync(&mut self, cx: &Cx, flags: SyncFlags) -> Result<()>;"
        ],
        "changed_headings": [
          "9.1 Storage Traits"
        ]
      }
    ]
  },
  {
    "commit": "80bc4494162ff420cdb48c5122324d9bdc8d328b",
    "change_groups": [
      {
        "summary": "Refined public API facade with `Database` wrapper and replaced Default-based function state with `initial_state()` factories for type erasure.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.6,
        "evidence": [
          "+pub struct Database(Connection);",
          "+fn initial_state(&self) -> Self::State;"
        ],
        "changed_headings": [
          "Public API",
          "Scalar/Aggregate/Window Functions"
        ]
      }
    ]
  },
  {
    "commit": "a5a85e64c6356176ad86795d69e1293bd76f2601",
    "change_groups": [
      {
        "summary": "Added `Cx` parameter to rollback for cancelation-aware abort path.",
        "categories": [
          3,
          7
        ],
        "primary_category": 7,
        "confidence": 0.6,
        "evidence": [
          "+fn rollback(&self, cx: &Cx, txn: Transaction);"
        ],
        "changed_headings": [
          "MvccPager Trait"
        ]
      },
      {
        "summary": "Clarified recursive CTE semantics for UNION vs UNION ALL in SQLite.",
        "categories": [
          2,
          9
        ],
        "primary_category": 2,
        "confidence": 0.58,
        "evidence": [
          "+Recursive CTEs use `UNION ALL` (keeps duplicates) or `UNION` (discards duplicates, providing implicit cycle detection)"
        ],
        "changed_headings": [
          "Recursive CTEs"
        ]
      }
    ]
  }
];
      const CLASS_MIDDLE = [
  {
    "commit": "7c93dbb54e2d939f820d97a469bf204829ce97c8",
    "change_groups": [
      {
        "summary": "Fix WAL checksum endianness semantics and clarify WAL header magic meaning, correct WAL-index hash and lock region layout, and add rollback journal format details",
        "categories": [
          1,
          2,
          4,
          9
        ],
        "primary_category": 2,
        "confidence": 0.84,
        "evidence": [
          "WAL checksum parameter semantics corrected; bigEndCksum handling clarified",
          "WAL-index hash function corrected to multiplicative hash; lock region layout fixed",
          "Rollback journal format and checksum described"
        ],
        "changed_headings": [
          "7.1 WAL Checksum Algorithm",
          "11.9.1 WAL Checksum Algorithm",
          "11.10 WAL Index (wal-index / SHM)",
          "11.14 Rollback Journal Format"
        ]
      },
      {
        "summary": "Correct ARC competitive ratio claim",
        "categories": [
          1,
          9
        ],
        "primary_category": 1,
        "confidence": 0.58,
        "evidence": [
          "ARC claim changed from competitive ratio 2 to containment/self-tuning wording"
        ],
        "changed_headings": [
          "6. Buffer Pool: ARC Cache"
        ]
      }
    ]
  },
  {
    "commit": "07236c61ff86a87f22bb1aa6fd26885eb527d268",
    "change_groups": [
      {
        "summary": "Revert WAL checksum endianness guidance to match nativeCksum derivation; update examples accordingly",
        "categories": [
          1,
          2,
          9
        ],
        "primary_category": 2,
        "confidence": 0.8,
        "evidence": [
          "big_end_cksum now feeds nativeCksum via target endianness; swap logic changed",
          "endianness table updated to per-reader nativeCksum outcomes"
        ],
        "changed_headings": [
          "7.1 WAL Checksum Algorithm",
          "11.9.1 WAL Checksum Algorithm"
        ]
      },
      {
        "summary": "Clarify Windows VFS inclusion and adjust layering/ARC placement notes",
        "categories": [
          4,
          9
        ],
        "primary_category": 4,
        "confidence": 0.63,
        "evidence": [
          "WindowsVfs noted as in-scope; VFS locking/SHM mechanisms clarified",
          "ARC cache and MvccPager trait placement moved to pager layer"
        ],
        "changed_headings": [
          "15. VFS",
          "8. Implementation Plan"
        ]
      },
      {
        "summary": "Adjust B-tree depth capacity note",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.45,
        "evidence": [
          "B-tree depth capacity explanation rewritten"
        ],
        "changed_headings": [
          "8. Implementation Plan"
        ]
      }
    ]
  },
  {
    "commit": "d9146f771541d7e134f540e11016df82cc6e17b9",
    "change_groups": [
      {
        "summary": "Reframe GF(256) byte algebra as encoding-only; forbid raw XOR merges for structured pages and introduce SAFE/LAB_UNSAFE write-merge policy",
        "categories": [
          1,
          4,
          9
        ],
        "primary_category": 4,
        "confidence": 0.74,
        "evidence": [
          "Byte-disjoint XOR merge prohibited for structured SQLite pages; SAFE merge ladder required",
          "PRAGMA fsqlite.write_merge introduced with SAFE/LAB_UNSAFE"
        ],
        "changed_headings": [
          "3.4.5 Algebraic Write Merging Over GF(256)",
          "5.10 Commit-Time Merge Policy"
        ]
      },
      {
        "summary": "Specify commit marker stream format with fixed-size records, O(1) seek, and divergence checks",
        "categories": [
          4,
          7,
          9
        ],
        "primary_category": 4,
        "confidence": 0.78,
        "evidence": [
          "New marker segment header and record layout; fixed-size records",
          "O(1) seek and binary search by time described"
        ],
        "changed_headings": [
          "3.5.4.1 Commit Marker Stream Format (Random-Access, Auditable)"
        ]
      },
      {
        "summary": "Harden ECS compaction publication ordering and ARC eviction edge case; add RaptorQ overhead slack + adaptive tuning note",
        "categories": [
          4,
          1,
          8,
          9
        ],
        "primary_category": 4,
        "confidence": 0.67,
        "evidence": [
          "Compaction publish ordering now two-phase with durability before retiring old segments",
          "ARC T1 eviction path clarified to avoid B1 invariant violation",
          "RaptorQ overhead slack and adaptive tuning (e-process) added"
        ],
        "changed_headings": [
          "3.4.6 Erasure-Coded Page Storage",
          "3.5.4.1 Commit Marker Stream Format (Random-Access, Auditable)",
          "7.11 Publish Protocol",
          "7.9 Compaction Algorithm",
          "6.4 ARC Algorithm: REQUEST Subroutine"
        ]
      },
      {
        "summary": "Correct JSONB node types and size claim; rename R-tree to R*-Tree; tighten conflict model and retry policy",
        "categories": [
          1,
          2,
          9
        ],
        "primary_category": 2,
        "confidence": 0.62,
        "evidence": [
          "JSONB node type codes updated and size claim adjusted",
          "R*-Tree naming corrected; geometry trait name fixed",
          "Conflict modeling and retry policy clarified"
        ],
        "changed_headings": [
          "JSON1/JSONB section",
          "14.4 R*-Tree",
          "18. Conflict Modeling"
        ]
      }
    ]
  },
  {
    "commit": "30619b34ea187d682283f8cd137d8d30e306e6bd",
    "change_groups": [
      {
        "summary": "Clarify geopoly as built on R*-tree and refine conflict probability interpretation with explicit examples",
        "categories": [
          9,
          1
        ],
        "primary_category": 9,
        "confidence": 0.6,
        "evidence": [
          "Geopoly extension description updated to R*-tree",
          "Birthday-paradox conflict probability explanation corrected with numeric examples"
        ],
        "changed_headings": [
          "14.4 R*-Tree",
          "18.3 Conflict Probability (Uniform)"
        ]
      }
    ]
  },
  {
    "commit": "7e9cace601485722e7b31411ad74796c0685bd05",
    "change_groups": [
      {
        "summary": "Refine Zipf conflict probability derivation with sum of squared probabilities and numerical comparison",
        "categories": [
          1,
          9
        ],
        "primary_category": 1,
        "confidence": 0.74,
        "evidence": [
          "P(any conflict) expressed as exp(-C(N,2)*sum p(k)^2)",
          "Numeric comparison vs uniform added"
        ],
        "changed_headings": [
          "18.4 Non-Uniform Page Access: Zipf Distribution"
        ]
      }
    ]
  },
  {
    "commit": "ff937f3d678f88eb855051dbbd32e8e5046bea0f",
    "change_groups": [
      {
        "summary": "Clarify PageVersion hash storage and temper byte-conflict reduction claims; add retry policy",
        "categories": [
          9,
          1,
          7
        ],
        "primary_category": 9,
        "confidence": 0.62,
        "evidence": [
          "XXH3 hash belongs to CachedPage not PageVersion",
          "Byte-level conflict reduction reframed as upper bound; practical limits listed",
          "Retry policy added"
        ],
        "changed_headings": [
          "5. MVCC Formal Model",
          "18.7 Impact of Write Merging",
          "18.8 Throughput Model"
        ]
      }
    ]
  },
  {
    "commit": "61ee3e03f4cd5e44b189da55bb70e91cdba63a29",
    "change_groups": [
      {
        "summary": "Harden ARC eviction failure path to skip dirty pages on WAL flush failure",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.7,
        "evidence": [
          "flush_to_wal errors cause candidate rotation instead of eviction"
        ],
        "changed_headings": [
          "6.4 ARC Algorithm: REPLACE Subroutine"
        ]
      },
      {
        "summary": "Add math-function compile flag note and geopoly_xform function",
        "categories": [
          6,
          9
        ],
        "primary_category": 9,
        "confidence": 0.46,
        "evidence": [
          "SQLite math functions require compile flag; FrankenSQLite always includes",
          "geopoly_xform listed"
        ],
        "changed_headings": [
          "13.2 Math Functions",
          "14.4 R*-Tree"
        ]
      }
    ]
  },
  {
    "commit": "a90a3794616e12b083f88276e3cb47a045f3ac10",
    "change_groups": [
      {
        "summary": "Deep audit fixes for ARC: flush under mutex note, eviction error handling, ghost list overhead corrected, cache_size=0 semantics corrected",
        "categories": [
          1,
          2,
          4,
          9
        ],
        "primary_category": 1,
        "confidence": 0.75,
        "evidence": [
          "cache_size=0 now maps to SQLite default cache size behavior",
          "ghost list overhead recalculated; eviction path handles flush errors"
        ],
        "changed_headings": [
          "6. ARC Cache",
          "PRAGMA cache_size"
        ]
      }
    ]
  },
  {
    "commit": "246102e6b4266fa15000580a3d016d76bcb1c801",
    "change_groups": [
      {
        "summary": "Add adaptive redundancy autopilot and merge-retry loop; add witness refinement policy",
        "categories": [
          4,
          8,
          9
        ],
        "primary_category": 4,
        "confidence": 0.72,
        "evidence": [
          "New adaptive redundancy section with e-process monitoring and policy actions",
          "Commit path restructured with coordinator conflict retry loop",
          "Witness refinement VOI policy added"
        ],
        "changed_headings": [
          "3.5.12 Adaptive Redundancy",
          "5.7 Witness Refinement Policy",
          "5.9 Commit pseudocode"
        ]
      },
      {
        "summary": "Update merge model to structured patches and operator precedence tables (ESCAPE and relational/equality split)",
        "categories": [
          1,
          2,
          9
        ],
        "primary_category": 1,
        "confidence": 0.68,
        "evidence": [
          "Physical merge retitled to structured page patches; raw XOR forbidden",
          "Precedence table split equality vs relational; ESCAPE right-associative; JSON -> operators added"
        ],
        "changed_headings": [
          "5.10 Physical Merge",
          "10.2 Pratt Parser Table",
          "12.15 Expression Grammar"
        ]
      }
    ]
  },
  {
    "commit": "5ea1b6fb02945e4bc147c37ae7d054d92fdd9045",
    "change_groups": [
      {
        "summary": "Fix SSI pseudocode edge direction and dangerous structure explanation; add claiming timestamp cleanup; cross-process note",
        "categories": [
          1,
          4,
          9
        ],
        "primary_category": 1,
        "confidence": 0.74,
        "evidence": [
          "ssi_validate_and_publish scans in_edges and sets has_out_rw",
          "dangerous structure definition clarified; claiming cleanup via timeout",
          "cross-process visibility note added"
        ],
        "changed_headings": [
          "5.7 SSI",
          "5.6.2 TxnSlot",
          "SSI correctness"
        ]
      },
      {
        "summary": "Fix precedence tables and RETURNING semantics",
        "categories": [
          2,
          1,
          9
        ],
        "primary_category": 2,
        "confidence": 0.65,
        "evidence": [
          "ISNULL/NOTNULL added to comparison precedence; COLLATE/unary swap fixed",
          "RETURNING reflects BEFORE but not AFTER triggers"
        ],
        "changed_headings": [
          "10.2 Pratt Parser Table",
          "12.15 Expression Grammar",
          "12.4 RETURNING"
        ]
      },
      {
        "summary": "Rename merge terminology and fix RaptorQ size unit detail",
        "categories": [
          9,
          1
        ],
        "primary_category": 9,
        "confidence": 0.42,
        "evidence": [
          "algebraic write merging renamed to safe write-merge ladder",
          "~220 MiB wording corrected"
        ],
        "changed_headings": [
          "3.1 SSI rationale",
          "3.4 RaptorQ"
        ]
      }
    ]
  },
  {
    "commit": "f1f25abe3b40ad0fb8704eaee310aee17de78d25",
    "change_groups": [
      {
        "summary": "Enforce gap-free commit_seq allocation from marker stream; tighten commit_seq publication semantics and memory ordering",
        "categories": [
          1,
          4,
          9
        ],
        "primary_category": 4,
        "confidence": 0.78,
        "evidence": [
          "CommitSeq derived from marker stream tip; no in-memory counter gaps",
          "commit_seq published only after durable marker; ordering clarified"
        ],
        "changed_headings": [
          "3.5.4.1 Commit Marker Stream",
          "5.6.1 SharedMemoryLayout",
          "7.11 Commit Protocol"
        ]
      },
      {
        "summary": "Clarify WAL-index layout details and rollback journal checksum loop; page size change rules",
        "categories": [
          2,
          1,
          9
        ],
        "primary_category": 2,
        "confidence": 0.63,
        "evidence": [
          "WAL-index first segment layout clarified with header overlap",
          "Rollback journal checksum loop bounds clarified",
          "Page size change rules updated"
        ],
        "changed_headings": [
          "11.10 WAL Index",
          "11.14 Rollback Journal Format",
          "11.1 Database Header"
        ]
      },
      {
        "summary": "Add SSI witness rollback note for savepoints",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.4,
        "evidence": [
          "Witness registrations not rolled back on savepoint"
        ],
        "changed_headings": [
          "5.4 Savepoints"
        ]
      }
    ]
  },
  {
    "commit": "9725f136b2450c6411f5735a1575a068f2aacc0b",
    "change_groups": [
      {
        "summary": "Clarify commit_seq publication and cross-process visibility in native vs compatibility modes",
        "categories": [
          4,
          9
        ],
        "primary_category": 4,
        "confidence": 0.7,
        "evidence": [
          "commit_seq described as published high-water mark; cross-process visibility split by mode"
        ],
        "changed_headings": [
          "5.6.1 SharedMemoryLayout",
          "7.11 Commit Protocol"
        ]
      },
      {
        "summary": "Define witness hot-plane epoch lock protocol to avoid false negatives",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.66,
        "evidence": [
          "epoch_lock, bucket_epoch swap discipline defined to prevent lost updates"
        ],
        "changed_headings": [
          "5.6.4 Hot Witness Plane"
        ]
      },
      {
        "summary": "Add DB header forward-compat rules and lock-byte page reservation; move WAL checksum impl to \u00a77.1 reference",
        "categories": [
          2,
          9
        ],
        "primary_category": 2,
        "confidence": 0.6,
        "evidence": [
          "Read/write version compatibility behavior specified",
          "Pending-byte page reserved; WAL checksum impl referenced"
        ],
        "changed_headings": [
          "11.1 Database Header",
          "11.13 Lock-Byte Page",
          "11.9.1 WAL Checksum Algorithm"
        ]
      }
    ]
  },
  {
    "commit": "56742886aae65bdca3a34acda8d677b52a0c241d",
    "change_groups": [
      {
        "summary": "Restrict hot-plane epoch advancement to quiescent concurrent transactions; require epoch_lock refresh semantics",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.72,
        "evidence": [
          "Epoch advance allowed only when no active concurrent txns",
          "Bucket refresh under epoch_lock with Release semantics"
        ],
        "changed_headings": [
          "5.6.4 Hot Witness Plane"
        ]
      }
    ]
  },
  {
    "commit": "71a41750d478e8ecad031a2f490e84a4424a839e",
    "change_groups": [
      {
        "summary": "Publish shm.commit_seq only after marker durable (post-fsync)",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.73,
        "evidence": [
          "commit_seq publish inserted after marker fsync"
        ],
        "changed_headings": [
          "7.11 Commit Protocol"
        ]
      }
    ]
  },
  {
    "commit": "9fa8f7b6082f164d4cb1026296a360cfbafe1f73",
    "change_groups": [
      {
        "summary": "Require commit_seq allocation from marker tip and align sequencer steps",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.73,
        "evidence": [
          "commit_seq allocated from marker stream tip within commit section"
        ],
        "changed_headings": [
          "7.11 Commit Protocol"
        ]
      }
    ]
  },
  {
    "commit": "86d63af874d7a3dc7d522d47bf5f004f12913adb",
    "change_groups": [
      {
        "summary": "Define legacy writer exclusion for compatibility mode and hybrid SHM bridge to wal-index",
        "categories": [
          2,
          4,
          9
        ],
        "primary_category": 2,
        "confidence": 0.74,
        "evidence": [
          "WAL_WRITE_LOCK exclusion required; concurrent mode must exclude legacy writers",
          "Hybrid protocol maintains standard wal-index and reader marks"
        ],
        "changed_headings": [
          "5.6.6 Compatibility",
          "5.6.7 Compatibility Mode Hybrid SHM"
        ]
      },
      {
        "summary": "Clarify gc_horizon derivation from published commit_seq high-water mark",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.36,
        "evidence": [
          "begin_seq derived from published commit_seq"
        ],
        "changed_headings": [
          "5.6.5 GC Horizon"
        ]
      }
    ]
  },
  {
    "commit": "23f575f7d4ee20399865e33abca1f93e60eda2d7",
    "change_groups": [
      {
        "summary": "Use Acquire load for snapshot commit_seq; constrain deterministic rebase scope and compatibility note",
        "categories": [
          1,
          4,
          9
        ],
        "primary_category": 1,
        "confidence": 0.7,
        "evidence": [
          "begin snapshot uses Acquire load",
          "Rebase restricted: no splits/overflow/freelist/multi-page ops"
        ],
        "changed_headings": [
          "5.2 begin()",
          "5.10 Deterministic Rebase"
        ]
      }
    ]
  },
  {
    "commit": "6228e273d3dce6ca67c97fd43afb9018d19c6f5f",
    "change_groups": [
      {
        "summary": "Reconcile shm.commit_seq to durable tip on open; add lock-table rebuild lease protocol and epoch_lock acquisition rules",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.73,
        "evidence": [
          "commit_seq initialization/reconciliation rules added",
          "Lock table rebuild lease + quiescence; epoch_lock acquisition details"
        ],
        "changed_headings": [
          "5.6.1 SharedMemoryLayout",
          "5.6.3 SharedPageLockTable",
          "5.6.4 Hot Witness Plane"
        ]
      },
      {
        "summary": "Add ARC patent note and clarify legacy writer exclusion reminder",
        "categories": [
          9,
          2
        ],
        "primary_category": 9,
        "confidence": 0.4,
        "evidence": [
          "ARC patent expired note",
          "Legacy writer exclusion restated"
        ],
        "changed_headings": [
          "6. ARC Cache",
          "7.11 Compatibility mode verifiability"
        ]
      }
    ]
  },
  {
    "commit": "2e3ea218a908f4a22791b4c8b30a0dbf746f867b",
    "change_groups": [
      {
        "summary": "Make SharedPageLockTable key-stable (no tombstones), adjust acquire/release and rebuild rationale",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.76,
        "evidence": [
          "page_number no longer tombstoned; release only clears owner_txn",
          "Acquire CAS failure re-reads same slot; rebuild to clear keys"
        ],
        "changed_headings": [
          "5.6.3 SharedPageLockTable",
          "5.6.3.1 Table Rebuild"
        ]
      },
      {
        "summary": "Define marker stream byte-exact encoding constants and marker_id domain separation",
        "categories": [
          4,
          9
        ],
        "primary_category": 4,
        "confidence": 0.69,
        "evidence": [
          "Marker header/record sizes fixed (36/88 bytes)",
          "MarkerId uses domain-separated BLAKE3"
        ],
        "changed_headings": [
          "3.5.4.1 Commit Marker Stream"
        ]
      },
      {
        "summary": "Tighten compatibility-mode legacy writer exclusion and merge terminology",
        "categories": [
          2,
          9
        ],
        "primary_category": 2,
        "confidence": 0.6,
        "evidence": [
          "Legacy writer exclusion required whenever fsqlite-shm used",
          "Safe write merging terminology replaces algebraic merging"
        ],
        "changed_headings": [
          "5.6.6 Compatibility",
          "5.10 Safe Write Merging"
        ]
      }
    ]
  },
  {
    "commit": "d05de23ea89da924e86232c8da519b6b58b4a0d5",
    "change_groups": [
      {
        "summary": "Align lock-table rebuild prose with key-stability and load factor accounting",
        "categories": [
          9,
          1
        ],
        "primary_category": 9,
        "confidence": 0.57,
        "evidence": [
          "Rebuild rationale now for non-deleted keys; load factor counts page_number!=0"
        ],
        "changed_headings": [
          "5.6.3.1 Table Rebuild"
        ]
      },
      {
        "summary": "Add database header constraints (usable_size >= 480) and db-size validity condition",
        "categories": [
          2,
          9
        ],
        "primary_category": 2,
        "confidence": 0.66,
        "evidence": [
          "usable_size constraint added; db size valid only when version-valid-for matches change counter"
        ],
        "changed_headings": [
          "11.1 Database Header"
        ]
      }
    ]
  },
  {
    "commit": "fa1830e4593854a492d78bec24da1db90351c2f0",
    "change_groups": [
      {
        "summary": "Clarify ECS permeation map distinction between marker stream and coded objects",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.52,
        "evidence": [
          "CommitMarkerRecord separated from coded CommitCapsule/Proof"
        ],
        "changed_headings": [
          "3.5.5 RootManifest / ECS map"
        ]
      },
      {
        "summary": "Correct fragment limit semantics and cell payload local byte wording",
        "categories": [
          2,
          1
        ],
        "primary_category": 2,
        "confidence": 0.65,
        "evidence": [
          "Fragment limit is threshold before insertion; cell payload references local_bytes"
        ],
        "changed_headings": [
          "11.2 B-Tree Page Layout",
          "11.3 Cell Format"
        ]
      }
    ]
  },
  {
    "commit": "70436b5c2c04e42c0aaf4b776a8f779416abdeff",
    "change_groups": [
      {
        "summary": "Specify WAL-index SHM native byte order, iVersion constraint, and WalCkptInfo layout fix",
        "categories": [
          2,
          1,
          9
        ],
        "primary_category": 2,
        "confidence": 0.74,
        "evidence": [
          "SHM fields are native endian; iVersion must be 3007000; WalCkptInfo 40-byte block"
        ],
        "changed_headings": [
          "11.10 WAL Index (wal-index / SHM)"
        ]
      }
    ]
  },
  {
    "commit": "a4e773481bd417530fe83f33868ef6ce5656e933",
    "change_groups": [
      {
        "summary": "Add VFS shared-memory API methods and expand trait signatures (cursor ops, virtual tables)",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.72,
        "evidence": [
          "VfsFile adds shm_map/shm_lock/shm_barrier/shm_unmap",
          "BtreeCursorOps methods take &Cx and add first/last; VirtualTable lifecycle methods added"
        ],
        "changed_headings": [
          "VFS Trait Definitions",
          "BtreeCursorOps",
          "VirtualTable"
        ]
      },
      {
        "summary": "Clarify CommitMarkerRecord marker_id domain-separated hash reference",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.38,
        "evidence": [
          "marker_id comment updated to refer to domain-separated definition"
        ],
        "changed_headings": [
          "3.5.4.1 Commit Marker Stream"
        ]
      }
    ]
  },
  {
    "commit": "09c095901ddb1e5db47f55ddf5c616b5d51bf74e",
    "change_groups": [
      {
        "summary": "Define CheckpointPageWriter trait for WAL checkpoint and add replication gate acceptance criteria",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.63,
        "evidence": [
          "CheckpointPageWriter trait added in pager; replication gate added in Phase 9"
        ],
        "changed_headings": [
          "9.1 Function Traits",
          "Phase 9 Gates"
        ]
      }
    ]
  },
  {
    "commit": "40a2ac7739778b30cb11129a782d8746bd13e6ed",
    "change_groups": [
      {
        "summary": "Bump document version footer to 1.18 with summary of prior fixes",
        "categories": [
          5
        ],
        "primary_category": 5,
        "confidence": 0.86,
        "evidence": [
          "Footer version line updated"
        ],
        "changed_headings": [
          "Document footer"
        ]
      }
    ]
  },
  {
    "commit": "40c6c3d152cf7d86104ad9ca19fbb850d6fdf343",
    "change_groups": [
      {
        "summary": "Lock-table acquire must return BUSY on CAS race to avoid duplicate keys",
        "categories": [
          1
        ],
        "primary_category": 1,
        "confidence": 0.69,
        "evidence": [
          "CAS failure now returns SQLITE_BUSY and forbids continued probing"
        ],
        "changed_headings": [
          "5.6.3 SharedPageLockTable"
        ]
      }
    ]
  },
  {
    "commit": "8512e62aed8835864a8800ed895c91a8773dc996",
    "change_groups": [
      {
        "summary": "Move compatibility page-FEC repair symbols to sidecar; add UDP payload limits and MTU guidance",
        "categories": [
          2,
          4,
          9
        ],
        "primary_category": 2,
        "confidence": 0.7,
        "evidence": [
          "Compatibility DB file must not append repair region; use .db-fec sidecar",
          "UDP payload <= 65507 and MTU-safe symbol sizing guidance"
        ],
        "changed_headings": [
          "3.4 Replication Packet",
          "3.4.6 Erasure-Coded Page Storage",
          "Compatibility mode verifiability"
        ]
      },
      {
        "summary": "Adjust WAL framing note, checkpoint chunk sizes, and recovery wording",
        "categories": [
          1,
          9
        ],
        "primary_category": 1,
        "confidence": 0.46,
        "evidence": [
          "WAL frames are tightly packed; not sector-aligned",
          "Checkpoint chunk size guidance changed; Native mode recovery path tweak"
        ],
        "changed_headings": [
          "7. WAL Durability",
          "3.5 ECS tuning",
          "7.12 Recovery"
        ]
      },
      {
        "summary": "Clarify cross-process MVCC phase gates and compat sidecars",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.38,
        "evidence": [
          "Phase 6 covers cross-process MVCC; .db-fec referenced"
        ],
        "changed_headings": [
          "Phase gates",
          "Implementation notes"
        ]
      }
    ]
  },
  {
    "commit": "e31897fd92088e5305735d080e224a23b7315a11",
    "change_groups": [
      {
        "summary": "Clarify MTU-aware checkpoint chunk sizing rationale",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.58,
        "evidence": [
          "CheckpointChunk sizing note updated to prefer <=1366 on UDP"
        ],
        "changed_headings": [
          "3.5 ECS tuning"
        ]
      }
    ]
  },
  {
    "commit": "b03db7bc0b8a2705fe6fda19f5f149452755284c",
    "change_groups": [
      {
        "summary": "Fix SQL coverage details: join types, LIMIT comma order, MATCH semantics, DROP COLUMN behavior, json(X) error behavior",
        "categories": [
          2,
          1,
          9
        ],
        "primary_category": 2,
        "confidence": 0.76,
        "evidence": [
          "JOIN types are nested-loop; LIMIT offset,count; MATCH not enforced; DROP COLUMN rewrite/failure conditions",
          "json(X) throws error on invalid JSON"
        ],
        "changed_headings": [
          "12. SQL Coverage",
          "12.1 SELECT",
          "12.2 INSERT",
          "12.8 ALTER TABLE",
          "14.1 JSON1 Functions"
        ]
      },
      {
        "summary": "Replace hash-join mention in vectorized VDBE section",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.44,
        "evidence": [
          "Vectorized VDBE applies to nested-loop joins"
        ],
        "changed_headings": [
          "21.7 Vectorized VDBE"
        ]
      }
    ]
  },
  {
    "commit": "dd190e4f7f24aa2a8b7bb1bbc7020aead6d014c0",
    "change_groups": [
      {
        "summary": "Correct GF(256) worked example and add MMR/IBLT replication enhancements",
        "categories": [
          1,
          8,
          6
        ],
        "primary_category": 1,
        "confidence": 0.72,
        "evidence": [
          "GF(256) log/exp values corrected",
          "MMR inclusion/prefix proofs added; IBLT anti-entropy described"
        ],
        "changed_headings": [
          "3.2 GF(256) Worked Example",
          "3.5.4 Marker Stream (MMR)",
          "3.5.6 Replication"
        ]
      },
      {
        "summary": "Rename WAL checksum variable names to hdr_cksum1/2 in chain description",
        "categories": [
          9,
          1
        ],
        "primary_category": 1,
        "confidence": 0.46,
        "evidence": [
          "hdr_s0/hdr_s1 renamed to hdr_cksum1/2"
        ],
        "changed_headings": [
          "7.1 WAL Checksum Chain",
          "11.9.1 WAL Checksum Chain"
        ]
      },
      {
        "summary": "Add policy controller and mixture e-process guidance; expand IntentFootprint and merge certificates",
        "categories": [
          8,
          4,
          6
        ],
        "primary_category": 8,
        "confidence": 0.66,
        "evidence": [
          "PolicyController section with expected loss and BOCPD guardrails",
          "Mixture e-processes described; IntentFootprint, commutativity rules, MergeCertificate schema added"
        ],
        "changed_headings": [
          "4.3 E-processes",
          "4.17 Policy Controller",
          "5.10 Write Merge System"
        ]
      }
    ]
  },
  {
    "commit": "a664265e3f5876177b7c1410dad7119e01e7f9e9",
    "change_groups": [
      {
        "summary": "Add policy controller, epochs, remote capability/idempotency/saga requirements, and symbol log format",
        "categories": [
          4,
          8,
          6
        ],
        "primary_category": 4,
        "confidence": 0.78,
        "evidence": [
          "Symbol log segment format with epoch_id; RootManifest ecs_epoch added",
          "RemoteCap, idempotency, saga requirements for remote ops",
          "PolicyController section with expected loss and guardrails"
        ],
        "changed_headings": [
          "3.5.4.2 Symbol Record Logs",
          "3.5.5 RootManifest",
          "4.17 Policy Controller",
          "4.18 Epochs",
          "4.19 Remote Effects"
        ]
      },
      {
        "summary": "Revise hot witness plane to double-buffered epochs; add epoch-advance rules and candidate discovery over two epochs",
        "categories": [
          4,
          1
        ],
        "primary_category": 4,
        "confidence": 0.69,
        "evidence": [
          "HotWitnessBucketEntry now has epoch_a/epoch_b buffers",
          "Candidate discovery unions both epochs; epoch advancement rule updated"
        ],
        "changed_headings": [
          "5.6.4 Hot Witness Plane",
          "5.7 SSI"
        ]
      },
      {
        "summary": "ARC eviction safety valve and flush_dirty_page naming; compaction saga requirement",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.58,
        "evidence": [
          "Case IV eviction adds safety valve and flush_dirty_page name",
          "Compaction must be a Saga"
        ],
        "changed_headings": [
          "6. ARC Cache",
          "7.9 Compaction"
        ]
      }
    ]
  },
  {
    "commit": "599cafb3f29c52dd2a44f27d5632032c54a34a25",
    "change_groups": [
      {
        "summary": "Clarify SQLite 3.52.0 forward target and unsafe prefetch requirement; add symbol auth guidance and epoch helper semantics",
        "categories": [
          9,
          4
        ],
        "primary_category": 9,
        "confidence": 0.57,
        "evidence": [
          "3.52.0 noted as forward target; prefetch must use helper crate due to unsafe_code forbid",
          "Symbol auth guidance refined"
        ],
        "changed_headings": [
          "1.1 What It Is",
          "1.4 Constraints",
          "3.5.2 Symbol Auth"
        ]
      },
      {
        "summary": "Add helper views for witness epoch buffers and refine epoch-advance conditions; planner NGQP note updated",
        "categories": [
          9,
          4
        ],
        "primary_category": 4,
        "confidence": 0.55,
        "evidence": [
          "readers_for_epoch/writers_for_epoch helpers defined",
          "Epoch advancement criteria specified in terms of txn states",
          "NGQP described as N3 bounded algorithm"
        ],
        "changed_headings": [
          "5.6.4 Hot Witness Plane",
          "10.5 Query Planning"
        ]
      }
    ]
  },
  {
    "commit": "e42a43de57ff5d4ee616cd2ec3b54a0db249a3e6",
    "change_groups": [
      {
        "summary": "Define symbol auth master key derivation and adjust VdbeOp p5 usage note",
        "categories": [
          4,
          2,
          9
        ],
        "primary_category": 4,
        "confidence": 0.64,
        "evidence": [
          "master_key derived from DEK with domain separation or lab seed",
          "P5 note updated to allow full 16 bits per opcode"
        ],
        "changed_headings": [
          "4.18 Epoch-Scoped Symbol Auth",
          "10.7 VDBE Instruction Format"
        ]
      }
    ]
  },
  {
    "commit": "f158b44a5bd96ef34b378b58f61399568424f6d4",
    "change_groups": [
      {
        "summary": "Clarify symbol auth wording and refine planner cost model with ANALYZE note",
        "categories": [
          9,
          2
        ],
        "primary_category": 2,
        "confidence": 0.6,
        "evidence": [
          "Symbol auth description adjusted to emphasize auth_tag",
          "Cost model formulas updated; ANALYZE note added"
        ],
        "changed_headings": [
          "3.5 ECS",
          "10.5 Query Planning"
        ]
      }
    ]
  },
  {
    "commit": "3cf0f13571c101725f9ef34ce2476c90478ff07d",
    "change_groups": [
      {
        "summary": "Fix RaptorQ MTU and sub-blocking explanation; adjust symbol auth key derivation requirements",
        "categories": [
          1,
          3,
          9
        ],
        "primary_category": 1,
        "confidence": 0.72,
        "evidence": [
          "MTU-safe T corrected to 1464; sub-symbol calculation fixed",
          "Symbol auth key derivation clarified with explicit master key capability"
        ],
        "changed_headings": [
          "3.4 Replication Packet",
          "4.18 Symbol Auth"
        ]
      }
    ]
  },
  {
    "commit": "0cb22962ec87cb29a29a44acfc5ec14588aadb23",
    "change_groups": [
      {
        "summary": "Add lock references, row-value expression, and VDBE opcode examples; clarify blocking pool defaults",
        "categories": [
          7,
          9
        ],
        "primary_category": 7,
        "confidence": 0.62,
        "evidence": [
          "RowValue Expr added; UPDATE/DELETE VDBE examples added",
          "WAL_WRITE_LOCK reference updated; blocking pool defaults clarified"
        ],
        "changed_headings": [
          "10.7 VDBE Instruction Format",
          "10.6 Code Generation",
          "1.2 Innovations",
          "4.2 Blocking Pool"
        ]
      },
      {
        "summary": "Add ObjectId collision bound note; refine invariants monitor; several clarifications about WAL snapshot conflicts and risk sections",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.5,
        "evidence": [
          "ObjectId 128-bit truncation noted as ~2^64 birthday bound",
          "observe_lock_exclusivity now cross-checks lock table vs txn locks"
        ],
        "changed_headings": [
          "Glossary",
          "4.3 Invariant Monitoring",
          "7.4 Page Integrity"
        ]
      }
    ]
  },
  {
    "commit": "1680d6931f5773673fae021b7cc646e3f3faab44",
    "change_groups": [
      {
        "summary": "Fix e-process aggregation to arithmetic mean and correct INV-1 violation detection comment",
        "categories": [
          1,
          8
        ],
        "primary_category": 1,
        "confidence": 0.74,
        "evidence": [
          "E_global aggregation changed to sum of weighted e-values",
          "INV-1 comment corrected to ~20 violations for threshold"
        ],
        "changed_headings": [
          "4.3 E-processes"
        ]
      },
      {
        "summary": "Minor clarifications: WAL snapshot conflict phrasing, opcode traces illustrative, sqliteInt.h size corrected",
        "categories": [
          9
        ],
        "primary_category": 9,
        "confidence": 0.45,
        "evidence": [
          "WAL snapshot conflict clarified; opcode traces labeled illustrative; sqliteInt.h size corrected"
        ],
        "changed_headings": [
          "1.2 Innovations",
          "10.6 Code Generation",
          "21. Reference Files"
        ]
      }
    ]
  },
  {
    "commit": "1e2aae9940969190c6b9bebe94c94749eee91626",
    "change_groups": [
      {
        "summary": "Introduce UpdateExpression and RebaseExpr to enable deterministic rebase of read-modify-write; refine rebase safety and commutativity rules",
        "categories": [
          4,
          1,
          8
        ],
        "primary_category": 4,
        "confidence": 0.78,
        "evidence": [
          "UpdateExpression intent added with RebaseExpr AST",
          "Rebase safety distinguishes blocking reads vs expression reads; VDBE rules added",
          "Commutativity refined for column-disjoint UpdateExpression ops"
        ],
        "changed_headings": [
          "5.10 Deterministic Rebase",
          "5.10.1 Intent Logs",
          "5.10.7 Intent Footprints"
        ]
      },
      {
        "summary": "Require legacy WAL reader locks when updating read marks; cross-db atomic WAL transactions requirement",
        "categories": [
          2,
          4
        ],
        "primary_category": 2,
        "confidence": 0.64,
        "evidence": [
          "Readers must hold WAL_READ_LOCK when writing aReadMark",
          "Cross-database atomic WAL transactions required via 2PC"
        ],
        "changed_headings": [
          "5.6.7 Compatibility Mode Hybrid SHM",
          "12.11 ATTACH"
        ]
      }
    ]
  }
];
      const CLASS_LATE = [
  {
    "commit": "da22f479dc2f8d1e98294750b741b3cce168fd19",
    "change_groups": [
      {
        "summary": "Move WAL-FEC generation fully off the commit critical path; distinguish design-time p_design from runtime living corruption-rate monitoring.",
        "categories": [
          7,
          8,
          4
        ],
        "primary_category": 7,
        "confidence": 0.82,
        "evidence": [
          "Enqueue a background FEC job ... encoder reads source frames from .wal",
          "p_design = 10^-4; autopilot maintains living estimates + conservative bounds"
        ],
        "changed_headings": [
          "3.4.1 WAL-FEC",
          "3.5.12 Durability Autopilot"
        ]
      },
      {
        "summary": "Add rigorous durability policy math: Bayesian posterior for p (diagnostics) plus anytime-valid p_upper for guarantees; VOI-based monitoring budgeting.",
        "categories": [
          8,
          7
        ],
        "primary_category": 8,
        "confidence": 0.8,
        "evidence": [
          "p | data ~ Beta(\u03b10 + n_bad, \u03b20 + n_ok)",
          "derive p_upper via martingale inversion (anytime-valid under optional stopping)",
          "VOI(m) = E[\u0394Loss(m) | evidence] - Cost(m)"
        ],
        "changed_headings": [
          "3.5.12.2.1 Living Corruption-Rate Estimates",
          "4.16 Observability"
        ]
      },
      {
        "summary": "Reframe background compaction + retry as explicit decision policies: MDP compaction scheduling, online Zipf/s estimation, and optimal-stopping retry control; add interop notes (WAL-index locks, sqliteInt.h line count).",
        "categories": [
          8,
          7,
          2,
          4,
          5
        ],
        "primary_category": 8,
        "confidence": 0.72,
        "evidence": [
          "Model compaction scheduling as a finite-state MDP",
          "retry as expected-loss minimization; mentions Gittins-index threshold rule",
          "WAL-index lock slot mapping: aLock[0]=WAL_WRITE_LOCK ..."
        ],
        "changed_headings": [
          "7.13.1 Workload-Adaptive Compaction Policy",
          "18.4.1 Estimating Zipf s Online",
          "11.10 WAL-index"
        ]
      }
    ]
  },
  {
    "commit": "c25f0d00238fc36f355f818f571abc3d4a1198df",
    "change_groups": [
      {
        "summary": "Make UpdateExpression deterministic rebase correct: regenerate secondary-index ops from schema/row images during replay; forbid rebase for rowid/INTEGER PRIMARY KEY updates; refine commutativity check.",
        "categories": [
          4,
          1,
          2
        ],
        "primary_category": 4,
        "confidence": 0.86,
        "evidence": [
          "Index regeneration (critical): stale IndexDelete/IndexInsert MUST be discarded",
          "No SET clause targets rowid/INTEGER PRIMARY KEY column"
        ],
        "changed_headings": [
          "5.10 Safe Write Merging",
          "UpdateExpression"
        ]
      }
    ]
  },
  {
    "commit": "02e48a41810bdb2ad471926d9728a9a9af12b155",
    "change_groups": [
      {
        "summary": "Correct the maxLocal integer-division example by showing truncation and remainder explicitly.",
        "categories": [
          1,
          5
        ],
        "primary_category": 1,
        "confidence": 0.76,
        "evidence": [
          "261376 / 255 = 1025 (truncated; remainder 1)"
        ],
        "changed_headings": [
          "B-tree local payload"
        ]
      },
      {
        "summary": "Add encryption deliverables/acceptance criteria and risk/complexity gates to the implementation phases.",
        "categories": [
          6,
          7
        ],
        "primary_category": 6,
        "confidence": 0.7,
        "evidence": [
          "Page-level encryption: XChaCha20-Poly1305 with envelope DEK/KEK",
          "Encryption acceptance: PRAGMA key/rekey + AAD swap resistance"
        ],
        "changed_headings": [
          "Implementation Plan"
        ]
      }
    ]
  },
  {
    "commit": "bcd893da081fd15b1614a05c4d8b593f55cf7011",
    "change_groups": [
      {
        "summary": "Split BtreeCursorOps into table/index-specific methods to mirror SQLite's dual B-tree APIs (table vs index move/insert semantics).",
        "categories": [
          4,
          2
        ],
        "primary_category": 4,
        "confidence": 0.84,
        "evidence": [
          "sqlite3BtreeTableMoveTo(i64) vs sqlite3BtreeIndexMoveto(UnpackedRecord*)",
          "Replace key()/data() with payload()"
        ],
        "changed_headings": [
          "Trait Hierarchy",
          "BtreeCursorOps"
        ]
      },
      {
        "summary": "Make SHM mapping spec memory-safe: change shm_map return type to a safe ShmRegion wrapper; update unsafe-code policy boundary for VFS.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.78,
        "evidence": [
          "fn shm_map(...) -> Result<ShmRegion>",
          "ShmRegion wraps mmap'd shared memory behind bounds-checked APIs"
        ],
        "changed_headings": [
          "VFS",
          "Shared Memory"
        ]
      },
      {
        "summary": "Fix release profile optimization level inconsistency (size-opt to throughput-opt) and add a release-perf profile description.",
        "categories": [
          7,
          5
        ],
        "primary_category": 7,
        "confidence": 0.74,
        "evidence": [
          "opt-level = 3  # Full optimization (database engine needs throughput)"
        ],
        "changed_headings": [
          "Release Profile"
        ]
      }
    ]
  },
  {
    "commit": "6da51572d2b5b76eb9a37ffa99da76bfb850cd83",
    "change_groups": [
      {
        "summary": "Fix TxnSlot struct sizing math so shared-memory slot stride is exactly 128 bytes.",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.83,
        "evidence": [
          "_padding was 52 -> 132B total; corrected to 128B stride"
        ],
        "changed_headings": [
          "5.6.2 TxnSlot"
        ]
      },
      {
        "summary": "Tighten group-commit and request semantics: use TxnSlot.mode via request.txn, clarify durability point for batched capsules, and fix term usage (K_source + R).",
        "categories": [
          4,
          7,
          5
        ],
        "primary_category": 4,
        "confidence": 0.72,
        "evidence": [
          "PublishRequest has no mode field; derive mode via request.txn -> TxnSlot.mode",
          "Coordinator FSYNC_1 is the durability point for batched capsule symbols"
        ],
        "changed_headings": [
          "7.11 Group Commit"
        ]
      }
    ]
  },
  {
    "commit": "22e75e65b831adda852df3a11628e793aaa9e7a9",
    "change_groups": [
      {
        "summary": "Add RecentlyCommittedReadersIndex to preserve committed readers' SSI read evidence so incoming rw-edges aren\"t lost when TxnSlots are freed; enforce T3 rule for committed pivots.",
        "categories": [
          4,
          1,
          6,
          7
        ],
        "primary_category": 4,
        "confidence": 0.86,
        "evidence": [
          "RecentlyCommittedReadersIndex retains committed transactions' read witness summary",
          "If entry.has_in_rw is true, committer MUST abort (T3 rule for committed pivots)"
        ],
        "changed_headings": [
          "5.6.2.1 Recently Committed Readers"
        ]
      }
    ]
  },
  {
    "commit": "2f0970b9a5d8fc755269ba848169f5a1369ec6a6",
    "change_groups": [
      {
        "summary": "Fix lexer/identifier semantics: double-quoted strings are TK_ID; DQS reinterpretation happens in name resolution, not tokenization.",
        "categories": [
          2,
          5
        ],
        "primary_category": 2,
        "confidence": 0.8,
        "evidence": [
          "SQLite tokenize.c emits TK_ID for \"...\"; resolve.c reinterprets"
        ],
        "changed_headings": [
          "Lexer",
          "QuotedId"
        ]
      },
      {
        "summary": "Correct planner range-scan cost model by adding index-leaf scanning term; avoids preferring long-range index scans incorrectly.",
        "categories": [
          1,
          7
        ],
        "primary_category": 1,
        "confidence": 0.86,
        "evidence": [
          "cost = log2(N_idx_pages) + sel*N_idx_pages + sel*N_tbl_pages"
        ],
        "changed_headings": [
          "Planner Cost Model"
        ]
      },
      {
        "summary": "Fix UPDATE VDBE trace example: MakeRecord must encode all columns (overlay updated column on full-row image).",
        "categories": [
          2,
          6
        ],
        "primary_category": 2,
        "confidence": 0.74,
        "evidence": [
          "Overlay updated column, then MakeRecord(all columns)"
        ],
        "changed_headings": [
          "VDBE",
          "UPDATE"
        ]
      }
    ]
  },
  {
    "commit": "5cc32a6bc813ff59db231f29db5b95a13ea94704",
    "change_groups": [
      {
        "summary": "Specify required asupersync networking + deterministic VirtualTcp testing and map FrankenSQLite work onto scheduler priority lanes (cancel/timed/ready) for tail-latency control.",
        "categories": [
          3,
          7,
          6
        ],
        "primary_category": 3,
        "confidence": 0.82,
        "evidence": [
          "MUST use asupersync cancel-safe network stack; lab transport swappable to VirtualTcp",
          "Scheduler lanes: Cancel (highest), Timed (EDF), Ready (background)"
        ],
        "changed_headings": [
          "4.19.6 Networking Stack",
          "4.20 Scheduler Priority Lanes"
        ]
      }
    ]
  },
  {
    "commit": "dc92e549d05ac8dc0c23937d6e6dfa874729fdd4",
    "change_groups": [
      {
        "summary": "Correct MVCC version-chain compression: use sparse XOR deltas between adjacent versions (compression), not RaptorQ repair symbols; RaptorQ remains the durability layer for delta objects.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.86,
        "evidence": [
          "V2 delta: XOR(V2,V3)",
          "RaptorQ applies at ECS object level for durability"
        ],
        "changed_headings": [
          "Version Chains",
          "Compression"
        ]
      }
    ]
  },
  {
    "commit": "2433562451b188c9233f6fd83a9ef7573ce48f97",
    "change_groups": [
      {
        "summary": "Add independent per-source hashes to WAL-FEC metadata so recovery can validate surviving source frames after the WAL checksum chain breaks.",
        "categories": [
          7,
          2,
          4
        ],
        "primary_category": 7,
        "confidence": 0.86,
        "evidence": [
          "WalFecGroupMeta.source_page_xxh3: Vec<u64> (length=K)",
          "frames at/after checksum mismatch validate via .wal-fec hashes"
        ],
        "changed_headings": [
          "3.4.1 WAL-FEC"
        ]
      }
    ]
  },
  {
    "commit": "82dfd4bf99283e5ab283073651787b3c7975561f",
    "change_groups": [
      {
        "summary": "Clarify cumulative WAL checksums: after first mismatch, frames i+1.. aren\"t WAL-validated; require random-access validation via WAL-FEC and attempt repair-before-truncate.",
        "categories": [
          2,
          7,
          4
        ],
        "primary_category": 2,
        "confidence": 0.86,
        "evidence": [
          "Because checksum is cumulative, WAL format alone cannot validate frames i+1..",
          "Attempt repair first if matching .wal-fec group exists"
        ],
        "changed_headings": [
          "7.5 WAL Checksums",
          "7.8 Error Recovery"
        ]
      },
      {
        "summary": "Correct SQLite header semantics: file change counter is updated when header page is written (not forced on every WAL commit).",
        "categories": [
          2,
          9
        ],
        "primary_category": 2,
        "confidence": 0.78,
        "evidence": [
          "In WAL mode, file change counter is NOT forced on every commit"
        ],
        "changed_headings": [
          "11.1 Database Header"
        ]
      }
    ]
  },
  {
    "commit": "ab20d7fac6f83ac527475927aa53b204c11ed7c8",
    "change_groups": [
      {
        "summary": "Prevent lock-table rebuild deadlocks: transactions holding page locks must not busy-wait on rebuild-busy; abort/retry to reach lock quiescence.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.8,
        "evidence": [
          "MUST NOT spin on SQLITE_BUSY while holding page locks; abort/retry"
        ],
        "changed_headings": [
          "PageLockTable Rebuild"
        ]
      },
      {
        "summary": "Add research note: view ARC p-update as online learning (OCO-style) while keeping canonical ARC update rules normative.",
        "categories": [
          8,
          7,
          6
        ],
        "primary_category": 8,
        "confidence": 0.64,
        "evidence": [
          "p_{t+1} = clamp(p_t + \u03b7_t * s_t, 0, capacity)"
        ],
        "changed_headings": [
          "6.4.1 Optional: p-Update as Online Learning"
        ]
      }
    ]
  },
  {
    "commit": "96f32f3174453e68960d849672cb4b7a8d33d1c7",
    "change_groups": [
      {
        "summary": "Harden UpdateExpression replay: enforce NOT NULL/CHECK semantics at replay time; restrict V1 to no foreign keys and rebase-safe CHECK constraints.",
        "categories": [
          4,
          1,
          2
        ],
        "primary_category": 4,
        "confidence": 0.86,
        "evidence": [
          "Constraint checks (normative): NOT NULL + CHECK enforced during replay",
          "Foreign keys (V1 restriction): fall back to materialized Update"
        ],
        "changed_headings": [
          "UpdateExpression"
        ]
      }
    ]
  },
  {
    "commit": "f37158f2468145272817fd65794d78be539be19b",
    "change_groups": [
      {
        "summary": "Add explicit WAL-FEC invariants tying group metadata to WAL frame geometry and commit db_size.",
        "categories": [
          7,
          4
        ],
        "primary_category": 7,
        "confidence": 0.75,
        "evidence": [
          "k_source == end_frame_no - start_frame_no + 1"
        ],
        "changed_headings": [
          "WalFecGroupMeta"
        ]
      },
      {
        "summary": "Fix UpdateExpression rebase step numbering after inserting constraint-check step.",
        "categories": [
          5
        ],
        "primary_category": 5,
        "confidence": 0.9,
        "evidence": [
          "Index regeneration step renumbered (6 -> 7)"
        ],
        "changed_headings": [
          "UpdateExpression"
        ]
      }
    ]
  },
  {
    "commit": "19fa01f6d66c19892c634a02da44f7f536b29e43",
    "change_groups": [
      {
        "summary": "Update built-in function coverage to match SQLite 3.52 target: add missing strftime specifiers and document ORDER BY inside group_concat/string_agg aggregates.",
        "categories": [
          2,
          6
        ],
        "primary_category": 2,
        "confidence": 0.84,
        "evidence": [
          "strftime adds: %e %k %I %l %p %P %R %T %u %G %g %V",
          "group_concat(name, ', ' ORDER BY name)"
        ],
        "changed_headings": [
          "Built-in Functions"
        ]
      }
    ]
  },
  {
    "commit": "a3e7ae521e6a0dcb3cf9489ac5a67b0f62aad526",
    "change_groups": [
      {
        "summary": "Clarify Compatibility-mode I/O constraints: WAL frames are 24+page_size and break sector alignment, so O_DIRECT must not be required for .wal.",
        "categories": [
          2,
          7,
          9
        ],
        "primary_category": 2,
        "confidence": 0.8,
        "evidence": [
          "SQLite .wal frames are 24+page_size; MUST NOT require O_DIRECT for .wal"
        ],
        "changed_headings": [
          "Mechanical Sympathy"
        ]
      },
      {
        "summary": "Tighten safety boundary: prefetch and VFS platform ops must use safe abstractions (or move behind external dependency boundary), not unsafe inside workspace crates.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.76,
        "evidence": [
          "Workspace members forbid unsafe; VFS MUST rely on safe SHM/locking APIs"
        ],
        "changed_headings": [
          "Unsafe Policy",
          "VFS"
        ]
      },
      {
        "summary": "Correct FTS5 query semantics: NOT is strictly a binary operator; unary NOT is a syntax error in FTS5 (unlike FTS3/4).",
        "categories": [
          2
        ],
        "primary_category": 2,
        "confidence": 0.86,
        "evidence": [
          "FTS5 NOT: expr NOT expr (binary only); unary NOT not in fts5parse.y"
        ],
        "changed_headings": [
          "FTS5"
        ]
      }
    ]
  },
  {
    "commit": "56a4e91425fa5cba9aecf08f786691ef03563750",
    "change_groups": [
      {
        "summary": "Define replication changeset_bytes encoding as a self-delimiting header + sorted PageEntry records with per-page xxh3; receivers validate hashes before applying.",
        "categories": [
          4,
          7,
          6
        ],
        "primary_category": 4,
        "confidence": 0.8,
        "evidence": [
          "ChangesetHeader.total_len (u64) before padding",
          "PageEntry: page_number + page_xxh3 + page_bytes"
        ],
        "changed_headings": [
          "Replication"
        ]
      }
    ]
  },
  {
    "commit": "d2a49862864dff9799e236d2cf81b51ed268f93f",
    "change_groups": [
      {
        "summary": "Harden crash cleanup and SQLite WAL-index interop: reclaim stuck CLEANING slots; enforce WAL_READ_LOCK discipline (exclusive update then shared hold) and state the 5-reader mark limitation explicitly.",
        "categories": [
          4,
          2,
          7
        ],
        "primary_category": 4,
        "confidence": 0.84,
        "evidence": [
          "Reclaim stuck TXN_ID_CLEANING after timeout",
          "Acquire WAL_READ_LOCK(i) EXCLUSIVE to write aReadMark, then downgrade to SHARED",
          "Legacy WAL-index has only 5 reader marks/locks"
        ],
        "changed_headings": [
          "5.6.2 TxnSlot",
          "Hybrid SHM Interop"
        ]
      },
      {
        "summary": "Fix ARC replacement liveness: track rotations per list so a pinned preferred list can\"t spin forever; add explicit fall-through path.",
        "categories": [
          1,
          7,
          4
        ],
        "primary_category": 1,
        "confidence": 0.8,
        "evidence": [
          "if rotations_t1 >= |T1| AND rotations_t2 >= |T2|: safety valve triggers"
        ],
        "changed_headings": [
          "ARC Cache"
        ]
      },
      {
        "summary": "Add deterministic safety directives: triggers must be non-recursive in Rust (explicit frame stack); correct conformal baseline sample size math under Bonferroni across M metrics.",
        "categories": [
          8,
          7,
          4
        ],
        "primary_category": 8,
        "confidence": 0.76,
        "evidence": [
          "Trigger execution MUST NOT use Rust call-stack recursion",
          "N_base >= ceil(M/alpha_total) to satisfy split conformal per-metric alpha"
        ],
        "changed_headings": [
          "Triggers",
          "Conformal No-Regression"
        ]
      }
    ]
  },
  {
    "commit": "859e81752809cd343b00ad6499c159d2f341cc46",
    "change_groups": [
      {
        "summary": "Ensure cleanup fully resets TxnSlot to a Free/Serialized state (not Aborted) when reclaiming or freeing slots.",
        "categories": [
          4,
          1
        ],
        "primary_category": 4,
        "confidence": 0.82,
        "evidence": [
          "slot.state = Free; slot.mode = Serialized during cleanup"
        ],
        "changed_headings": [
          "5.6.2 TxnSlot"
        ]
      }
    ]
  },
  {
    "commit": "be3d256b12679a32f2fb4e44eeb82e513607b284",
    "change_groups": [
      {
        "summary": "Specify Compatibility-mode .db-fec as a checkpoint-owned sidecar: only checkpointer mutates .db/.db-fec; enforce WAL truncation ordering; add independent per-source validation and repair writeback discipline.",
        "categories": [
          7,
          4,
          2,
          6
        ],
        "primary_category": 7,
        "confidence": 0.84,
        "evidence": [
          "V1 rule: .db-fec maintained ONLY by checkpoint subsystem",
          "WAL truncation safety: RESTART/TRUNCATE only after .db-fec updated+fsync'd",
          "DbFecGroupMeta.source_page_xxh3_128 for random-access validation"
        ],
        "changed_headings": [
          "3.4.6 .db-fec",
          "Read Path with On-the-Fly Repair"
        ]
      }
    ]
  },
  {
    "commit": "65ab2f709f724750878f2cb5d045649bf0c53e89",
    "change_groups": [
      {
        "summary": "Add out-of-the-box, bounded auto-tuning knobs and derived defaults to prevent self-DoS on many-core CPUs (bg_cpu_max, remote_max_in_flight, commit_encode_max; profile-based scaling).",
        "categories": [
          7,
          6
        ],
        "primary_category": 7,
        "confidence": 0.83,
        "evidence": [
          "PRAGMA fsqlite.auto_tune=ON (default) + PRAGMA fsqlite.profile",
          "Defaults scale sublinearly: bg_cpu_max_default = clamp(P/8,1,16)"
        ],
        "changed_headings": [
          "4.17.1 Out-of-the-Box Auto-Tuning"
        ]
      }
    ]
  },
  {
    "commit": "63ee097eb68a72ac23effab3979978adafb5b411",
    "change_groups": [
      {
        "summary": "Clarify SERIALIZABLE phantom protection: predicate/range reads must register leaf-page witnesses that intersect any phantom-producing writes.",
        "categories": [
          4,
          2,
          6
        ],
        "primary_category": 4,
        "confidence": 0.74,
        "evidence": [
          "Predicate reads MUST register witnesses (e.g., WitnessKey::Page(leaf_pgno))"
        ],
        "changed_headings": [
          "Witness Plane"
        ]
      },
      {
        "summary": "Warn against common SQLite WAL checksum mis-transcriptions that would break binary interoperability.",
        "categories": [
          2,
          1
        ],
        "primary_category": 2,
        "confidence": 0.74,
        "evidence": [
          "walChecksumBytes: s1 += a + s2; s2 += b + s1 (no avalanche)"
        ],
        "changed_headings": [
          "WAL Checksums"
        ]
      }
    ]
  },
  {
    "commit": "29f7ebe942a43891f2a55fd998d02c37411efc9f",
    "change_groups": [
      {
        "summary": "Place deterministic rebase outside the sequencer: committing txn must rebase before entering serialized commit section; coordinator validates certificates but must not execute B-tree/expression logic.",
        "categories": [
          4,
          7
        ],
        "primary_category": 4,
        "confidence": 0.84,
        "evidence": [
          "Deterministic rebase MUST run before entering WriteCoordinator serialized section",
          "coordinator MUST NOT perform B-tree traversal or index-key regeneration"
        ],
        "changed_headings": [
          "5.10 Safe Write Merging"
        ]
      },
      {
        "summary": "Harden UpdateExpression index regeneration: handle partial/expression indexes, participation predicates, and enforce UNIQUE constraints during replay.",
        "categories": [
          4,
          2,
          1
        ],
        "primary_category": 4,
        "confidence": 0.82,
        "evidence": [
          "Partial indexes: evaluate WHERE predicate; base/updated participation drives delete/insert"
        ],
        "changed_headings": [
          "UpdateExpression"
        ]
      },
      {
        "summary": "Replace Zipf-only policy input with model-free write-set skew estimation: define collision mass M2 / effective pool P_eff, add second-moment estimators and keep Zipf s_hat interpretability-only.",
        "categories": [
          8,
          7,
          9
        ],
        "primary_category": 8,
        "confidence": 0.8,
        "evidence": [
          "Collision mass M2 := \u03a3 q(pgno)^2; P_eff := 1/M2",
          "Zipf is interpretability-only; policy should prefer M2_hat/P_eff_hat"
        ],
        "changed_headings": [
          "18.4.1 Estimating Write-Set Skew Online"
        ]
      }
    ]
  },
  {
    "commit": "6b0c12fc40c5759f5d53028ba692e8aaff80fc92",
    "change_groups": [
      {
        "summary": "Correct planner join-ordering description: SQLite NGQP uses beam search (mxChoice), not an exhaustive search threshold; update the corresponding phase write-up.",
        "categories": [
          2,
          1,
          4
        ],
        "primary_category": 2,
        "confidence": 0.82,
        "evidence": [
          "wherePathSolver uses beam search; mxChoice=12/18 for 3+ tables"
        ],
        "changed_headings": [
          "Planner"
        ]
      }
    ]
  },
  {
    "commit": "b181b6d148e02862b195824020954abadae8de88",
    "change_groups": [
      {
        "summary": "Fix a second instance of the join-ordering error: beam search for all 3+ table joins, no exhaustive join enumeration path.",
        "categories": [
          2,
          1,
          5
        ],
        "primary_category": 2,
        "confidence": 0.8,
        "evidence": [
          "No greedy/exhaustive split; beam search modeled on SQLite NGQP"
        ],
        "changed_headings": [
          "Planner"
        ]
      }
    ]
  },
  {
    "commit": "d302b391af5c2932c9097dd85923a8535593e45e",
    "change_groups": [
      {
        "summary": "Add a witness hot-index sizing manifest driven by skew: introduce deterministic second-moment (AMS F2) sketch and heavy-hitter decomposition for explainability; use M2_shard to derive S_eff under skew.",
        "categories": [
          8,
          7,
          4,
          6
        ],
        "primary_category": 8,
        "confidence": 0.82,
        "evidence": [
          "M2_shard := \u03a3 q(shard)^2; S_eff := 1/M2_shard",
          "AMS F2 sketch (normative default)"
        ],
        "changed_headings": [
          "18.4.1.3 Estimator A",
          "HotWitnessIndex"
        ]
      }
    ]
  },
  {
    "commit": "017745631c79f0c9061e4fdba8d0ce09ecc6c86d",
    "change_groups": [
      {
        "summary": "Clarify skew section naming (write-set skew, not page access) and make commit-ledger requirements explicit when contention telemetry influences commit/abort and policy actions.",
        "categories": [
          9,
          8,
          6
        ],
        "primary_category": 9,
        "confidence": 0.72,
        "evidence": [
          "Ledger must include regime_id, writers_active, M2_hat/P_eff_hat, expected losses"
        ],
        "changed_headings": [
          "18.4 Non-Uniform Write-Set Skew",
          "Evidence Ledger"
        ]
      }
    ]
  },
  {
    "commit": "5dae90d79aef7d70300fe53a17f9e40dba24b309",
    "change_groups": [
      {
        "summary": "Tighten Zipf s_hat guidance: require deterministic windowing under LabRuntime and prohibit using s_hat as a direct policy input when M2_hat is available.",
        "categories": [
          9,
          7,
          8
        ],
        "primary_category": 9,
        "confidence": 0.72,
        "evidence": [
          "s_hat MUST NOT be used as direct policy input when M2_hat is available"
        ],
        "changed_headings": [
          "Zipf s_hat"
        ]
      }
    ]
  },
  {
    "commit": "ca60e008352585f2501ca3f81337849396b4d140",
    "change_groups": [
      {
        "summary": "Define Compatibility .db-fec physical layout for O(1) seek and specify crash-consistent group updates (meta-as-commit-record discipline).",
        "categories": [
          7,
          4,
          6
        ],
        "primary_category": 7,
        "confidence": 0.82,
        "evidence": [
          "segment_off = sizeof(DbFecHeader) + SEG1_LEN + g*SEGG_LEN",
          "Write SymbolRecords first, DbFecGroupMeta last"
        ],
        "changed_headings": [
          ".db-fec physical layout"
        ]
      }
    ]
  },
  {
    "commit": "30203fb1f91acfb45c6085432a66029896e26e66",
    "change_groups": [
      {
        "summary": "Reserve TxnId sentinel values for shared-memory protocol and guard TxnId allocation from ever producing them (fail fast on overflow).",
        "categories": [
          4,
          1
        ],
        "primary_category": 4,
        "confidence": 0.82,
        "evidence": [
          "TxnId allocation MUST never produce u64::MAX/u64::MAX-1 sentinels"
        ],
        "changed_headings": [
          "TxnId",
          "TxnSlot"
        ]
      }
    ]
  },
  {
    "commit": "75ac25db5853747e3f56877c943d51ce8d649a5e",
    "change_groups": [
      {
        "summary": "Clarify replication identifier semantics: rename changeset_object_id to ChangesetId, validate K_source and symbol_size consistency, and truncate decoded bytes using total_len.",
        "categories": [
          4,
          7,
          6
        ],
        "primary_category": 4,
        "confidence": 0.8,
        "evidence": [
          "ChangesetId is NOT ECS ObjectId; parse ChangesetHeader.total_len to drop padding"
        ],
        "changed_headings": [
          "Replication"
        ]
      },
      {
        "summary": "Harden TxnId allocation and witness semantics: forbid fetch_add wrap; use CAS loop to avoid publishing illegal TxnIds; tighten predicate-witness registration to include initial seek inspection.",
        "categories": [
          4,
          1,
          2
        ],
        "primary_category": 4,
        "confidence": 0.8,
        "evidence": [
          "fetch_add forbidden; CAS loop prevents wrap into TxnId=0",
          "Register leaf-page witnesses during initial Seek*/MoveTo and OP_Next/Prev"
        ],
        "changed_headings": [
          "TxnId",
          "Witness Plane"
        ]
      },
      {
        "summary": "Make ARC cache cancel-safe and singleflight-friendly: add flush_inflight flag + protocol, define Loading watch status enum, and require cancellation to resolve latches.",
        "categories": [
          7,
          4
        ],
        "primary_category": 7,
        "confidence": 0.8,
        "evidence": [
          "flush_inflight: false -> true via CAS; must clear on all paths",
          "Loading state uses watch::Receiver<LoadStatus>"
        ],
        "changed_headings": [
          "ARC Cache",
          "Cancellation Safety"
        ]
      }
    ]
  },
  {
    "commit": "ec9adc1a8f61663d224399711d165a7dd623b919",
    "change_groups": [
      {
        "summary": "Update the monotonic TxnId proof text to match the CAS-loop allocator and explicitly fail closed on wrap/reserved-sentinel values.",
        "categories": [
          1,
          4
        ],
        "primary_category": 1,
        "confidence": 0.8,
        "evidence": [
          "Abort with FATAL_TXN_ID_OVERFLOW rather than publish illegal TxnId"
        ],
        "changed_headings": [
          "TxnId Invariants"
        ]
      },
      {
        "summary": "Clarify interpretation of P_eff: it\"s an effective collision pool for transaction write sets, not an estimate of physical page count.",
        "categories": [
          9,
          8
        ],
        "primary_category": 9,
        "confidence": 0.76,
        "evidence": [
          "P_eff plays role of \"year length\" in birthday paradox for transactions"
        ],
        "changed_headings": [
          "18.4.1.1 Collision Mass (M2)"
        ]
      }
    ]
  },
  {
    "commit": "e80fdde018b3ce359f49271f082366bbad19f928",
    "change_groups": [
      {
        "summary": "Require deterministic RaptorQ block seeding for replication changesets by deriving the seed from ChangesetId (matches asupersync determinism expectations).",
        "categories": [
          3,
          7,
          4
        ],
        "primary_category": 3,
        "confidence": 0.8,
        "evidence": [
          "seed = xxh3_64(changeset_id_bytes); decoder stores seed"
        ],
        "changed_headings": [
          "Replication"
        ]
      }
    ]
  },
  {
    "commit": "fa25db0b24e84470e8271309370e0f8093463ddd",
    "change_groups": [
      {
        "summary": "Tighten TxnSlot acquisition safety: seed claiming_timestamp via CAS after slot claim; document publish-phase CAS to detect cleanup reclaim.",
        "categories": [
          4,
          1
        ],
        "primary_category": 4,
        "confidence": 0.82,
        "evidence": [
          "Phase 3 publish uses CAS(TXN_ID_CLAIMING -> real_txn_id); store is forbidden"
        ],
        "changed_headings": [
          "5.6.2 TxnSlot"
        ]
      },
      {
        "summary": "Expand ARC implementation guidance and ensure REPLACE terminates: treat prefer_t1 as a hint and fall back to the other list when preferred list is exhausted.",
        "categories": [
          7,
          4
        ],
        "primary_category": 7,
        "confidence": 0.78,
        "evidence": [
          "prefer_t1 is a hint, not a mandate; fallback to other list to ensure termination"
        ],
        "changed_headings": [
          "ARC Cache"
        ]
      },
      {
        "summary": "Adopt NGQP-style beam search join ordering for V1 (mxChoice bounded best-first search) and remove the exhaustive/greedy split narrative.",
        "categories": [
          2,
          4,
          7
        ],
        "primary_category": 2,
        "confidence": 0.84,
        "evidence": [
          "Maintain up to mxChoice best partial join paths; no N! exhaustive path"
        ],
        "changed_headings": [
          "Join Ordering"
        ]
      }
    ]
  },
  {
    "commit": "1d8bbfb40fa7a1fe90e710b593dfa2b3e9113c2d",
    "change_groups": [
      {
        "summary": "Make TxnSlot publish phase robust against cleanup races: publish real TxnId via CAS so a reclaimed slot can\"t be silently overwritten.",
        "categories": [
          4,
          1
        ],
        "primary_category": 4,
        "confidence": 0.82,
        "evidence": [
          "if !slot.txn_id.CAS(TXN_ID_CLAIMING, real_txn_id): restart_slot_acquire()"
        ],
        "changed_headings": [
          "TxnSlot acquire protocol"
        ]
      },
      {
        "summary": "Correct beam-search complexity statement to reflect bounded worst-case candidate expansions (~O(mxChoice*N^2)).",
        "categories": [
          1,
          2
        ],
        "primary_category": 1,
        "confidence": 0.8,
        "evidence": [
          "Complexity: worst-case ~O(mxChoice * N^2) candidate expansions"
        ],
        "changed_headings": [
          "Join Ordering"
        ]
      }
    ]
  },
  {
    "commit": "4432a3daa2f875bc2d3fe47caf783b3eb286ebfe",
    "change_groups": [
      {
        "summary": "Make snapshots self-consistent across (commit_seq, schema_epoch) and enforce schema-epoch publication ordering to prevent mixed-schema rebase.",
        "categories": [
          4,
          1,
          7
        ],
        "primary_category": 4,
        "confidence": 0.82,
        "evidence": [
          "load_consistent_snapshot() reads commit_seq twice around schema_epoch"
        ],
        "changed_headings": [
          "5.4 BEGIN",
          "Memory ordering"
        ]
      },
      {
        "summary": "Replace toy WAL timing constants with measured distributions (fsync dominates); require histograms and policy-driven batching; add I/O-stall semantics that prefer safety over liveness.",
        "categories": [
          7,
          6,
          4
        ],
        "primary_category": 7,
        "confidence": 0.8,
        "evidence": [
          "T_wal = T_wal_write + T_fsync + T_wal_overhead; fsync can be multi-ms",
          "Coordinator MUST record histogram of T_fsync"
        ],
        "changed_headings": [
          "Group Commit",
          "Cancellation Safety"
        ]
      },
      {
        "summary": "Add a normative conformance mode matrix: fixtures must declare required operating modes (compatibility/native) and CI must check both against Oracle and each other.",
        "categories": [
          6,
          4,
          5
        ],
        "primary_category": 6,
        "confidence": 0.8,
        "evidence": [
          "Fixture field: fsqlite_modes: [\"compatibility\",\"native\"] (default both)"
        ],
        "changed_headings": [
          "Harness",
          "Mode matrix"
        ]
      }
    ]
  },
  {
    "commit": "aa8e81601a80b6910a3791490d07c5691b517850",
    "change_groups": [
      {
        "summary": "Tighten serialized-mode correctness: add FCW freshness validation (reader-turned-writer stale snapshot must abort) and simplify commit sequencing in the COMMIT table.",
        "categories": [
          4,
          1,
          2
        ],
        "primary_category": 4,
        "confidence": 0.82,
        "evidence": [
          "Commit (Serialized): FCW freshness validation still required; abort with SQLITE_BUSY_SNAPSHOT"
        ],
        "changed_headings": [
          "5.4 COMMIT semantics"
        ]
      },
      {
        "summary": "Harden open-sequence invariants: initialize and reconcile shm.schema_epoch from durable schema cookie/manifest and forbid shared-memory schema_epoch ahead of durable reality.",
        "categories": [
          4,
          2
        ],
        "primary_category": 4,
        "confidence": 0.8,
        "evidence": [
          "On open: set shm.schema_epoch to durable schema cookie (offset 40)"
        ],
        "changed_headings": [
          "SharedMemoryLayout",
          "Schema epoch"
        ]
      },
      {
        "summary": "Refine rebase read-footprint semantics: footprint.reads captures non-replayable blocking reads; uniqueness checks for written keys are revalidated during replay and must not be recorded as blocking reads.",
        "categories": [
          4,
          9
        ],
        "primary_category": 4,
        "confidence": 0.78,
        "evidence": [
          "Do NOT include uniqueness checks in footprint.reads; re-validated during replay"
        ],
        "changed_headings": [
          "IntentFootprint"
        ]
      }
    ]
  },
  {
    "commit": "0a8d8676ac74f994535eceaa1c6c47cb17868038",
    "change_groups": [
      {
        "summary": "Make TxnSlot crash cleanup retryable: add cleanup_txn_id, broaden claiming_timestamp to sentinel states, and define release_page_locks_for(txn_id) for crashed writers.",
        "categories": [
          1,
          7,
          4
        ],
        "primary_category": 1,
        "confidence": 0.9,
        "evidence": [
          "cleanup_txn_id preserves the original TxnId under TXN_ID_CLEANING so crash cleanup can be retried safely.",
          "release_page_locks_for(txn_id) scans SharedPageLockTable and CASes owner_txn from txn_id to 0 without clearing the key.",
          "claiming_timestamp now tracks time spent in sentinel states (CLAIMING/CLEANING) for stuck-slot detection."
        ],
        "changed_headings": [
          "5.6.2 TxnSlot",
          "5.6.3 SharedPageLockTable",
          "5.8 Conflict Detection and Resolution Detail"
        ]
      },
      {
        "summary": "Reconcile single-process vs multi-process lock semantics: SharedPageLockTable is canonical in Concurrent mode; in-process lock tables are reference-only; resolve() must materialize committed versions from durable storage when caches are stale.",
        "categories": [
          4,
          7,
          9
        ],
        "primary_category": 4,
        "confidence": 0.86,
        "evidence": [
          "Cross-process writer exclusion is enforced via SharedPageLockTable in shm (not a per-process HashMap).",
          "InProcessPageLockTable is permitted only as a single-process reference implementation / unit-test harness.",
          "resolve(pgno, snapshot) consults WAL/marker stream to materialize committed versions when the in-process version chain cache is missing or stale."
        ],
        "changed_headings": [
          "MVCC resolve()",
          "5.4 COMMIT semantics",
          "5.6.3 SharedPageLockTable"
        ]
      },
      {
        "summary": "Tighten deterministic rebase correctness: treat existence/uniqueness probes as blocking reads for branchy conflict policies (OR IGNORE/REPLACE/UPSERT) unless the chosen branch is encoded in intent.",
        "categories": [
          4,
          2,
          1
        ],
        "primary_category": 4,
        "confidence": 0.82,
        "evidence": [
          "For OR IGNORE/REPLACE/UPSERT, probes can change observable behavior; replay must not silently take a different branch.",
          "Until intent encodes the chosen branch, V1 deterministic rebase requires recording the probe in footprint.reads (blocking) or forbids the op."
        ],
        "changed_headings": [
          "5.10.2 Deterministic Rebase",
          "IntentFootprint"
        ]
      },
      {
        "summary": "Correct schema-cookie assumptions: the SQLite schema cookie is a 32-bit counter modulo 2^32; merge safety requires equality checks, not numeric monotonicity.",
        "categories": [
          2,
          1,
          9
        ],
        "primary_category": 2,
        "confidence": 0.78,
        "evidence": [
          "SQLite schema cookie increments modulo 2^32; numeric decreases are not necessarily corruption.",
          "For safety we require schema cookie equality across participants; any change must produce a different cookie."
        ],
        "changed_headings": [
          "Schema epoch / schema cookie"
        ]
      }
    ]
  },
  {
    "commit": "3d568547fd6b8036481df0d73a2c60b5bf30d5a7",
    "change_groups": [
      {
        "summary": "Scrivening + minor clarification: normalize Vfs trait doc formatting and refine cleanup_txn_id comments (ignored unless txn_id==TXN_ID_CLEANING; should be zeroed on free).",
        "categories": [
          5,
          9
        ],
        "primary_category": 5,
        "confidence": 0.96,
        "evidence": [
          "Reindent Vfs trait docs for consistent Rustdoc rendering.",
          "cleanup_txn_id is meaningful only when txn_id==TXN_ID_CLEANING; otherwise ignored; must be zeroed when freeing the slot."
        ],
        "changed_headings": [
          "5.6.2 TxnSlot",
          "VFS trait"
        ]
      }
    ]
  },
  {
    "commit": "4c07e10c56ee30db5d03453237ae6c29c31a7cf0",
    "change_groups": [
      {
        "summary": "Clarify deterministic rebase semantics for rowid reuse: re-execution is defined on the semantic key (rowid); reused rowids can be updated and this matches commit-time serial order semantics.",
        "categories": [
          2,
          9,
          4
        ],
        "primary_category": 2,
        "confidence": 0.84,
        "evidence": [
          "SQLite rowids may be reused unless AUTOINCREMENT is used.",
          "Deterministic rebase is merge-by-reexecution on the semantic key; a delete/insert that reuses a rowid can cause replay to update the new row at that key."
        ],
        "changed_headings": [
          "5.10.2 Deterministic Rebase"
        ]
      },
      {
        "summary": "Harden page-encryption metadata + AAD rules: define stable DatabaseId, keep AAD independent of encrypted bytes (no circular dependencies), and allow optional pre-decrypt page_context_tag only when known.",
        "categories": [
          4,
          7,
          6
        ],
        "primary_category": 4,
        "confidence": 0.86,
        "evidence": [
          "DatabaseId is generated once and remains stable across the database lifetime including PRAGMA rekey.",
          "AAD inputs must be known before decryption; do not derive AAD from encrypted page bytes such as B-tree flags.",
          "Optional page_context_tag is permitted only when known pre-decrypt; otherwise use a fixed constant."
        ],
        "changed_headings": [
          "Page Encryption",
          "AAD (swap resistance)"
        ]
      },
      {
        "summary": "Ministerial cleanup: normalize indentation in VersionArena / ARC prose blocks and sharpen reserved-space checksum interoperability wording.",
        "categories": [
          5,
          9,
          2
        ],
        "primary_category": 5,
        "confidence": 0.76,
        "evidence": [
          "Normalize indentation in long code-comment blocks for readability.",
          "Clarify that C SQLite can read reserved-space checksums (reserved bytes are opaque) but default remains OFF for interoperability."
        ],
        "changed_headings": [
          "VersionArena",
          "ARC/CAR",
          "Reserved-space checksums"
        ]
      }
    ]
  },
  {
    "commit": "df0313b00f2854929c376434c56129ef6e2740e0",
    "change_groups": [
      {
        "summary": "Scrivening: fix ARC/CAR commentary indentation so rendered docs align with surrounding prose.",
        "categories": [
          5
        ],
        "primary_category": 5,
        "confidence": 0.99,
        "evidence": [
          "Indentation-only change in ARC/CAR explanatory comments."
        ],
        "changed_headings": [
          "ARC/CAR"
        ]
      }
    ]
  },
  {
    "commit": "97df1f07893def20701570db65418ff75ed45656",
    "change_groups": [
      {
        "summary": "Clarify what 'zero-copy' means in this spec: no extra heap allocations or staging copies in hot paths, but not kernel-bypass I/O; small stack buffers are allowed.",
        "categories": [
          9,
          6
        ],
        "primary_category": 9,
        "confidence": 0.9,
        "evidence": [
          "Zero-copy excludes userspace staging buffers/allocations, not buffered syscalls.",
          "Fixed-size header stack buffers are permitted."
        ],
        "changed_headings": [
          "1.5 Hot-path constraints"
        ]
      }
    ]
  },
  {
    "commit": "bbc4a3114572200d7a8a674eb3a4e430ae1d0b47",
    "change_groups": [
      {
        "summary": "Define canonical page-encryption AAD bytes: be_u32(page_number) || database_id_bytes; forbid native-endian encodings for cross-endian opens.",
        "categories": [
          4,
          7,
          5
        ],
        "primary_category": 4,
        "confidence": 0.92,
        "evidence": [
          "aad = be_u32(page_number) || database_id_bytes (normative).",
          "Native-endian integer encoding is forbidden for AAD (cross-endian open must work)."
        ],
        "changed_headings": [
          "Page Encryption",
          "AAD (swap resistance)"
        ]
      }
    ]
  },
  {
    "commit": "4363f50065239e47d56f12250d933f5a1ab75a00",
    "change_groups": [
      {
        "summary": "Add a cross-cutting 'Critical Implementation Controls' checklist to make corruption/deadlock-sensitive invariants impossible to miss.",
        "categories": [
          6,
          7,
          4
        ],
        "primary_category": 6,
        "confidence": 0.88,
        "evidence": [
          "Hybrid SHM interop requires legacy WAL_READ_LOCK/WAL_WRITE_LOCK protocol (not just layout).",
          "Witness instrumentation must be semantic/sub-page for point ops (avoid whole-page reads that collapse merge/rebase).",
          "RaptorQ repair generation must be off the commit critical path; rebuild quiescence is 'no lock holders', not 'no txns'."
        ],
        "changed_headings": [
          "1.6 Critical Implementation Controls (Non-Negotiable)"
        ]
      },
      {
        "summary": "Tighten TxnSlot cleanup: stamp a fresh sentinel-time when transitioning into TXN_ID_CLEANING so stuck-cleaner detection measures time in CLEANING (not inherited CLAIMING time).",
        "categories": [
          1,
          7
        ],
        "primary_category": 1,
        "confidence": 0.84,
        "evidence": [
          "On CAS into TXN_ID_CLEANING, set claiming_timestamp = now to start the CLEANING timeout window.",
          "Overwrite stale CLAIMING timestamps when entering CLEANING."
        ],
        "changed_headings": [
          "5.6.2 TxnSlot cleanup"
        ]
      }
    ]
  },
  {
    "commit": "d9021cffb65692624f3990ae2544a96ae0c07021",
    "change_groups": [
      {
        "summary": "Clarification pass: make rowid reuse semantics explicit as a serial-order effect for deterministic rebase, and define DatabaseId precisely as 16 opaque bytes stable across rekey.",
        "categories": [
          9,
          2,
          4
        ],
        "primary_category": 9,
        "confidence": 0.9,
        "evidence": [
          "Rowid reuse is expected: replay matches serial order (delete/insert then update) rather than implying corruption.",
          "DatabaseId is 16 opaque bytes (not host-endian integer) and must remain stable across PRAGMA rekey."
        ],
        "changed_headings": [
          "5.10.2 Deterministic Rebase",
          "Page Encryption"
        ]
      }
    ]
  },
  {
    "commit": "29107df6b155bb20934ef369b934699191076b32",
    "change_groups": [
      {
        "summary": "Draw a hard durability boundary: ARC eviction is memory-only and MUST NOT append to .wal; only the write coordinator may append WAL frames, with large write sets spilled to per-txn temp files.",
        "categories": [
          4,
          7,
          1
        ],
        "primary_category": 4,
        "confidence": 0.9,
        "evidence": [
          "ARC eviction MUST NOT append to .wal; legacy WAL commit markers assume coordinator-only contiguous appends.",
          "Uncommitted/private page images live in txn write_set and are spillable to a per-txn spill file in Compatibility mode."
        ],
        "changed_headings": [
          "6.6 Eviction: Pinned Pages and Durability Boundaries",
          "5.9.2 Write-set spill"
        ]
      },
      {
        "summary": "Simplify ARC REQUEST/REPLACE behavior around pinned pages: remove flush-dirty loops, allow temporary over-capacity as a safety valve when all candidates are pinned, and update resize protocol accordingly.",
        "categories": [
          7,
          4
        ],
        "primary_category": 7,
        "confidence": 0.82,
        "evidence": [
          "Eviction scans skip only pinned pages (ref_count > 0); durability I/O is not part of eviction.",
          "When all pages are pinned, permit bounded capacity overflow rather than spinning forever."
        ],
        "changed_headings": [
          "6.4 Full ARC Algorithm: REQUEST Subroutine",
          "6.6 Eviction: Pinned Pages and Durability Boundaries"
        ]
      },
      {
        "summary": "Clarify interoperability boundary for encryption: encrypted databases are not readable by stock C SQLite; fail closed rather than attempting legacy interop on ciphertext pages.",
        "categories": [
          9,
          2,
          4
        ],
        "primary_category": 9,
        "confidence": 0.78,
        "evidence": [
          "Encrypted databases are not readable by stock C SQLite; compatibility interop applies only to plaintext.",
          "If encryption is enabled, FrankenSQLite fails closed rather than letting legacy clients treat ciphertext as page bytes."
        ],
        "changed_headings": [
          "Page Encryption",
          "Compatibility mode interoperability"
        ]
      }
    ]
  },
  {
    "commit": "f708f338cb6e435db3822e2aa81b2785b51bf39b",
    "change_groups": [
      {
        "summary": "Clarify pipelined WAL-FEC semantics: commits can be durable before they are repairable; if wal-fec metadata is missing at recovery, fall back to SQLite truncation semantics; optional synchronous mode can require immediate repairability.",
        "categories": [
          7,
          4,
          9
        ],
        "primary_category": 7,
        "confidence": 0.86,
        "evidence": [
          "Eventual repairability: a group is repairable only once WalFecGroupMeta + repair symbols are durable.",
          "If wal-fec metadata is missing, recovery truncates before the affected group (SQLite semantics).",
          "An opt-in synchronous mode may wait for wal-fec fsync before acknowledging COMMIT."
        ],
        "changed_headings": [
          "3.4.1 Self-Healing WAL (Erasure-Coded Durability)"
        ]
      },
      {
        "summary": "Specify Compatibility-mode write-set spilling and coordinator-only WAL append: introduce CommitWriteSet (Inline vs Spilled) and PRAGMA fsqlite.txn_write_set_mem_bytes with a bounded auto default.",
        "categories": [
          4,
          7,
          6
        ],
        "primary_category": 4,
        "confidence": 0.88,
        "evidence": [
          "CommitRequest.write_set becomes CommitWriteSet::Inline(...) or CommitWriteSet::Spilled(...).",
          "Spill files are temporary and explicitly not durability / crash recovery.",
          "Auto spill threshold: clamp(4 * cache.max_bytes, 32 MiB, 512 MiB).",
          "Only the write coordinator may append to .wal to avoid interleaving corruption."
        ],
        "changed_headings": [
          "5.9.2 Write Coordinator",
          "6.6 Eviction: Pinned Pages and Durability Boundaries"
        ]
      },
      {
        "summary": "Add a V1 rule for OP_NewRowid under concurrent writers: allocate rowids from a snapshot-independent per-table allocator and record the concrete RowId in the Insert intent at execution time.",
        "categories": [
          4,
          2,
          7
        ],
        "primary_category": 4,
        "confidence": 0.84,
        "evidence": [
          "Concurrent mode cannot use max(rowid)+1 per snapshot; writers would collide.",
          "RowId must come from a global allocator and be stable for statement/transaction (preserves last_insert_rowid/RETURNING)."
        ],
        "changed_headings": [
          "5.10.1.1 RowId Allocation in Concurrent Mode (Avoid the Pre-Binding Trap)"
        ]
      }
    ]
  },
  {
    "commit": "a71e1d95715c1190335e8738413382b31b6d167c",
    "change_groups": [
      {
        "summary": "Correct the RFC 6330 LDPC constraint description: each source column updates exactly three LDPC rows using stride a=1+floor(j/S), implying total nonzeros=3*K'.",
        "categories": [
          1,
          6
        ],
        "primary_category": 1,
        "confidence": 0.82,
        "evidence": [
          "For j in 0..K'-1: a=1+floor(j/S); b=j%S; set A[b][j]=1 then advance b=(b+a)%S twice more.",
          "Each source column contributes exactly 3 nonzeros; average LDPC row has ~3*K'/S nonzeros."
        ],
        "changed_headings": [
          "RaptorQ LDPC rows",
          "RFC 6330 §5.3.3.3"
        ]
      },
      {
        "summary": "Harden WAL-FEC metadata + recovery: require per-source xxh3_128 validation hashes, add SQLite fallback when wal-fec is missing, and widen OTI.T to u32 to represent page_size=65536 with explicit corruption invariants.",
        "categories": [
          4,
          7,
          1
        ],
        "primary_category": 4,
        "confidence": 0.9,
        "evidence": [
          "If wal-fec metadata is missing for a torn group, recovery falls back to SQLite truncation semantics.",
          "WalFecGroupMeta stores source_page_xxh3_128: Vec<[u8;16]> for random-access validation.",
          "OTI.T is widened to u32 so symbol_size can equal page_size=65536; mismatches are treated as corruption."
        ],
        "changed_headings": [
          "3.4.1 Self-Healing WAL",
          "WalFecGroupMeta",
          "SymbolRecord / OTI"
        ]
      },
      {
        "summary": "Make ECS root pointer updates truly crash-safe: fsync temp file before rename and fsync the directory after rename to persist the rename itself.",
        "categories": [
          7,
          1
        ],
        "primary_category": 7,
        "confidence": 0.88,
        "evidence": [
          "Crash-safe sequence: write temp, fsync(temp), rename(temp, ecs/root), fsync(directory).",
          "Omitting fsync(temp) risks garbage content; omitting fsync(dir) risks losing the rename after crash."
        ],
        "changed_headings": [
          "ECS directory layout",
          "ecs/root"
        ]
      },
      {
        "summary": "Fix TxnSlot cleanup race: snapshot txn_id once per slot iteration (Acquire load) to avoid branching on multiple unsynchronized reads while sentinels are changing.",
        "categories": [
          1,
          7
        ],
        "primary_category": 1,
        "confidence": 0.86,
        "evidence": [
          "Snapshot txn_id once: tid = slot.txn_id.load(Acquire); skip tid==0 early.",
          "Avoid freeing a slot while another cleaner is still releasing locks due to inconsistent reads of sentinel states."
        ],
        "changed_headings": [
          "5.6.2 TxnSlot cleanup"
        ]
      },
      {
        "summary": "Clarify ESCAPE handling in the Pratt parser: ESCAPE is not an infix operator; it is parsed as an optional suffix of LIKE/GLOB/MATCH productions (parse.y %right ESCAPE is for Lemon conflict resolution).",
        "categories": [
          2,
          9,
          5
        ],
        "primary_category": 2,
        "confidence": 0.8,
        "evidence": [
          "ESCAPE is parsed inside the LIKE/GLOB handler after the pattern expression, not in the infix dispatch table.",
          "C SQLite declares %right ESCAPE for conflict resolution, but it is not a standalone expression operator."
        ],
        "changed_headings": [
          "SQL operator precedence",
          "LIKE/GLOB ESCAPE handling"
        ]
      }
    ]
  },
  {
    "commit": "120eee252b4caa7fe6389602bd2c8a316c6cee2a",
    "change_groups": [
      {
        "summary": "Correct GF(256) elimination language: use nonzero pivot selection (no rounding error over fields), avoiding misleading 'partial pivoting' phrasing.",
        "categories": [
          1,
          9
        ],
        "primary_category": 1,
        "confidence": 0.9,
        "evidence": [
          "Over exact fields, pivot choice is simply selecting any nonzero pivot; numerical stability concerns do not apply."
        ],
        "changed_headings": [
          "GF(256) Gaussian solve"
        ]
      },
      {
        "summary": "Strengthen random-access WAL-FEC source-frame validation: compute xxh3_128(page_data) and compare against source_page_xxh3_128 before feeding symbols into the decoder.",
        "categories": [
          7,
          4,
          1
        ],
        "primary_category": 7,
        "confidence": 0.88,
        "evidence": [
          "Validate candidate frames with xxh3_128 and compare to WalFecGroupMeta.source_page_xxh3_128.",
          "Random-access validation remains valid even when the cumulative WAL checksum chain is broken."
        ],
        "changed_headings": [
          "3.4.1 Self-Healing WAL",
          "WAL recovery"
        ]
      }
    ]
  },
  {
    "commit": "975f65c78a5745424665a95f0c222287306c9dd5",
    "change_groups": [
      {
        "summary": "Clarify GF(256) elimination commentary: in exact fields there is no rounding error, so the pivot rule is nonzero selection, not numerical 'partial pivoting'.",
        "categories": [
          1,
          9
        ],
        "primary_category": 1,
        "confidence": 0.9,
        "evidence": [
          "Explicitly note that pivoting is for finding a nonzero pivot; there is no floating-point stability issue over GF(256)."
        ],
        "changed_headings": [
          "GF(256) Gaussian elimination"
        ]
      },
      {
        "summary": "Add replication endianness boundary note: UDP packet headers use big-endian network order, while the decoded changeset payload uses little-endian per canonical encoding rules.",
        "categories": [
          6,
          7
        ],
        "primary_category": 6,
        "confidence": 0.84,
        "evidence": [
          "Replication packet header fields are big-endian; the boundary is symbol_data, whose decoded changeset payload is little-endian."
        ],
        "changed_headings": [
          "UDP Packet Format",
          "Canonical encodings"
        ]
      },
      {
        "summary": "Bound version-chain delta reconstruction cost and clarify ECS root file format: chain depth is kept small by GC scheduling targets, and ecs/root includes a magic+version prefix with manifest id + checksum.",
        "categories": [
          7,
          4
        ],
        "primary_category": 7,
        "confidence": 0.86,
        "evidence": [
          "Reconstructing the oldest version in a chain of depth L requires L-1 sequential delta applications; GC scheduling targets depth ~8.",
          "ecs/root contains: magic (\"FSRT\"), version, manifest_object_id, checksum."
        ],
        "changed_headings": [
          "Version chain deltas",
          "ECS root file format"
        ]
      }
    ]
  },
  {
    "commit": "24b6f60e9e751b699cb232759d85a06c42792019",
    "change_groups": [
      {
        "summary": "Fix GC scheduling cross-references: point Theorem-5 discussion at the GC scheduling policy section (5.6.5), not the lock-table section.",
        "categories": [
          5,
          9
        ],
        "primary_category": 5,
        "confidence": 0.95,
        "evidence": [
          "Replace incorrect §5.6.3 references with §5.6.5 for GC scheduling policy citations."
        ],
        "changed_headings": [
          "GC scheduling policy",
          "Theorem 5 discussion"
        ]
      }
    ]
  },
	  {
	    "commit": "80decf6b8ba71dd4331f559a08ee0fba3fbdf4bb",
	    "change_groups": [
	      {
        "summary": "Clarify .db-fec generation binding and repair terminology: specify db_gen_digest derivation inputs, use RFC 6330 ESI naming for symbols, and make commit_time_unix_ns monotonicity enforcement explicit.",
        "categories": [
          9,
          5,
          1,
          7
        ],
        "primary_category": 9,
        "confidence": 0.86,
        "evidence": [
          "db_gen_digest comment now specifies Trunc128(BLAKE3(\"fsqlite:compat:dbgen:v1\" || change_counter || page_count || freelist_count || schema_cookie)) with big-endian u32 reads from header offsets.",
          "Repair pseudocode switches from ISI to ESI terminology and uses repair_rec.esi rather than deriving ids from loop indices.",
          "Binary-search-by-time note now cites the protocol rule: commit_time_unix_ns := max(now_unix_ns(), last_commit_time_unix_ns + 1)."
        ],
        "changed_headings": [
          "Compatibility DB-FEC",
          "Repair pseudocode",
          "Commit protocol time"
        ]
	      }
	    ]
	  },
	  {
	    "commit": "7cc726325faa5cad71d2132480cbb68fb046563d",
	    "change_groups": [
	      {
	        "summary": "Add `.db-fec` freshness and foreign-sidecar guardrails: define db_gen_digest derivation from DB header, require digest match before using repair symbols, and make the .db-fec header update the commit record (fsync-ordered).",
	        "categories": [
	          7,
	          1,
	          2,
	          4
	        ],
	        "primary_category": 7,
	        "confidence": 0.85,
	        "evidence": [
	          "Verify DbFecHeader.checksum and require db_gen_digest_current == DbFecHeader.db_gen_digest before using any .db-fec metadata.",
	          "If repairing page 1, recompute digest from repaired bytes; mismatch fails closed (SQLITE_CORRUPT).",
	          "Checkpoint must fsync .db, then write db_gen_digest + header checksum, then fsync .db-fec before WAL RESTART/TRUNCATE."
	        ],
	        "changed_headings": [
	          "Compatibility DB-FEC",
	          "DbFecHeader",
	          "DbFecGroupMeta"
	        ]
	      },
	      {
	        "summary": "Harden shared-memory coordination: introduce snapshot_seq seqlock for consistent snapshot capture, add serialized-writer indicator fields (token/pid/lease), and define a stale-indicator clearing algorithm for concurrent writers.",
	        "categories": [
	          4,
	          7,
	          1
	        ],
	        "primary_category": 4,
	        "confidence": 0.8,
	        "evidence": [
	          "SharedMemoryLayout gains snapshot_seq (odd/even seqlock) plus serialized_writer_token/pid/pid_birth/lease_expiry.",
	          "Snapshot publish protocol specified: even -> odd -> even around backbone field stores; openers repair crash-stale odd values.",
	          "Concurrent writers check indicator and may clear stale tokens after lease expiry / owner death."
	        ],
	        "changed_headings": [
	          "5.6.1 Shared-Memory Coordination Region",
	          "Serialized writer acquisition ordering"
	        ]
	      },
	      {
	        "summary": "Replace constant TxnSlot sentinels with a tagged txn_id word to prevent ABA claim/cleanup races; update cleanup_orphaned_slots and gc_horizon logic to treat sentinel-tagged slots as blockers.",
	        "categories": [
	          4,
	          1,
	          7
	        ],
	        "primary_category": 4,
	        "confidence": 0.85,
	        "evidence": [
	          "TxnSlot.txn_id reserves the top 2 bits as a tag: Active/CLAIMING/CLEANING; Phase 1 uses claim_word=encode_claiming(txn_id).",
	          "Phase 3 publish is CAS(claim_word -> real_txn_id); claim timestamps are cleared after publish.",
	          "cleanup_orphaned_slots branches on decode_tag(tid) and transitions to encode_cleaning(payload)."
	        ],
	        "changed_headings": [
	          "5.6.2 TxnSlot",
	          "cleanup_orphaned_slots()",
	          "raise_gc_horizon()"
	        ]
	      },
	      {
	        "summary": "Make recently-committed-readers summaries cross-process stable: specify a fixed-layout SHM ring with a per-entry Bloom filter (fail-closed on overflow) instead of RoaringBitmap-in-SHM.",
	        "categories": [
	          7,
	          4
	        ],
	        "primary_category": 7,
	        "confidence": 0.78,
	        "evidence": [
	          "RecentlyCommittedReadersRing lives at committed_readers_offset and uses commit_seq as a publication word (0=unpublished).",
	          "Each entry includes a 4096-bit Bloom filter with k=3 probes over pgno (xxh3_64 domain-separated).",
	          "If inserting would evict an entry with commit_seq > gc_horizon, the committer aborts with SQLITE_BUSY_SNAPSHOT (no false negatives)."
	        ],
	        "changed_headings": [
	          "RecentlyCommittedReadersRing",
	          "Committed readers index"
	        ]
	      },
	      {
	        "summary": "Redesign shared page-lock rebuild as a rolling rotate+drain protocol using two tables (active+draining) to avoid stop-the-world abort storms; update acquire/release/crash-cleanup to consult both tables.",
	        "categories": [
	          4,
	          7
	        ],
	        "primary_category": 4,
	        "confidence": 0.8,
	        "evidence": [
	          "SharedPageLockTable gains active_table/draining_table and two LockTableInstance tables.",
	          "try_acquire consults draining table first for idempotent re-acquire and conflict detection; release probes both tables.",
	          "Rebuild rotates quickly, drains in background, and clears only after lock-quiescence; capacity defaults to 1,048,576 entries."
	        ],
	        "changed_headings": [
	          "5.6.3.1 Table Rebuild",
	          "SharedPageLockTable"
	        ]
	      }
	    ]
	  },
	  {
	    "commit": "9ad50ae2a2c7fce76a75d1200e22374eb729914d",
	    "change_groups": [
	      {
	        "summary": "Specify the coordinator IPC transport for multi-process deployments: Unix domain sockets, framed messages, two-phase reserve/submit discipline, SCM_RIGHTS spill-fd passing, and TxnToken idempotency.",
	        "categories": [
	          4,
	          7,
	          6
	        ],
	        "primary_category": 4,
	        "confidence": 0.82,
	        "evidence": [
	          "Defines socket endpoints, permissions, and peer credential checks (UID) for coordinator IPC.",
	          "Framing: len_be + version/kind/request_id + payload; RESERVE returns permit_id or BUSY; SUBMIT_* binds to permit_id.",
	          "Large write sets are passed via spill fd using SCM_RIGHTS; duplicates keyed by (txn_id, txn_epoch) are idempotent."
	        ],
	        "changed_headings": [
	          "5.9.0 Coordinator IPC Transport"
	        ]
	      },
	      {
	        "summary": "Make snapshot capture explicitly seqlock-based: load_consistent_snapshot reads snapshot_seq, retries on odd or change, and returns a self-consistent (commit_seq, schema_epoch) pair.",
	        "categories": [
	          4,
	          1,
	          7
	        ],
	        "primary_category": 4,
	        "confidence": 0.78,
	        "evidence": [
	          "Read s1=snapshot_seq; if odd retry; read commit_seq + schema_epoch; read s2; accept only if s1==s2 and even.",
	          "Cross-process snapshot correctness is now specified as a seqlock read, not two commit_seq loads."
	        ],
	        "changed_headings": [
	          "Snapshot capture",
	          "load_consistent_snapshot()"
	        ]
	      },
	      {
	        "summary": "Clarify in-process vs cross-process commit schemas and harden spill semantics: use OwnedFd for spill transfer, allow optional diagnostic path, and recommend unlink-after-open for crash robustness.",
	        "categories": [
	          7,
	          4,
	          6
	        ],
	        "primary_category": 7,
	        "confidence": 0.75,
	        "evidence": [
	          "Adds normative notes: cross-process routing MUST NOT attempt to transmit Vec/HashMap/oneshot through shared memory.",
	          "SpilledWriteSet carries an OwnedFd; cross-process commits MUST use CommitWriteSet::Spilled with SCM_RIGHTS fd passing.",
	          "Recommends creator unlink spill file after open so cleanup is automatic on crash."
	        ],
	        "changed_headings": [
	          "CommitRequest (compatibility/WAL)",
	          "Write-set spilling"
	        ]
	      }
	    ]
	  },
	  {
	    "commit": "19106d19e531ea75918241b2858024c34a73f037",
	    "change_groups": [
	      {
	        "summary": "Expand and harden the TxnSlot acquire/publish protocol: make the 3-phase claim/init/publish sequence explicit and require begin_seq/snapshot_high derive from a self-consistent snapshot (seqlock).",
	        "categories": [
	          4,
	          7
	        ],
	        "primary_category": 4,
	        "confidence": 0.82,
	        "evidence": [
	          "acquire_and_publish_txn_slot() wraps claim (CAS 0->claim_word), initializes slot fields, captures snap via load_consistent_snapshot, then publishes real TxnId via CAS(claim_word->txn_id).",
	          "Clears claiming_timestamp after publish to avoid polluting stuck-cleaner detection."
	        ],
	        "changed_headings": [
	          "TxnSlot acquire protocol",
	          "5.6.2 TxnSlot"
	        ]
	      },
	      {
	        "summary": "Fix write_page idempotency for cross-process hints: only increment write_set_pages on first lock acquisition per page; clarify hints are not correctness sources of truth.",
	        "categories": [
	          1,
	          7
	        ],
	        "primary_category": 1,
	        "confidence": 0.8,
	        "evidence": [
	          "Guard write_set_pages.fetch_add(1) behind `newly_locked` to prevent inflated counts on repeated writes to the same page.",
	          "States explicitly that lock tables, not write_set_pages, are the correctness source of truth."
	        ],
	        "changed_headings": [
	          "write_page()"
	        ]
	      },
	      {
	        "summary": "SHM layout hardening: define layout_checksum over immutable metadata only; forbid unsafe reinterpret casts in this repo; and state DDL publication ordering relies on snapshot_seq seqlock windows.",
	        "categories": [
	          4,
	          7,
	          6
	        ],
	        "primary_category": 4,
	        "confidence": 0.78,
	        "evidence": [
	          "Renames header checksum -> layout_checksum and documents immutable-only coverage (magic/version/page_size/offsets), excluding dynamic atomics.",
	          "Adds normative safe-Rust constraint: SHM access uses safe offset-based accessors; no &[u8] -> &SharedMemoryLayout casts.",
	          "Clarifies mixed snapshots are prevented by snapshot_seq + load_consistent_snapshot, not store ordering alone."
	        ],
	        "changed_headings": [
	          "5.6.1 Shared-Memory Coordination Region",
	          "Alignment requirement"
	        ]
	      }
	    ]
	  },
	  {
	    "commit": "7313951174e317f889851f696de264b546e4b54e",
	    "change_groups": [
	      {
	        "summary": "Fix coordinator IPC framing math and add canonical V1 wire payload schemas (including strict size caps and exact SCM_RIGHTS rules).",
	        "categories": [
	          1,
	          4,
	          7,
	          6
	        ],
	        "primary_category": 4,
	        "confidence": 0.8,
	        "evidence": [
	          "Frame.payload corrected from len_be-16 to len_be-12 (excluding the 4-byte length field itself).",
	          "Defines Reserve/SubmitNativePublish/SubmitWalCommit/RowIdReserve payload schemas + size caps; SUBMIT_WAL_COMMIT requires exactly one spill fd in SCM_RIGHTS.",
	          "CommitRequest identity updated to TxnToken for cross-process stability."
	        ],
	        "changed_headings": [
	          "Coordinator IPC Transport",
	          "Wire payload schemas"
	        ]
	      },
	      {
	        "summary": "Define coordinator-owned per-table RowId allocator state and ROWID_RESERVE semantics (schema_epoch validation, monotone non-reusable ranges, gaps permitted).",
	        "categories": [
	          4,
	          7,
	          6
	        ],
	        "primary_category": 4,
	        "confidence": 0.78,
	        "evidence": [
	          "Allocator state lives in coordinator memory keyed by (schema_epoch, TableId), not in SQLite file format or a SHM hash table.",
	          "Initialize next_rowid from max_committed_rowid+1 (AUTOINCREMENT override); reject schema_epoch mismatch with SQLITE_SCHEMA.",
	          "Coordinator advances by count even if caller later aborts (gaps permitted)."
	        ],
	        "changed_headings": [
	          "ROWID_RESERVE",
	          "RowId allocator"
	        ]
	      }
	    ]
	  },
	  {
	    "commit": "d329df055a1a13459d0f507bc222b05525bb7522",
	    "change_groups": [
	      {
	        "summary": "Define serialized writer exclusion acquire/release pseudocode and ordering: acquire global exclusion, publish token/pid/lease, drain concurrent writers, and clear indicator before releasing the global exclusion.",
	        "categories": [
	          4,
	          7
	        ],
	        "primary_category": 4,
	        "confidence": 0.78,
	        "evidence": [
	          "Adds acquire_serialized_writer_exclusion/release_serialized_writer_exclusion with publication edge on serialized_writer_token (Release).",
	          "Drain concurrent writers via lock-table scan and orphan cleanup; clear token before releasing the global exclusion."
	        ],
	        "changed_headings": [
	          "Serialized writer acquisition ordering",
	          "Indicator check algorithm"
	        ]
	      },
	      {
	        "summary": "Make write_set_summary cross-process canonical (sorted u32_le array, no roaring dependency) and add response payload schemas for SUBMIT_NATIVE_PUBLISH and SUBMIT_WAL_COMMIT.",
	        "categories": [
	          4,
	          7,
	          1,
	          6
	        ],
	        "primary_category": 4,
	        "confidence": 0.74,
	        "evidence": [
	          "write_set_summary_len must be a multiple of 4; pages are sorted ascending with no duplicates.",
	          "Adds NativePublishRespV1 and WalCommitRespV1 Ok/Conflict/Err variants with explicit fields."
	        ],
	        "changed_headings": [
	          "Wire payload schemas",
	          "write_set_summary encoding"
	        ]
	      },
	      {
	        "summary": "Remove duplicate expression precedence table and centralize parsing rules: NOT precedence, ESCAPE not an operator, and unary-vs-COLLATE binding.",
	        "categories": [
	          2,
	          5,
	          9
	        ],
	        "primary_category": 2,
	        "confidence": 0.7,
	        "evidence": [
	          "12.15 now points to the normative Pratt precedence table in §10.2 instead of restating the full table.",
	          "Key rules reiterated: NOT x=y parses as NOT(x=y); ESCAPE is parsed as part of LIKE; unary binds tighter than COLLATE."
	        ],
	        "changed_headings": [
	          "12.15 Expression Syntax",
	          "Pratt precedence table"
	        ]
	      }
	    ]
	  },
	  {
	    "commit": "351c282e9a9a2ee118496651cef7a4b33cf309f2",
	    "change_groups": [
	      {
	        "summary": "Apply the no-unsafe constraint to aligned allocation: require safe page-aligned buffers and make PageBuf explicitly page-sized + page-aligned in both the I/O model and formal type definitions.",
	        "categories": [
	          7,
	          4,
	          6
	        ],
	        "primary_category": 7,
	        "confidence": 0.75,
	        "evidence": [
	          "Page alignment section now requires safe abstractions for aligned allocation (dependencies may use unsafe internally).",
	          "PageBuf/PageData and buffer model updated to state page-aligned explicitly."
	        ],
	        "changed_headings": [
	          "1.5 Mechanical Sympathy",
	          "4.10 PageBufferPool",
	          "Formal type definitions"
	        ]
	      },
	      {
	        "summary": "Fix TxnSlot acquire pseudocode: detect lost claim before seeding claiming_timestamp; clean up indentation and clarify omitted fields for brevity.",
	        "categories": [
	          1,
	          4,
	          5
	        ],
	        "primary_category": 1,
	        "confidence": 0.72,
	        "evidence": [
	          "After CAS(0->claim_word), re-load txn_id and require it still equals claim_word before writing claiming_timestamp.",
	          "Normalizes indentation and clarifies begin() pseudocode omits fields that default to empty/false."
	        ],
	        "changed_headings": [
	          "TxnSlot acquire protocol",
	          "acquire_and_publish_txn_slot()"
	        ]
	      }
	    ]
	  },
	  {
	    "commit": "b1c1e72d9331952247c804af6589fe3b349d0b8a",
	    "change_groups": [
	      {
	        "summary": "Tighten coordinator IPC framing and capability discipline: enforce len/version/kind validity, bind permit_id to a connection as a single-use capability, and make responses canonically tagged.",
	        "categories": [
	          4,
	          7,
	          1
	        ],
	        "primary_category": 4,
	        "confidence": 0.78,
	        "evidence": [
	          "Add len_be bounds (>=12 and <=4MiB), require version_be==1, and enumerate kind_be values (unknown kinds rejected).",
	          "Permit binding: SUBMIT_* must reference a permit_id previously returned by RESERVE on the same connection; consumed permits cannot be reused.",
	          "Response payloads become explicit tag+padding+body wrappers (ReserveRespV1, NativePublishRespV1, WalCommitRespV1, RowIdReserveRespV1)."
	        ],
	        "changed_headings": [
	          "5.9.0 Coordinator IPC Transport",
	          "Wire payload schemas"
	        ]
	      },
	      {
	        "summary": "Fix BEGIN TxnId allocation pseudocode to use SharedMemoryLayout.next_txn_id (cross-process global), not a per-manager field.",
	        "categories": [
	          1,
	          5
	        ],
	        "primary_category": 1,
	        "confidence": 0.9,
	        "evidence": [
	          "begin() pseudocode now reads manager.shm.next_txn_id and CASes it for TxnId allocation."
	        ],
	        "changed_headings": [
	          "BEGIN / TxnId allocation"
	        ]
	      }
	    ]
	  },
	  {
	    "commit": "e60049751d7e238e46e45ce8009d9e9f053a41c2",
	    "change_groups": [
	      {
	        "summary": "Harden TAG_CLAIMING liveness safety: require early pid/pid_birth/lease publication before snapshot capture and forbid reclaiming live claimers; add conservative timeouts when pid identity is still unknown.",
	        "categories": [
	          4,
	          1,
	          7
	        ],
	        "primary_category": 4,
	        "confidence": 0.85,
	        "evidence": [
	          "Phase 2 ordering: publish pid/pid_birth/lease_expiry before any potentially blocking work (including seqlock snapshot capture).",
	          "cleanup_orphaned_slots never reclaims a CLAIMING slot if pid/birth are non-zero and process_alive(pid,birth) is true.",
	          "Split timeouts: short fast-path timeout vs longer no-pid fallback timeout."
	        ],
	        "changed_headings": [
	          "5.6.2 TxnSlot",
	          "cleanup_orphaned_slots()"
	        ]
	      },
	      {
	        "summary": "Fix check_serialized_writer_exclusion stale-token cleanup: retry on CAS failure so we never return Ok while a new serialized writer installs a fresh token.",
	        "categories": [
	          1,
	          4,
	          7
	        ],
	        "primary_category": 1,
	        "confidence": 0.8,
	        "evidence": [
	          "Stale indicator clearing loops: if CAS(tok->0) fails, retry because token changed (cleared or replaced).",
	          "CAS uses AcqRel/Acquire to ensure correct publication edges."
	        ],
	        "changed_headings": [
	          "Indicator check algorithm"
	        ]
	      },
	      {
	        "summary": "Make IPC payload set-ordering canonical: ObjectId arrays sorted/deduped, conflict page arrays sorted, and spill_pages sorted by pgno with no duplicates.",
	        "categories": [
	          7,
	          4
	        ],
	        "primary_category": 7,
	        "confidence": 0.75,
	        "evidence": [
	          "Any payload field that semantically represents a set must be encoded in sorted order without duplicates.",
	          "Applies to witness/edge/merge arrays, conflict page lists, and spill_pages ordering."
	        ],
	        "changed_headings": [
	          "Framing",
	          "Canonical ordering"
	        ]
	      }
	    ]
	  },
	  {
	    "commit": "6d5d36a1380d3b4b4b6ddba71828825ef942975b",
	    "change_groups": [
	      {
	        "summary": "Update Round 16 audit note wording to explicitly include TAG_CLAIMING liveness hardening as part of the round's scope.",
	        "categories": [
	          5,
	          9
	        ],
	        "primary_category": 5,
	        "confidence": 0.9,
	        "evidence": [
	          "Footer audit note now mentions early pid/birth publication and forbidding reclaim of live claimers."
	        ],
	        "changed_headings": [
	          "Document version footer"
	        ]
	      }
	    ]
	  }
		];

      const CSS_CACHE = new Map();
      function getCss(varName) {
        if (CSS_CACHE.has(varName)) return CSS_CACHE.get(varName);
        const v = getComputedStyle(document.documentElement).getPropertyValue(varName).trim();
        CSS_CACHE.set(varName, v);
        return v;
      }

      // -----------------------------
      // Buckets
      // -----------------------------

      const BUCKETS = [
        {
          id: 1,
          name: "Logic/Math Fixes",
          desc: "Fixing outright mistakes in logic, math, or reasoning.",
          color: getCss("--c1"),
        },
        {
          id: 2,
          name: "SQLite Legacy Corrections",
          desc: "Fixing inaccurate statements about the C SQLite codebase or semantics.",
          color: getCss("--c2"),
        },
        {
          id: 3,
          name: "asupersync Corrections",
          desc: "Fixing inaccurate statements about asupersync APIs or behavior.",
          color: getCss("--c3"),
        },
        {
          id: 4,
          name: "Architecture Fixes",
          desc: "Fixing conceptual errors or architectural mistakes.",
          color: getCss("--c4"),
        },
        {
          id: 5,
          name: "Scrivening",
          desc: "Ministerial fixes: numbering, references, footers, wording cleanup.",
          color: getCss("--c5"),
        },
        {
          id: 6,
          name: "Added Context",
          desc: "Added background information to make the spec more self-contained.",
          color: getCss("--c6"),
        },
        {
          id: 7,
          name: "Standard Engineering",
          desc: "Improvements based on standard computer engineering: cache, concurrency, I/O, durability mechanics.",
          color: getCss("--c7"),
        },
        {
          id: 8,
          name: "Alien Artifact Math",
          desc: "Esoteric math/rigor additions: e-processes, conformal, BOCPD, VOI, proofs, bounds.",
          color: getCss("--c8"),
        },
        {
          id: 9,
          name: "Clarification",
          desc: "Clarification/elaboration without substantive improvements or fixes.",
          color: getCss("--c9"),
        },
        {
          id: 10,
          name: "Other",
          desc: "Catch-all category.",
          color: getCss("--c10"),
        },
      ];

      // Story Mode: Milestone Schema + Curated Data (bd-24q.4.1)
      const MILESTONES = [
        { id: "genesis", title: "Genesis: 8,628 Lines", commitHash: "c08f160", annotationMd: "The spec is born. A single massive commit lays down the entire architectural blueprint: MVCC page-level versioning, RaptorQ erasure coding, asupersync integration, and the 23-crate workspace structure.", defaultTab: "spec" },
        { id: "ssi-promotion", title: "SSI Promoted to V1", commitHash: "f9d88aa", annotationMd: "Serializable Snapshot Isolation promoted from future goal to V1 requirement. Intent logs, sharded lock tables, and the conservative Page-SSI rule introduced.", focusHeading: "serializable-snapshot-isolation-ssi", defaultTab: "diff" },
        { id: "scope-doctrine", title: "V1.3: Scope Doctrine", commitHash: "9800b17", annotationMd: "\"There is no V1 scope\" \u2014 everything ships. ECS substrate, multi-process MVCC, encryption, WindowsVfs all mandatory.", defaultTab: "diff" },
        { id: "codex-synthesis", title: "V1.4: Codex Synthesis", commitHash: "5ad3487", annotationMd: "Codex (GPT-5.3) spec merged. RaptorQ pervasive: WAL sidecar, ECS layout, replication. Spec crosses 9,000 lines.", defaultTab: "diff" },
        { id: "alien-artifact", title: "V1.5: Alien-Artifact Discipline", commitHash: "7b2c677", annotationMd: "Decision-theoretic rigor: BOCPD, e-process monitoring, conformal calibration. SSI abort gets formal loss matrix. Spec crosses 10,000 lines.", focusHeading: "decision-theoretic-ssi-abort-policy", defaultTab: "diff" },
        { id: "witness-plane", title: "V1.6a: RaptorQ Witness Plane", commitHash: "bf04264", annotationMd: "SSI gains RaptorQ-native witness plane for cross-process proof-carrying commits.", defaultTab: "diff" },
        { id: "perf-optimizations", title: "V1.6c: Performance Hardening", commitHash: "e8ddf46", annotationMd: "Arena allocators, CAR/CLOCK-Pro cache, per-invariant e-process calibration. Cache-line alignment scrutiny.", defaultTab: "diff" },
        { id: "canonical-ssi", title: "V1.6g: Canonical SSI Algorithm", commitHash: "643c89c", annotationMd: "SSI detection algorithm reaches final form. Canonical proof-carrying commit with eager abort.", defaultTab: "diff" },
        { id: "deep-audit-mvcc", title: "V1.7: MVCC Deep Audit", commitHash: "d7b38ef", annotationMd: "Word-by-word audit of Section 5 (~3,000 lines). Finds SSI incoming-edge false negative. RecentlyCommittedReadersIndex added.", focusHeading: "mvcc-formal-model", defaultTab: "diff" },
        { id: "deep-audit-raptorq", title: "V1.7e: RaptorQ Deep Audit", commitHash: "3cf0f13", annotationMd: "Section 3 audit: MTU/sub-blocking fixes, ESI/ISI confusion, OTI.T u16\u2192u32. GF(256) verified against RFC 6330.", defaultTab: "diff" },
        { id: "deep-audit-query", title: "V1.7h: Query Pipeline Audit", commitHash: "2f0970b", annotationMd: "Section 10: lexer DQS, cost model, UPDATE trace. Ne/BangEq fixed. ESCAPE removed from Pratt table.", defaultTab: "diff" },
        { id: "deep-audit-fts5", title: "V1.7j: FTS5 + SQL Coverage", commitHash: "a3e7ae5", annotationMd: "Section 14: FTS5 NOT binary-only. Full section-by-section audit coverage reached.", defaultTab: "diff" },
      ];
      /** Resolve milestones against loaded commit data. Call after ALL_COMMITS is populated. */
      function getMilestones() {
        if (!ALL_COMMITS || !ALL_COMMITS.length) return [];
        const hashToIdx = new Map();
        for (let i = 0; i < ALL_COMMITS.length; i++) {
          const c = ALL_COMMITS[i];
          hashToIdx.set(c.hash, i);
          if (c.short) hashToIdx.set(c.short, i);
        }
        return MILESTONES.map((m) => {
          const idx = hashToIdx.get(m.commitHash) ?? null;
          if (idx === null) console.warn(`Milestone "${m.id}": commit ${m.commitHash} not found`);
          return { ...m, commitIdx: idx, warning: idx === null ? `commit ${m.commitHash} not found` : null };
        });
      }

      // -----------------------------
      // App state
      // -----------------------------

      const STATE = {
        q: "",
        minImpact: 0,
        bucketMode: "primary", // 'primary' | 'multi'
        bucketEnabled: new Set(BUCKETS.map((b) => b.id)),
      };

      // -----------------------------
      // Spec evolution dataset (local)
      // -----------------------------

      const SPEC_EVOLUTION_DATA_URL = "spec_evolution_data_v1.json.gz";

      const DATASET = {
        data: null,
        loaded: false,
        error: null,
      };

      const DOC = {
        idx: 0,
        tab: "spec", // 'spec' | 'diff' | 'metrics'
        rawSpec: false,
        diffMode: "pretty", // 'pretty' | 'raw'
        compareMode: false, // true = A/B compare between two arbitrary commits
        compareFromIdx: 0, // "A" commit index
        compareToIdx: 0, // "B" commit index
        diffLayout: "side-by-side", // 'side-by-side' | 'line-by-line'
        diffCollapse: true, // collapse unchanged context in A/B diffs
      };

      const METRICS = {
        // commit hash -> metric
        tokensChanged: new Map(),
        bytesChanged: new Map(),
        hunks: new Map(),
        lev: new Map(),
      };

      const DOC_CACHE = new Map(); // idx -> { text: string, lines?: string[] }

      const OUTLINE_CACHE = new Map(); // idx -> outline[]

      const LEV_WASM_URL = "levenshtein_bytes.wasm";
      let LEV_WASM = null;

      const WORKER_STATE = {
        worker: null,
        ready: false,
        disabled: false,
        reqSeq: 1,
        pending: new Map(), // reqId -> { resolve, reject, onProgress, timeoutId }
        datasetHash: "",
      };

      const WORKER_DERIVED = {
        searchReady: false,
        clusterReady: false,
        phase: null,
        phaseKey: "",
        outliers: null,
        outlierKey: "",
      };

      let COMPUTE_ABORT_CONTROLLER = null;
      let PHASE_ABORT_CONTROLLER = null;
      let OUTLIER_ABORT_CONTROLLER = null;

      // -----------------------------
      // URL State Schema (Permalinks) — v1
      // -----------------------------
      // Canonical param order: v, c, t, raw, dm, q, mi, bm, b
      // Default values are omitted for minimal URLs.
      // Invalid values are clamped or reset to defaults.

      const URL_SCHEMA_VERSION = 1;

      const URL_DEFAULTS = {
        c: -1,          // commit index; -1 means "latest"
        t: "spec",      // tab: spec | diff | metrics
        raw: false,     // raw spec toggle
        dm: "pretty",   // diff mode: pretty | raw
        q: "",          // search query
        mi: 0,          // minimum impact filter
        bm: "primary",  // bucket mode: primary | multi
        b: null,        // enabled bucket ids; null = all
      };

      const URL_VALID_TABS = new Set(["spec", "diff", "metrics", "sections"]);
      const URL_VALID_DIFF_MODES = new Set(["pretty", "raw"]);
      const URL_VALID_BUCKET_MODES = new Set(["primary", "multi"]);
      const URL_ALL_BUCKET_IDS = new Set(BUCKETS.map((b) => b.id));

      function encodeUrlState() {
        const p = new URLSearchParams();
        const maxIdx = Math.max(0, (ALL_COMMITS?.length || 1) - 1);

        // Always include version when any non-default param is present.
        // We build params first, then prepend v= if non-empty.

        if (DOC.idx !== maxIdx && DOC.idx >= 0) p.set("c", String(DOC.idx));
        if (DOC.tab !== URL_DEFAULTS.t && URL_VALID_TABS.has(DOC.tab)) p.set("t", DOC.tab);
        if (DOC.rawSpec) p.set("raw", "1");
        if (DOC.diffMode !== URL_DEFAULTS.dm && URL_VALID_DIFF_MODES.has(DOC.diffMode)) p.set("dm", DOC.diffMode);
        if (STATE.q) p.set("q", STATE.q);
        if (STATE.minImpact > 0) p.set("mi", String(STATE.minImpact));
        if (STATE.bucketMode !== URL_DEFAULTS.bm) p.set("bm", STATE.bucketMode);

        // Encode enabled buckets only if not all are enabled.
        const allEnabled = URL_ALL_BUCKET_IDS.size === STATE.bucketEnabled.size &&
          [...URL_ALL_BUCKET_IDS].every((id) => STATE.bucketEnabled.has(id));
        if (!allEnabled) {
          const sorted = [...STATE.bucketEnabled].sort((a, b) => a - b);
          p.set("b", sorted.join(","));
        }

        // Canonical ordering: rebuild with v first, then alphabetical key order.
        if ([...p.keys()].length === 0) return "";
        const canon = new URLSearchParams();
        canon.set("v", String(URL_SCHEMA_VERSION));
        // Canonical key order: v, c, t, raw, dm, cmp, ca, cb, dl, q, mi, bm, b
        if (DOC.compareMode) { p.set("cmp", "1"); p.set("ca", String(DOC.compareFromIdx)); p.set("cb", String(DOC.compareToIdx)); if (DOC.diffLayout !== "side-by-side") p.set("dl", DOC.diffLayout); }
        for (const k of ["c", "t", "raw", "dm", "cmp", "ca", "cb", "dl", "q", "mi", "bm", "b"]) {
          if (p.has(k)) canon.set(k, p.get(k));
        }
        return canon.toString();
      }

      function decodeUrlState(search) {
        const p = new URLSearchParams(search || "");
        if (!p.has("v")) return null; // No URL state present.

        const v = Number(p.get("v"));
        if (v !== URL_SCHEMA_VERSION) return null; // Unknown schema version; ignore.

        const result = {};

        // Commit index.
        if (p.has("c")) {
          const raw = Number(p.get("c"));
          result.c = Number.isFinite(raw) && raw >= 0 ? Math.floor(raw) : URL_DEFAULTS.c;
        } else {
          result.c = URL_DEFAULTS.c;
        }

        // Tab.
        const tab = p.get("t") || URL_DEFAULTS.t;
        result.t = URL_VALID_TABS.has(tab) ? tab : URL_DEFAULTS.t;

        // Raw spec.
        result.raw = p.get("raw") === "1";

        // Diff mode.
        const dm = p.get("dm") || URL_DEFAULTS.dm;
        result.dm = URL_VALID_DIFF_MODES.has(dm) ? dm : URL_DEFAULTS.dm;

        // Search query.
        result.q = p.get("q") || URL_DEFAULTS.q;

        // Min impact.
        if (p.has("mi")) {
          const mi = Number(p.get("mi"));
          result.mi = Number.isFinite(mi) && mi >= 0 ? Math.floor(mi) : URL_DEFAULTS.mi;
        } else {
          result.mi = URL_DEFAULTS.mi;
        }

        // Bucket mode.
        const bm = p.get("bm") || URL_DEFAULTS.bm;
        result.bm = URL_VALID_BUCKET_MODES.has(bm) ? bm : URL_DEFAULTS.bm;

        // Enabled buckets.
        if (p.has("b")) {
          const ids = p.get("b").split(",")
            .map((s) => Number(s.trim()))
            .filter((n) => Number.isFinite(n) && URL_ALL_BUCKET_IDS.has(n));
          result.b = ids.length > 0 ? new Set(ids) : new Set(URL_ALL_BUCKET_IDS);
        } else {
          result.b = null; // null = all enabled
        }

        // A/B Compare mode.
        result.cmp = p.get("cmp") === "1";
        if (result.cmp) {
          const ca = Number(p.get("ca")); result.ca = Number.isFinite(ca) && ca >= 0 ? Math.floor(ca) : 0;
          const cb = Number(p.get("cb")); result.cb = Number.isFinite(cb) && cb >= 0 ? Math.floor(cb) : 0;
          const dl = p.get("dl") || "side-by-side";
          result.dl = (dl === "line-by-line") ? "line-by-line" : "side-by-side";
        }

        return result;
      }

      function applyUrlState(s) {
        if (!s) return;
        const maxIdx = Math.max(0, (ALL_COMMITS?.length || 1) - 1);

        // Commit index: -1 means latest, otherwise clamp.
        DOC.idx = s.c < 0 ? maxIdx : clamp(s.c, 0, maxIdx);
        DOC.tab = s.t;
        DOC.rawSpec = s.raw;
        DOC.diffMode = s.dm;
        STATE.q = s.q;
        STATE.minImpact = s.mi;
        STATE.bucketMode = s.bm;
        STATE.bucketEnabled = s.b ? new Set(s.b) : new Set(URL_ALL_BUCKET_IDS);
        if (s.cmp) { DOC.compareMode = true; DOC.compareFromIdx = clamp(s.ca || 0, 0, maxIdx); DOC.compareToIdx = clamp(s.cb || 0, 0, maxIdx); DOC.diffLayout = s.dl || "side-by-side"; }

        // Sync UI inputs to match restored state.
        const qEl = document.getElementById("q");
        const qMob = document.getElementById("qMobile");
        if (qEl) qEl.value = STATE.q;
        if (qMob) qMob.value = STATE.q;

        const impactEl = document.getElementById("impact");
        const impactMob = document.getElementById("impactMobile");
        if (impactEl) impactEl.value = String(STATE.minImpact);
        if (impactMob) impactMob.value = String(STATE.minImpact);
      }

      let _urlSyncScheduled = false;

      function syncUrlToState(opts = {}) {
        // Debounce: avoid thrashing history on rapid slider moves.
        if (_urlSyncScheduled && !opts.immediate) return;
        _urlSyncScheduled = true;
        requestAnimationFrame(() => {
          _urlSyncScheduled = false;
          const qs = encodeUrlState();
          const newUrl = qs ? `${location.pathname}?${qs}` : location.pathname;
          if (location.search !== (qs ? `?${qs}` : "")) {
            history.replaceState(null, "", newUrl);
          }
        });
      }

      function copyPermalink() {
        const qs = encodeUrlState();
        const url = qs
          ? `${location.origin}${location.pathname}?${qs}`
          : `${location.origin}${location.pathname}`;
        if (navigator.clipboard?.writeText) {
          navigator.clipboard.writeText(url).then(() => {
            showCopyToast("Link copied");
          }, () => {
            fallbackCopy(url);
          });
        } else {
          fallbackCopy(url);
        }
      }

      function fallbackCopy(text) {
        const ta = document.createElement("textarea");
        ta.value = text;
        ta.style.position = "fixed";
        ta.style.opacity = "0";
        document.body.appendChild(ta);
        ta.select();
        try {
          document.execCommand("copy");
          showCopyToast("Link copied");
        } catch {
          showCopyToast("Copy failed");
        }
        document.body.removeChild(ta);
      }

      function showCopyToast(msg) {
        const btn = document.getElementById("btnCopyLink");
        if (!btn) return;
        const orig = btn.textContent;
        btn.textContent = msg;
        btn.classList.add("bg-emerald-600", "text-white");
        btn.classList.remove("bg-white/70", "text-slate-900");
        setTimeout(() => {
          btn.textContent = orig;
          btn.classList.remove("bg-emerald-600", "text-white");
          btn.classList.add("bg-white/70", "text-slate-900");
        }, 1500);
      }

      function toggleShareHelp() {
        const el = document.getElementById("shareHelpPopover");
        if (el) el.classList.toggle("hidden");
      }

      // -----------------------------
      // Helpers
      // -----------------------------

      function clamp(n, lo, hi) {
        return Math.max(lo, Math.min(hi, n));
      }

      function escapeHtml(s) {
        return String(s)
          .replaceAll("&", "&amp;")
          .replaceAll("<", "&lt;")
          .replaceAll(">", "&gt;")
          .replaceAll('"', "&quot;")
          .replaceAll("'", "&#39;");
      }

      function fmtInt(n) {
        return Intl.NumberFormat(undefined).format(n);
      }

      function debounce(fn, ms) {
        let t;
        return function (...args) {
          clearTimeout(t);
          t = setTimeout(() => fn.apply(this, args), ms);
        };
      }

      function parseCommitLog() {
        const d = DATASET.data;
        if (d && Array.isArray(d.commits) && d.commits.length) {
          return d.commits.map((c, idx) => {
            return {
              idx,
              hash: c.hash,
              short: c.short,
              dateIso: c.dateIso,
              author: c.author,
              subject: c.subject,
              url: `https://github.com/Dicklesworthstone/frankensqlite/commit/${c.hash}`,
            };
          });
        }

        const rows = COMMIT_LOG_RAW.trim()
          .split("\n")
          .map((l) => l.trim())
          .filter(Boolean);
        return rows.map((r, idx) => {
          const [hash, short, dateIso, author, subject] = r.split("|").map((x) => x.trim());
          return {
            idx,
            hash,
            short,
            dateIso,
            author,
            subject,
            url: `https://github.com/Dicklesworthstone/frankensqlite/commit/${hash}`,
          };
        });
      }

      function parseCommitStats() {
        const d = DATASET.data;
        if (d && Array.isArray(d.commits) && d.commits.length) {
          const m = new Map();
          for (const c of d.commits) {
            const a = Number(c.add || 0);
            const del = Number(c.del || 0);
            m.set(c.hash, { add: a, del, impact: a + del });
          }
          return m;
        }

        const rows = COMMIT_STATS_RAW.trim()
          .split("\n")
          .map((l) => l.trim())
          .filter(Boolean);
        const m = new Map();
        for (const r of rows) {
          const [hash, add, del] = r.split("|").map((x) => x.trim());
          const a = Number(add || 0);
          const d = Number(del || 0);
          m.set(hash, { add: a, del: d, impact: a + d });
        }
        return m;
      }

      function formatErr(e) {
        const name = e?.name || "Error";
        const message = e?.message || String(e || "Unknown error");
        const stack = e?.stack ? String(e.stack) : "";
        return stack ? `${name}: ${message}\n${stack}` : `${name}: ${message}`;
      }

      function setWorkerStatus(text, tone = "neutral", detail = "") {
        const el = document.getElementById("workerStatus");
        if (!el) return;
        el.textContent = text || "";
        el.title = detail ? String(detail) : "";
        if (tone === "error") {
          el.className = "mt-2 text-xs font-semibold text-rose-700";
        } else if (tone === "ok") {
          el.className = "mt-2 text-xs font-semibold text-emerald-700";
        } else {
          el.className = "mt-2 text-xs text-slate-500";
        }
      }

      function supportsAnalysisWorker() {
        return typeof Worker !== "undefined" && typeof URL !== "undefined" && typeof Blob !== "undefined";
      }

      function toHex(buf) {
        return Array.from(new Uint8Array(buf))
          .map((b) => b.toString(16).padStart(2, "0"))
          .join("");
      }

      async function computeDatasetHash(data) {
        const commitHashes = Array.isArray(data?.commits) ? data.commits.map((c) => String(c.hash || "")) : [];
        const patchSizes = Array.isArray(data?.patches) ? data.patches.map((p) => String((p || "").length)) : [];
        const basis = `${String(data?.base_doc || "").length}|${commitHashes.join(",")}|${patchSizes.join(",")}`;
        const enc = new TextEncoder().encode(basis);
        if (crypto?.subtle?.digest) {
          const dig = await crypto.subtle.digest("SHA-256", enc);
          return toHex(dig);
        }
        // Fallback hash (djb2 xor), deterministic but non-cryptographic.
        let h = 5381;
        for (let i = 0; i < basis.length; i++) {
          h = ((h << 5) + h) ^ basis.charCodeAt(i);
        }
        return `djb2-${(h >>> 0).toString(16)}`;
      }

      function makeAnalysisWorkerSource() {
        return `
          const LEV_WASM_URL = ${JSON.stringify(LEV_WASM_URL)};
          const STATE = {
            dataset: null,
            datasetHash: "",
            patchHunks: [],
            snapshotCache: new Map(),
            snapshotCursorIdx: 0,
            snapshotCursorLines: null,
            levWasm: null,
            searchIndex: null,
            clusterData: null,
          };
          const CANCELLED_REQS = new Set();

          class AbortErr extends Error {
            constructor(message) {
              super(message || "Request cancelled");
              this.name = "AbortError";
            }
          }

          function isCancelled(reqId) {
            return CANCELLED_REQS.has(reqId);
          }

          function throwIfCancelled(reqId) {
            if (isCancelled(reqId)) throw new AbortErr("Request cancelled by main thread");
          }

          function serializeError(err) {
            return {
              name: err?.name || "Error",
              message: err?.message || String(err || "Unknown error"),
              stack: err?.stack ? String(err.stack) : "",
            };
          }

          function countRoughTokens(s) {
            let n = 0;
            const re = /[A-Za-z0-9_]+|[^\\\\s]/g;
            while (re.exec(String(s))) n++;
            return n;
          }

          function parseUnifiedHunks(patch) {
            const lines = String(patch || "").split("\\n");
            const hunks = [];
            for (let i = 0; i < lines.length; i++) {
              const line = lines[i];
              if (!line.startsWith("@@")) continue;
              const m = /^@@ -(\\\\d+)(?:,(\\\\d+))? \\\\+(\\\\d+)(?:,(\\\\d+))? @@/.exec(line);
              if (!m) continue;
              const oldStart = Number(m[1]);
              const oldCount = Number(m[2] || "1");
              const newStart = Number(m[3]);
              const newCount = Number(m[4] || "1");
              const hunkLines = [];
              i++;
              for (; i < lines.length; i++) {
                const l = lines[i];
                if (l.startsWith("@@")) {
                  i--;
                  break;
                }
                if (l.startsWith("diff --git")) break;
                if (l.startsWith("index ") || l.startsWith("---") || l.startsWith("+++")) continue;
                hunkLines.push(l);
              }
              hunks.push({ oldStart, oldCount, newStart, newCount, lines: hunkLines });
            }
            return hunks;
          }

          function quickMetricsFromPatch(patch) {
            const lines = String(patch || "").split("\\n");
            let hunks = 0;
            let addLines = 0;
            let delLines = 0;
            let tokAdd = 0;
            let tokDel = 0;
            let bytesAdd = 0;
            let bytesDel = 0;
            for (const l of lines) {
              if (l.startsWith("@@")) hunks += 1;
              if (l.startsWith("+")) {
                if (l.startsWith("+++")) continue;
                addLines += 1;
                const s = l.slice(1);
                tokAdd += countRoughTokens(s);
                bytesAdd += s.length + 1;
              } else if (l.startsWith("-")) {
                if (l.startsWith("---")) continue;
                delLines += 1;
                const s = l.slice(1);
                tokDel += countRoughTokens(s);
                bytesDel += s.length + 1;
              }
            }
            return {
              hunks,
              addLines,
              delLines,
              tokensChanged: tokAdd + tokDel,
              bytesChanged: bytesAdd + bytesDel,
              tokensDelta: tokAdd - tokDel,
              bytesDelta: bytesAdd - bytesDel,
            };
          }

          function clamp(n, lo, hi) {
            return Math.max(lo, Math.min(hi, n));
          }

          function applyPatchLines(prevLines, patch) {
            const hunks = parseUnifiedHunks(patch);
            let out = prevLines.slice();
            let offset = 0;
            for (const h of hunks) {
              let at = (h.oldStart - 1) + offset;
              at = clamp(at, 0, out.length);
              let cursor = at;
              const next = [];
              for (const hl of h.lines) {
                if (!hl) continue;
                const p = hl[0];
                const content = hl.slice(1);
                if (p === " ") {
                  next.push(content);
                  cursor += 1;
                } else if (p === "-") {
                  cursor += 1;
                } else if (p === "+") {
                  next.push(content);
                }
              }
              out.splice(at, cursor - at, ...next);
              offset += next.length - (cursor - at);
            }
            return out;
          }

          function patchForIdx(idx) {
            const d = STATE.dataset;
            if (!d || !Array.isArray(d.patches)) return "";
            return d.patches[idx] || "";
          }

          function docTextAtLocal(idx, reqId, progressCb) {
            const d = STATE.dataset;
            if (!d) return "";
            if (idx <= 0) return String(d.base_doc || "");
            const cached = STATE.snapshotCache.get(idx);
            if (typeof cached === "string") return cached;

            if (STATE.snapshotCursorLines && idx === STATE.snapshotCursorIdx + 1) {
              throwIfCancelled(reqId);
              const nextLines = applyPatchLines(STATE.snapshotCursorLines, patchForIdx(idx));
              STATE.snapshotCursorIdx = idx;
              STATE.snapshotCursorLines = nextLines;
              const text = nextLines.join("\\n");
              STATE.snapshotCache.set(idx, text);
              return text;
            }

            let anchor = 0;
            for (let j = idx - 1; j > 0; j--) {
              if (STATE.snapshotCache.has(j)) {
                anchor = j;
                break;
              }
            }

            let lines = String(d.base_doc || "").split("\\n");
            if (anchor > 0) lines = String(STATE.snapshotCache.get(anchor) || "").split("\\n");

            for (let k = Math.max(1, anchor + 1); k <= idx; k++) {
              throwIfCancelled(reqId);
              lines = applyPatchLines(lines, patchForIdx(k));
              if (k === idx || k % 10 === 0) {
                STATE.snapshotCache.set(k, lines.join("\\n"));
              }
              if (progressCb && (k % 8 === 0 || k === idx)) {
                progressCb({ stage: "snapshot", done: k, total: idx, message: "Reconstructing snapshot" });
              }
            }

            STATE.snapshotCursorIdx = idx;
            STATE.snapshotCursorLines = lines;
            const out = STATE.snapshotCache.get(idx) || lines.join("\\n");
            STATE.snapshotCache.set(idx, out);
            return out;
          }

          async function initLevWasm() {
            if (STATE.levWasm) return STATE.levWasm;
            const res = await fetch(LEV_WASM_URL, { cache: "force-cache" });
            if (!res.ok) throw new Error("Failed to fetch " + LEV_WASM_URL + ": HTTP " + res.status);
            const buf = await res.arrayBuffer();
            const { instance } = await WebAssembly.instantiate(buf, {});
            STATE.levWasm = instance.exports;
            return STATE.levWasm;
          }

          async function levenshteinBytes(aBytes, bBytes) {
            const ex = await initLevWasm();
            const memory = ex.memory;
            const alloc = ex.alloc;
            const dealloc = ex.dealloc;
            const levenshtein = ex.levenshtein;
            if (!memory || !alloc || !dealloc || !levenshtein) {
              throw new Error("WASM exports missing (memory/alloc/dealloc/levenshtein)");
            }
            const a = aBytes || new Uint8Array();
            const b = bBytes || new Uint8Array();
            const ptrA = alloc(a.length);
            const viewA = new Uint8Array(memory.buffer, ptrA, a.length);
            viewA.set(a);
            const ptrB = alloc(b.length);
            const viewB = new Uint8Array(memory.buffer, ptrB, b.length);
            viewB.set(b);
            const d = levenshtein(ptrA, a.length, ptrB, b.length) >>> 0;
            dealloc(ptrA, a.length);
            dealloc(ptrB, b.length);
            return d;
          }

          async function levenshteinForPatch(patch, reqId) {
            const hunks = parseUnifiedHunks(patch);
            const enc = new TextEncoder();
            let sum = 0;
            for (const h of hunks) {
              throwIfCancelled(reqId);
              const oldLines = [];
              const newLines = [];
              for (const hl of h.lines) {
                if (!hl) continue;
                const p = hl[0];
                const content = hl.slice(1);
                if (p === "-") oldLines.push(content);
                if (p === "+") newLines.push(content);
              }
              if (!oldLines.length && !newLines.length) continue;
              const a = enc.encode(oldLines.join("\\n"));
              const b = enc.encode(newLines.join("\\n"));
              if (a.length > 20000 || b.length > 20000) {
                sum += a.length + b.length;
              } else {
                sum += await levenshteinBytes(a, b);
              }
            }
            return sum;
          }

          function slugifyHeadingW(text) {
            return String(text || "")
              .toLowerCase()
              .replace(/[^a-z0-9\\u00C0-\\u024F]+/g, "-")
              .replace(/^-+|-+$/g, "")
              || "heading";
          }

          function extractOutlineWorker(markdownText) {
            const lines = String(markdownText || "").split("\\n");
            const outline = [];
            const slugCounts = new Map();
            const atxRe = /^(#{1,6})\\s+(.+?)\\s*$/;
            let inFence = false;
            for (let li = 0; li < lines.length; li++) {
              const line = lines[li];
              if (line.startsWith("\`\`\`")) {
                inFence = !inFence;
                continue;
              }
              if (inFence) continue;
              const m = atxRe.exec(line);
              if (!m) continue;
              const level = m[1].length;
              const text = m[2].replace(/\\s+#+\\s*$/, "");
              const baseSlug = slugifyHeadingW(text);
              const count = slugCounts.get(baseSlug) || 0;
              slugCounts.set(baseSlug, count + 1);
              const id = count === 0 ? baseSlug : baseSlug + "-" + count;
              outline.push({ text: text.trim(), level, id, line: li + 1 });
            }
            return outline;
          }

          function buildLineToHeadingMapW(totalLines, outline) {
            const map = new Array(totalLines + 1);
            map[0] = "__preamble__";
            let ptr = 0;
            let currentId = "__preamble__";
            for (let ln = 1; ln <= totalLines; ln++) {
              while (ptr < outline.length && outline[ptr].line != null && outline[ptr].line <= ln) {
                currentId = outline[ptr].id;
                ptr++;
              }
              map[ln] = currentId;
            }
            return map;
          }

          function attributeHunksToHeadingsW(patch, lineToHeading) {
            const hunks = parseUnifiedHunks(patch);
            const metrics = {};
            for (const hunk of hunks) {
              let newLineNum = hunk.newStart;
              for (const hl of hunk.lines) {
                if (!hl) continue;
                const p = hl[0];
                const content = hl.slice(1);
                const hid = lineToHeading[newLineNum] || "__preamble__";
                if (!metrics[hid]) metrics[hid] = { addLines: 0, delLines: 0, tokensAdded: 0, tokensDeleted: 0 };
                if (p === "+") {
                  metrics[hid].addLines++;
                  metrics[hid].tokensAdded += countRoughTokens(content);
                  newLineNum++;
                } else if (p === "-") {
                  metrics[hid].delLines++;
                  metrics[hid].tokensDeleted += countRoughTokens(content);
                } else if (p === " ") {
                  newLineNum++;
                }
              }
            }
            return metrics;
          }

          function tokenize(text) {
            return String(text || "")
              .toLowerCase()
              .split(/[^a-z0-9_]+/g)
              .filter((x) => x.length >= 2);
          }

          function buildSearchIndex() {
            const commits = STATE.dataset?.commits || [];
            const postings = new Map();
            const docs = [];
            for (let i = 0; i < commits.length; i++) {
              const c = commits[i];
              const body = [c.hash || "", c.short || "", c.author || "", c.subject || ""].join(" ");
              const tokens = tokenize(body);
              docs.push({ i, hash: c.hash, short: c.short, subject: c.subject, author: c.author, body });
              const seen = new Set();
              for (const tok of tokens) {
                if (seen.has(tok)) continue;
                seen.add(tok);
                let arr = postings.get(tok);
                if (!arr) {
                  arr = [];
                  postings.set(tok, arr);
                }
                arr.push(i);
              }
            }
            STATE.searchIndex = { postings, docs };
            return { docs: docs.length, terms: postings.size };
          }

          function querySearch(q, limit) {
            const index = STATE.searchIndex;
            if (!index) return { query: q, hits: [] };
            const toks = tokenize(q);
            if (!toks.length) return { query: q, hits: [] };
            let candidate = null;
            for (const t of toks) {
              const posting = index.postings.get(t) || [];
              if (candidate === null) {
                candidate = new Set(posting);
              } else {
                const next = new Set();
                for (const docId of candidate) {
                  if (posting.includes(docId)) next.add(docId);
                }
                candidate = next;
              }
            }
            const out = [];
            for (const docId of candidate || []) {
              const d = index.docs[docId];
              if (!d) continue;
              let score = 0;
              for (const t of toks) {
                if (d.body.toLowerCase().includes(t)) score += 1;
              }
              out.push({ idx: d.i, hash: d.hash, short: d.short, subject: d.subject, author: d.author, score });
            }
            out.sort((a, b) => b.score - a.score || a.idx - b.idx);
            return { query: q, hits: out.slice(0, Math.max(1, Number(limit || 20))) };
          }

          function hash32(seed, str) {
            let h = seed >>> 0;
            for (let i = 0; i < str.length; i++) {
              h ^= str.charCodeAt(i);
              h = Math.imul(h, 16777619);
            }
            return h >>> 0;
          }

          function computeClusters(limit) {
            const commits = STATE.dataset?.commits || [];
            const seeds = [2166136261, 19088743, 591798841, 2654435761, 40503, 734539, 1889, 2651];
            const docs = commits.map((c, idx) => {
              const tokens = Array.from(new Set(tokenize((c.subject || "") + " " + (c.author || ""))));
              const sig = [];
              for (const seed of seeds) {
                let best = 0xffffffff;
                for (const tok of tokens) {
                  const h = hash32(seed, tok);
                  if (h < best) best = h;
                }
                sig.push(best >>> 0);
              }
              return { idx, hash: c.hash, short: c.short, subject: c.subject, sig };
            });

            const bands = new Map();
            for (const d of docs) {
              for (let b = 0; b < 4; b++) {
                const key = d.sig.slice(b * 2, b * 2 + 2).join("-");
                const bandKey = b + ":" + key;
                let arr = bands.get(bandKey);
                if (!arr) {
                  arr = [];
                  bands.set(bandKey, arr);
                }
                arr.push(d.idx);
              }
            }

            const clusterSets = [];
            const seenKeys = new Set();
            for (const ids of bands.values()) {
              if (ids.length < 2) continue;
              const uniq = Array.from(new Set(ids)).sort((a, b) => a - b);
              const key = uniq.join(",");
              if (seenKeys.has(key)) continue;
              seenKeys.add(key);
              clusterSets.push(uniq);
            }

            clusterSets.sort((a, b) => b.length - a.length);
            const max = Math.max(1, Number(limit || 12));
            const clusters = clusterSets.slice(0, max).map((ids, i) => ({
              id: i + 1,
              size: ids.length,
              items: ids.map((idx) => {
                const c = commits[idx] || {};
                return { idx, hash: c.hash, short: c.short, subject: c.subject };
              }),
            }));
            STATE.clusterData = clusters;
            return { clusters };
          }

          function median(values) {
            const arr = values.slice().sort((a, b) => a - b);
            if (!arr.length) return 0;
            const mid = Math.floor(arr.length / 2);
            if (arr.length % 2) return arr[mid];
            return (arr[mid - 1] + arr[mid]) / 2;
          }

          function computeOutliers(values, topK) {
            const xs = values.map((v) => Number(v || 0));
            const med = median(xs);
            const absDev = xs.map((x) => Math.abs(x - med));
            const mad = median(absDev) || 1e-9;
            const scored = xs.map((x, idx) => {
              const z = 0.6745 * (x - med) / mad;
              return { idx, value: x, z };
            });
            scored.sort((a, b) => Math.abs(b.z) - Math.abs(a.z));
            return { median: med, mad, top: scored.slice(0, Math.max(1, Number(topK || 10))) };
          }

          function logAddExp(a, b) {
            if (a === -Infinity) return b;
            if (b === -Infinity) return a;
            const m = Math.max(a, b);
            return m + Math.log(Math.exp(a - m) + Math.exp(b - m));
          }

          function updateWelford(st, x) {
            const n1 = st.n + 1;
            const delta = x - st.mean;
            const mean1 = st.mean + delta / n1;
            const delta2 = x - mean1;
            const m21 = st.m2 + delta * delta2;
            return { n: n1, mean: mean1, m2: m21 };
          }

          function logGamma(z) {
            const p = [
              0.99999999999980993, 676.5203681218851, -1259.1392167224028, 771.32342877765313,
              -176.61502916214059, 12.507343278686905, -0.13857109526572012, 9.9843695780195716e-6,
              1.5056327351493116e-7,
            ];
            if (z < 0.5) {
              return Math.log(Math.PI) - Math.log(Math.sin(Math.PI * z)) - logGamma(1 - z);
            }
            z -= 1;
            let x = p[0];
            for (let i = 1; i < p.length; i++) x += p[i] / (z + i);
            const t = z + 7.5;
            return 0.5 * Math.log(2 * Math.PI) + (z + 0.5) * Math.log(t) - t + Math.log(x);
          }

          function studentTLogPdf(x, mu, sigma, nu) {
            const z = (x - mu) / (sigma || 1e-9);
            return (
              logGamma((nu + 1) / 2) -
              logGamma(nu / 2) -
              Math.log((sigma || 1e-9) * Math.sqrt(nu * Math.PI)) -
              ((nu + 1) / 2) * Math.log(1 + (z * z) / nu)
            );
          }

          function computePhaseMap(values, hazard) {
            const y = values.map((v) => Number(v || 0));
            const H = Math.max(1e-4, Math.min(0.9999, Number(hazard || 0.03)));
            let mu0 = 0.0;
            let kappa0 = 0.01;
            let alpha0 = 0.5;
            let beta0 = 0.5;
            let logR = [0.0];
            let stats = [{ n: 0, mean: 0.0, m2: 0.0 }];
            const p0 = [];
            const changePoints = [];
            const logHaz = Math.log(H);
            const log1mHaz = Math.log(1.0 - H);
            for (let t = 0; t < y.length; t++) {
              const x = y[t];
              const logPred = [];
              for (let r = 0; r < stats.length; r++) {
                const st = stats[r];
                const n = st.n;
                const mean = st.mean;
                const kappa = kappa0 + n;
                const alpha = alpha0 + n / 2;
                const beta = beta0 + 0.5 * st.m2 + (kappa0 * n * (mean - mu0) * (mean - mu0)) / (2 * (kappa0 + n));
                const dof = 2 * alpha;
                const scale2 = (beta * (kappa + 1)) / (alpha * kappa);
                logPred.push(studentTLogPdf(x, mean, Math.sqrt(scale2), dof));
              }
              const newLogR = new Array(stats.length + 1).fill(-Infinity);
              let logSumCp = -Infinity;
              for (let r = 0; r < logR.length; r++) {
                logSumCp = logAddExp(logSumCp, logR[r] + logPred[r] + logHaz);
              }
              newLogR[0] = logSumCp;
              for (let r = 0; r < logR.length; r++) {
                newLogR[r + 1] = logR[r] + logPred[r] + log1mHaz;
              }
              const logZ = newLogR.reduce((a, b) => logAddExp(a, b), -Infinity);
              for (let i = 0; i < newLogR.length; i++) newLogR[i] -= logZ;
              const p_r0 = Math.exp(newLogR[0]);
              p0.push(p_r0);
              if (p_r0 > 0.5) changePoints.push(t);
              const newStats = new Array(stats.length + 1);
              newStats[0] = { n: 1, mean: x, m2: 0.0 };
              for (let r = 1; r < newStats.length; r++) {
                newStats[r] = updateWelford(stats[r - 1], x);
              }
              const K = 120;
              const idxs = newLogR
                .map((v, i) => [v, i])
                .sort((a, b) => b[0] - a[0])
                .slice(0, K)
                .map((x) => x[1])
                .sort((a, b) => a - b);
              logR = idxs.map((i) => newLogR[i]);
              stats = idxs.map((i) => newStats[i]);
              const logZ2 = logR.reduce((a, b) => logAddExp(a, b), -Infinity);
              logR = logR.map((v) => v - logZ2);
            }
            return { p0, changePoints };
          }

          async function computeAllMetrics(reqId, progressCb, includeLev) {
            const d = STATE.dataset;
            const commits = d?.commits || [];
            const patches = d?.patches || [];
            const tokensChanged = {};
            const bytesChanged = {};
            const hunks = {};
            const lev = {};
            const total = commits.length;
            for (let i = 0; i < total; i++) {
              throwIfCancelled(reqId);
              const c = commits[i] || {};
              const patch = patches[i] || "";
              const qm = quickMetricsFromPatch(patch);
              tokensChanged[c.hash] = qm.tokensChanged;
              bytesChanged[c.hash] = qm.bytesChanged;
              hunks[c.hash] = qm.hunks;
              if (includeLev && i > 0) {
                lev[c.hash] = await levenshteinForPatch(patch, reqId);
              }
              if (progressCb && (i === total - 1 || i % 3 === 0)) {
                progressCb({
                  stage: "metrics",
                  done: i + 1,
                  total,
                  haveLev: Object.keys(lev).length,
                  message: "Computing per-commit metrics",
                });
              }
            }
            return { tokensChanged, bytesChanged, hunks, lev, commitCount: total };
          }
          const AB_METRICS_CACHE = new Map();
          async function computeABMetrics(aIdx, bIdx, reqId, progressCb) {
            const key = aIdx + ":" + bIdx;
            if (AB_METRICS_CACHE.has(key)) return AB_METRICS_CACHE.get(key);
            progressCb({ stage: "ab_snapshot", message: "Reconstructing snapshot A" });
            const textA = docTextAtLocal(aIdx, reqId, progressCb);
            throwIfCancelled(reqId);
            progressCb({ stage: "ab_snapshot", message: "Reconstructing snapshot B" });
            const textB = docTextAtLocal(bIdx, reqId, progressCb);
            throwIfCancelled(reqId);
            progressCb({ stage: "ab_diff", message: "Computing A/B line diff" });
            const lA = textA.split("\\n"), lB = textB.split("\\n");
            let addL = 0, delL = 0, hn = 0, tokA = 0, tokD = 0, byA = 0, byD = 0;
            const n = lA.length, m = lB.length;
            let ia = 0, ib = 0, tags = [];
            while (ia < n || ib < m) {
              if (ia < n && ib < m && lA[ia] === lB[ib]) { tags.push(" "); ia++; ib++; }
              else {
                let fA = -1, fB = -1;
                const ahead = Math.min(200, Math.max(n - ia, m - ib));
                for (let k = 1; k <= ahead; k++) {
                  if (fA < 0 && ib + k < m && ia < n && lA[ia] === lB[ib + k]) fA = k;
                  if (fB < 0 && ia + k < n && ib < m && lB[ib] === lA[ia + k]) fB = k;
                  if (fA >= 0 || fB >= 0) break;
                }
                if (fA >= 0 && (fB < 0 || fA <= fB)) {
                  for (let k = 0; k < fA; k++) { const l = lB[ib++]; tags.push("+"); addL++; tokA += countRoughTokens(l); byA += l.length + 1; }
                } else if (fB >= 0) {
                  for (let k = 0; k < fB; k++) { const l = lA[ia++]; tags.push("-"); delL++; tokD += countRoughTokens(l); byD += l.length + 1; }
                } else {
                  if (ia < n) { const l = lA[ia++]; tags.push("-"); delL++; tokD += countRoughTokens(l); byD += l.length + 1; }
                  if (ib < m) { const l = lB[ib++]; tags.push("+"); addL++; tokA += countRoughTokens(l); byA += l.length + 1; }
                }
              }
              if (tags.length % 500 === 0) throwIfCancelled(reqId);
            }
            let prev = " ";
            for (const t of tags) { if (t !== " " && prev === " ") hn++; prev = t; }
            progressCb({ stage: "ab_lev", message: "Computing Levenshtein" });
            let lev = null;
            try { const enc = new TextEncoder(); lev = await levenshteinBytes(enc.encode(textA), enc.encode(textB)); } catch {}
            const res = { addLines: addL, delLines: delL, deltaLines: addL - delL, tokensChanged: tokA + tokD, tokensDelta: tokA - tokD, bytesChanged: byA + byD, bytesDelta: byA - byD, hunks: hn, lev, aIdx, bIdx };
            AB_METRICS_CACHE.set(key, res);
            return res;
          }

          function validateDatasetHash(inputHash) {
            if (!STATE.datasetHash) return;
            if (!inputHash) throw new Error("datasetHash missing on worker request");
            if (inputHash !== STATE.datasetHash) {
              throw new Error("datasetHash mismatch: stale request against a different dataset");
            }
          }

          self.onmessage = async (event) => {
            const msg = event?.data || {};
            const op = msg.op;
            const reqId = msg.reqId;
            const payload = msg.payload || {};
            const incomingHash = msg.datasetHash || "";
            if (!op) return;

            if (op === "cancel") {
              const targetReqId = payload.targetReqId;
              if (targetReqId) CANCELLED_REQS.add(targetReqId);
              self.postMessage({
                op,
                reqId,
                type: "ok",
                datasetHash: STATE.datasetHash || "",
                payload: { cancelledReqId: targetReqId || null },
              });
              return;
            }

            const progressCb = (info) => {
              self.postMessage({
                op,
                reqId,
                type: "progress",
                datasetHash: STATE.datasetHash || "",
                payload: info || {},
              });
            };

            try {
              if (!reqId) throw new Error("reqId is required");
              if (op !== "init_dataset") {
                if (!STATE.dataset) throw new Error("Worker dataset not initialized");
                validateDatasetHash(incomingHash);
              }
              throwIfCancelled(reqId);

              let result = null;
              switch (op) {
                case "init_dataset": {
                  const ds = payload.dataset;
                  const dsHash = payload.datasetHash || incomingHash;
                  if (!ds || !Array.isArray(ds.commits) || !Array.isArray(ds.patches)) {
                    throw new Error("Invalid dataset payload for init_dataset");
                  }
                  STATE.dataset = ds;
                  STATE.datasetHash = dsHash || "";
                  STATE.patchHunks = ds.patches.map((p) => parseUnifiedHunks(p));
                  STATE.snapshotCache.clear();
                  STATE.snapshotCache.set(0, String(ds.base_doc || ""));
                  STATE.snapshotCursorIdx = 0;
                  STATE.snapshotCursorLines = String(ds.base_doc || "").split("\\n");
                  STATE.searchIndex = null;
                  STATE.clusterData = null;
                  result = {
                    datasetHash: STATE.datasetHash,
                    commits: ds.commits.length,
                    patches: ds.patches.length,
                  };
                  break;
                }
                case "snapshot_at": {
                  const idx = Number(payload.idx || 0);
                  const text = docTextAtLocal(idx, reqId, progressCb);
                  result = { idx, text };
                  break;
                }
                case "quick_patch_metrics": {
                  result = quickMetricsFromPatch(payload.patch || "");
                  break;
                }
                case "levenshtein_patch": {
                  const lev = await levenshteinForPatch(payload.patch || "", reqId);
                  result = { lev };
                  break;
                }
                case "compute_all_metrics": {
                  const includeLev = payload.includeLev !== false;
                  result = await computeAllMetrics(reqId, progressCb, includeLev);
                  break;
                }
                case "ab_metrics": {
                  const aIdx = Number(payload.aIdx || 0);
                  const bIdx = Number(payload.bIdx || 0);
                  result = await computeABMetrics(aIdx, bIdx, reqId, progressCb);
                  break;
                }
                case "build_search_index": {
                  result = buildSearchIndex();
                  break;
                }
                case "query_search": {
                  result = querySearch(payload.q || "", payload.limit || 20);
                  break;
                }
                case "compute_clusters": {
                  result = computeClusters(payload.limit || 12);
                  break;
                }
                case "compute_phase_map": {
                  result = computePhaseMap(payload.values || [], payload.hazard);
                  break;
                }
                case "compute_outliers": {
                  result = computeOutliers(payload.values || [], payload.topK || 10);
                  break;
                }
                case "extract_outline": {
                  const idx = Number(payload.idx || 0);
                  const text = docTextAtLocal(idx, reqId, progressCb);
                  result = { idx, outline: extractOutlineWorker(text) };
                  break;
                }
                case "heading_metrics": {
                  const idx = Number(payload.idx || 0);
                  const text = docTextAtLocal(idx, reqId, progressCb);
                  const outline = extractOutlineWorker(text);
                  const patch = patchForIdx(idx);
                  const totalLines = text.split("\\n").length;
                  const lineMap = buildLineToHeadingMapW(totalLines, outline);
                  const metrics = attributeHunksToHeadingsW(patch, lineMap);
                  result = { idx, outline, metrics };
                  break;
                }
                default:
                  throw new Error("Unknown worker op: " + op);
              }

              throwIfCancelled(reqId);
              self.postMessage({
                op,
                reqId,
                type: "ok",
                datasetHash: STATE.datasetHash || "",
                payload: result,
              });
            } catch (err) {
              if (err?.name === "AbortError" || isCancelled(reqId)) {
                self.postMessage({
                  op,
                  reqId,
                  type: "cancelled",
                  datasetHash: STATE.datasetHash || "",
                  payload: { message: "Request cancelled" },
                });
              } else {
                self.postMessage({
                  op,
                  reqId,
                  type: "error",
                  datasetHash: STATE.datasetHash || "",
                  error: serializeError(err),
                });
              }
            } finally {
              CANCELLED_REQS.delete(reqId);
            }
          };
        `;
      }

      function tearDownWorker() {
        if (WORKER_STATE.worker) {
          try {
            WORKER_STATE.worker.terminate();
          } catch {
            // ignore
          }
        }
        WORKER_STATE.worker = null;
        WORKER_STATE.ready = false;
        for (const [reqId, req] of WORKER_STATE.pending.entries()) {
          if (req?.timeoutId) clearTimeout(req.timeoutId);
          req?.reject?.(new Error(`Worker terminated while handling reqId=${reqId}`));
        }
        WORKER_STATE.pending.clear();
      }

      function handleWorkerMessage(event) {
        const msg = event?.data || {};
        const reqId = msg.reqId;
        if (!reqId) return;
        const pending = WORKER_STATE.pending.get(reqId);
        if (!pending) return;

        if (msg.type === "progress") {
          pending.onProgress?.(msg.payload || {});
          return;
        }

        if (pending.timeoutId) clearTimeout(pending.timeoutId);
        WORKER_STATE.pending.delete(reqId);

        if (msg.type === "ok") {
          pending.resolve(msg.payload);
          return;
        }
        if (msg.type === "cancelled") {
          const abortErr = new DOMException("Worker request cancelled", "AbortError");
          pending.reject(abortErr);
          return;
        }

        const errObj = msg.error || {};
        const err = new Error(errObj.message || `Worker op failed (${msg.op || "unknown"})`);
        err.name = errObj.name || "WorkerError";
        if (errObj.stack) err.stack = errObj.stack;
        pending.reject(err);
      }

      function workerRequest(op, payload, opts = {}) {
        if (!WORKER_STATE.worker || (op !== "init_dataset" && !WORKER_STATE.ready)) {
          return Promise.reject(new Error(`Worker unavailable for op=${op}`));
        }
        const reqId = `w${WORKER_STATE.reqSeq++}`;
        const timeoutMs = Number(opts.timeoutMs || 0);
        const signal = opts.signal || null;
        let aborted = false;
        let timeoutId = null;

        return new Promise((resolve, reject) => {
          const abortHandler = () => {
            aborted = true;
            try {
              WORKER_STATE.worker.postMessage({
                op: "cancel",
                reqId: `cancel-${reqId}`,
                datasetHash: WORKER_STATE.datasetHash,
                payload: { targetReqId: reqId },
              });
            } catch {
              // ignore transport errors
            }
            WORKER_STATE.pending.delete(reqId);
            reject(new DOMException("Aborted by caller", "AbortError"));
          };

          if (signal) {
            if (signal.aborted) {
              abortHandler();
              return;
            }
            signal.addEventListener("abort", abortHandler, { once: true });
          }

          if (timeoutMs > 0) {
            timeoutId = setTimeout(() => {
              WORKER_STATE.pending.delete(reqId);
              reject(new Error(`Worker request timed out: op=${op} reqId=${reqId} after ${timeoutMs}ms`));
            }, timeoutMs);
          }

          WORKER_STATE.pending.set(reqId, {
            resolve: (value) => {
              if (signal) signal.removeEventListener("abort", abortHandler);
              if (aborted) return;
              resolve(value);
            },
            reject: (err) => {
              if (signal) signal.removeEventListener("abort", abortHandler);
              if (aborted) return;
              reject(err);
            },
            onProgress: opts.onProgress,
            timeoutId,
          });

          WORKER_STATE.worker.postMessage({
            op,
            reqId,
            datasetHash: WORKER_STATE.datasetHash,
            payload: payload || {},
          });
        });
      }

      async function initAnalysisWorker() {
        if (!supportsAnalysisWorker()) {
          WORKER_STATE.disabled = true;
          setWorkerStatus("Web Worker unavailable; using main-thread compute fallback.", "error");
          return false;
        }
        if (WORKER_STATE.ready) return true;
        if (!DATASET.loaded || !DATASET.data) return false;

        try {
          WORKER_STATE.datasetHash = await computeDatasetHash(DATASET.data);
          const src = makeAnalysisWorkerSource();
          const blob = new Blob([src], { type: "text/javascript" });
          const url = URL.createObjectURL(blob);
          const worker = new Worker(url, { name: "spec-evolution-worker" });
          URL.revokeObjectURL(url);
          worker.addEventListener("message", handleWorkerMessage);
          worker.addEventListener("error", (event) => {
            setWorkerStatus(`Worker runtime error: ${event.message || "unknown error"}`, "error");
          });
          WORKER_STATE.worker = worker;

          const initPayload = await workerRequest("init_dataset", {
            datasetHash: WORKER_STATE.datasetHash,
            dataset: DATASET.data,
          });

          if (!initPayload || initPayload.datasetHash !== WORKER_STATE.datasetHash) {
            throw new Error("Worker dataset initialization returned an unexpected dataset hash");
          }
          WORKER_STATE.ready = true;
          setWorkerStatus(
            `Worker online · dataset ${WORKER_STATE.datasetHash.slice(0, 12)}… · ${fmtInt(initPayload.commits)} commits`,
            "ok",
          );
          return true;
        } catch (e) {
          WORKER_STATE.disabled = true;
          tearDownWorker();
          setWorkerStatus("Worker init failed; fallback active.", "error", formatErr(e));
          return false;
        }
      }

      async function warmDerivedWorkerArtifacts() {
        if (!WORKER_STATE.ready) return;
        try {
          await workerRequest("build_search_index", {}, { timeoutMs: 30000 });
          WORKER_DERIVED.searchReady = true;
          await workerRequest("compute_clusters", { limit: 12 }, { timeoutMs: 30000 });
          WORKER_DERIVED.clusterReady = true;
          setWorkerStatus("Worker online · search index + clustering ready", "ok");
        } catch (e) {
          console.error("Worker warmup failed:", e);
          setWorkerStatus("Worker warmup partial failure.", "error", formatErr(e));
        }
      }

      async function maybeRefreshPhaseAndOutliers(commits) {
        if (!WORKER_STATE.ready || !Array.isArray(commits) || !commits.length) return;
        const hazard = Number(document.getElementById("hazard")?.value || 0.03);
        const values = commits.map((c) => Math.log1p(Number(c.impact || 0)));
        const outlierValues = commits.map((c) => Number(c.impact || 0));
        const phaseKey = `${hazard}|${values.join(",")}`;
        const outlierKey = outlierValues.join(",");

        if (WORKER_DERIVED.phaseKey === phaseKey && WORKER_DERIVED.outlierKey === outlierKey) return;

        PHASE_ABORT_CONTROLLER?.abort();
        OUTLIER_ABORT_CONTROLLER?.abort();
        PHASE_ABORT_CONTROLLER = new AbortController();
        OUTLIER_ABORT_CONTROLLER = new AbortController();

        try {
          const [phase, outliers] = await Promise.all([
            workerRequest(
              "compute_phase_map",
              { values, hazard },
              { signal: PHASE_ABORT_CONTROLLER.signal, timeoutMs: 30000 },
            ),
            workerRequest(
              "compute_outliers",
              { values: outlierValues, topK: 10 },
              { signal: OUTLIER_ABORT_CONTROLLER.signal, timeoutMs: 30000 },
            ),
          ]);
          WORKER_DERIVED.phase = phase || null;
          WORKER_DERIVED.outliers = outliers || null;
          WORKER_DERIVED.phaseKey = phaseKey;
          WORKER_DERIVED.outlierKey = outlierKey;
          setWorkerStatus(
            `Worker online · phase CP ${fmtInt(phase?.changePoints?.length || 0)} · outlier set ${fmtInt(
              outliers?.top?.length || 0,
            )}`,
            "ok",
          );
          renderBocpd(commits);
        } catch (e) {
          if (e?.name === "AbortError") return;
          console.error("Phase/outlier worker compute failed:", e);
        }
      }

      // -----------------------------
      // Dataset + WASM loading
      // -----------------------------

      async function gunzipArrayBufferToString(ab) {
        // Prefer native DecompressionStream; fall back to pako.
        if (typeof DecompressionStream !== "undefined") {
          const ds = new DecompressionStream("gzip");
          const decompressedStream = new Blob([ab]).stream().pipeThrough(ds);
          const buf = await new Response(decompressedStream).arrayBuffer();
          return new TextDecoder().decode(buf);
        }
        if (typeof pako !== "undefined" && pako?.ungzip) {
          return pako.ungzip(new Uint8Array(ab), { to: "string" });
        }
        throw new Error("No gzip decompressor available in this browser.");
      }

      async function loadEvolutionDataset() {
        try {
          const res = await fetch(SPEC_EVOLUTION_DATA_URL, { cache: "no-store" });
          if (!res.ok) throw new Error(`Failed to fetch ${SPEC_EVOLUTION_DATA_URL}: HTTP ${res.status}`);
          const ab = await res.arrayBuffer();
          const jsonText = await gunzipArrayBufferToString(ab);
          const data = JSON.parse(jsonText);
          if (!data || !Array.isArray(data.commits) || !Array.isArray(data.patches) || !data.base_doc) {
            throw new Error("Dataset schema mismatch (expected commits[], patches[], base_doc).");
          }
          DATASET.data = data;
          DATASET.loaded = true;
          DATASET.error = null;
          return true;
        } catch (e) {
          DATASET.error = String(e?.message || e);
          DATASET.loaded = false;
          return false;
        }
      }

      async function initLevWasm() {
        if (LEV_WASM) return LEV_WASM;
        const res = await fetch(LEV_WASM_URL, { cache: "force-cache" });
        if (!res.ok) throw new Error(`Failed to fetch ${LEV_WASM_URL}: HTTP ${res.status}`);
        const buf = await res.arrayBuffer();
        const { instance } = await WebAssembly.instantiate(buf, {});
        LEV_WASM = instance.exports;
        return LEV_WASM;
      }

      async function levenshteinBytes(aBytes, bBytes) {
        const ex = await initLevWasm();
        const { memory, alloc, dealloc, levenshtein } = ex;
        if (!memory || !alloc || !dealloc || !levenshtein) {
          throw new Error("WASM exports missing (expected memory/alloc/dealloc/levenshtein).");
        }

        const a = aBytes || new Uint8Array();
        const b = bBytes || new Uint8Array();

        const ptrA = alloc(a.length);
        let viewA = new Uint8Array(memory.buffer, ptrA, a.length);
        viewA.set(a);

        const ptrB = alloc(b.length);
        let viewB = new Uint8Array(memory.buffer, ptrB, b.length);
        viewB.set(b);

        const d = levenshtein(ptrA, a.length, ptrB, b.length) >>> 0;

        dealloc(ptrA, a.length);
        dealloc(ptrB, b.length);

        return d;
      }

      // -----------------------------
      // Unified diff helpers (single-file)
      // -----------------------------

      function countRoughTokens(s) {
        // Approximate tokens for visualization: words + punctuation runs.
        let n = 0;
        const re = /[A-Za-z0-9_]+|[^\\s]/g;
        while (re.exec(String(s))) n++;
        return n;
      }

      function quickMetricsFromPatch(patch) {
        const lines = String(patch || "").split("\n");
        let hunks = 0;
        let addLines = 0;
        let delLines = 0;
        let tokAdd = 0;
        let tokDel = 0;
        let bytesAdd = 0;
        let bytesDel = 0;
        for (const l of lines) {
          if (l.startsWith("@@")) hunks += 1;
          if (l.startsWith("+")) {
            if (l.startsWith("+++")) continue;
            addLines += 1;
            const s = l.slice(1);
            tokAdd += countRoughTokens(s);
            bytesAdd += s.length + 1;
          } else if (l.startsWith("-")) {
            if (l.startsWith("---")) continue;
            delLines += 1;
            const s = l.slice(1);
            tokDel += countRoughTokens(s);
            bytesDel += s.length + 1;
          }
        }
        return {
          hunks,
          addLines,
          delLines,
          tokensChanged: tokAdd + tokDel,
          bytesChanged: bytesAdd + bytesDel,
          tokensDelta: tokAdd - tokDel,
          bytesDelta: bytesAdd - bytesDel,
        };
      }

      function parseUnifiedHunks(patch) {
        const lines = String(patch || "").split("\n");
        const hunks = [];
        for (let i = 0; i < lines.length; i++) {
          const line = lines[i];
          if (!line.startsWith("@@")) continue;
          const m = /^@@ -(\\d+)(?:,(\\d+))? \\+(\\d+)(?:,(\\d+))? @@/.exec(line);
          if (!m) continue;
          const oldStart = Number(m[1]);
          const oldCount = Number(m[2] || "1");
          const newStart = Number(m[3]);
          const newCount = Number(m[4] || "1");

          const hunkLines = [];
          i++;
          for (; i < lines.length; i++) {
            const l = lines[i];
            if (l.startsWith("@@")) {
              i--;
              break;
            }
            if (l.startsWith("diff --git")) break;
            if (l.startsWith("index ") || l.startsWith("---") || l.startsWith("+++")) continue;
            hunkLines.push(l);
          }

          hunks.push({ oldStart, oldCount, newStart, newCount, lines: hunkLines });
        }
        return hunks;
      }

      const BUCKET_MAP = new Map();
      function bucketById(id) {
        if (BUCKET_MAP.size === 0) {
          for (const b of BUCKETS) BUCKET_MAP.set(b.id, b);
        }
        return BUCKET_MAP.get(id) || BUCKETS[BUCKETS.length - 1];
      }

      function pickPrimary(labels) {
        const s = new Set(labels);
        if (s.has(2)) return 2;
        if (s.has(3)) return 3;
        if (s.has(1)) return 1;
        if (s.has(4)) return 4;
        if (s.has(8)) return 8;
        if (s.has(7)) return 7;
        if (s.has(6)) return 6;
        if (s.has(5)) return 5;
        if (s.has(9)) return 9;
        return 10;
      }

      function normalizeClassification(commits) {
        const out = new Map();
        const add = (entry, source) => {
          if (!entry || !entry.commit) return;
          out.set(entry.commit, { ...entry, _source: source });
        };
        CLASS_EARLY.forEach((e) => add(e, "early"));
        CLASS_MIDDLE.forEach((e) => add(e, "middle"));
        CLASS_LATE.forEach((e) => add(e, "late"));

        const missing = [];
        for (const c of commits) {
          if (!out.has(c.hash)) missing.push(c.hash);
        }

        return { byHash: out, missing };
      }

      function deriveBucketsForGroup(group, commitSubject) {
        // If the group already uses numeric buckets, preserve.
        if (Array.isArray(group.categories) && group.categories.every((x) => Number.isInteger(x))) {
          const labels = uniqInts(group.categories);
          return { labels, primary: group.primary_category ?? pickPrimary(labels) };
        }

        const labels = new Set();
        const s = `${commitSubject || ""} ${group.summary || ""} ${(group.evidence || []).join(" ")}`.toLowerCase();

        // Tag-driven mapping (from middle/late agents)
        const tags = Array.isArray(group.categories) ? group.categories : [];
        for (const t of tags) {
          const tt = String(t).toLowerCase();
          if (tt.includes("doc meta") || tt.includes("summary_update")) labels.add(5);
          if (tt.includes("clarification")) labels.add(9);
          if (tt.includes("spec expansion") || tt.includes("addition")) labels.add(6);
          if (tt.includes("architecture")) labels.add(4);
          if (tt.includes("api/interface")) labels.add(4);
          if (tt.includes("sql semantics")) labels.add(2);
          if (tt.includes("file format")) labels.add(2);
          if (tt.includes("durability")) labels.add(7);
          if (tt.includes("concurrency")) labels.add(7);
          if (tt.includes("performance")) labels.add(7);
          if (tt.includes("math/modeling")) labels.add(8);
          if (tt.includes("correctness fix") || tt.includes("correction")) labels.add(1);
          if (tt.includes("requirement_change")) labels.add(4);
        }

        // Content heuristics to refine buckets.
        if (s.includes("sqlite") || s.includes("wal") || s.includes("wal-index") || s.includes("btree") || s.includes("vdbe") || s.includes("fts5") || s.includes("lemon") || s.includes("parse.y")) {
          labels.add(2);
        }
        if (s.includes("asupersync") || s.includes("cx") || s.includes("virtualtcp") || s.includes("region") || s.includes("spawn_blocking") || s.includes("deadline")) {
          labels.add(3);
        }
        if (s.includes("bocpd") || s.includes("conformal") || s.includes("e-process") || s.includes("evalue") || s.includes("vo i") || s.includes("gf(256)") || s.includes("raptorq") || s.includes("martingale")) {
          labels.add(8);
        }
        if (s.includes("cache") || s.includes("prefetch") || s.includes("alignment") || s.includes("atomic") || s.includes("acquire") || s.includes("release") || s.includes("bulkhead") || s.includes("rate_limit") || s.includes("shard") || s.includes("cache-line")) {
          labels.add(7);
        }
        if (s.includes("renumber") || s.includes("footer") || s.includes("document version") || s.includes("typo") || s.includes("wording tweak")) {
          labels.add(5);
        }
        if (s.startsWith("add ") || s.includes("added ") || s.includes("introduced ") || s.includes("defined ") || s.includes("expanded ")) {
          labels.add(6);
        }
        if (s.includes("clarif") || s.includes("explain") || s.includes("note")) {
          labels.add(9);
        }
        if (s.includes("fix ") || s.includes("fixed ") || s.includes("correct") || s.includes("arithmetic") || s.includes("inversion") || s.includes("swapped")) {
          labels.add(1);
        }
        if (s.includes("rework") || s.includes("redesign") || s.includes("protocol") || s.includes("invariant") || s.includes("formal model")) {
          labels.add(4);
        }

        if (labels.size === 0) labels.add(10);

        const labelsArr = Array.from(labels).sort((a, b) => a - b);
        const primary = pickPrimary(labelsArr);
        return { labels: labelsArr, primary };
      }

      function uniqInts(xs) {
        const s = new Set();
        for (const x of xs) s.add(Number(x));
        return Array.from(s).filter((n) => Number.isInteger(n)).sort((a, b) => a - b);
      }

      // -----------------------------
      // Rendering
      // -----------------------------

      let chartTimeline = null;
      let chartStack = null;
      let chartDonut = null;
      let chartBocpd = null;

      let ALL_COMMITS = [];
      let FILTERED_COMMITS = [];
      let DOCK_READY = false;

      // --- Phase 1: Lazy enrichment cache (parse + enrich once) ---
      let ENRICHED_READY = false;
      let ENRICHED_MISSING = [];

      function ensureEnriched() {
        if (ENRICHED_READY) return;
        const commits = parseCommitLog();
        const stats = parseCommitStats();
        const { byHash: clsByHash, missing } = normalizeClassification(commits);
        ENRICHED_MISSING = missing;

        ALL_COMMITS = commits.map((c) => {
          const st = stats.get(c.hash) || { add: 0, del: 0, impact: 0 };
          const raw = clsByHash.get(c.hash) || null;
          const changeGroupsRaw = raw?.change_groups || [];
          const changeGroups = changeGroupsRaw.map((g) => {
            const { labels, primary } = deriveBucketsForGroup(g, c.subject);
            return {
              summary: g.summary || "",
              evidence: Array.isArray(g.evidence) ? g.evidence : [],
              changed_headings: Array.isArray(g.changed_headings) ? g.changed_headings : [],
              confidence: typeof g.confidence === "number" ? g.confidence : 0.55,
              labels,
              primary,
            };
          });

          const groupLabels = new Set();
          for (const g of changeGroups) {
            for (const b of g.labels) groupLabels.add(b);
          }
          const labels = Array.from(groupLabels).sort((a, b) => a - b);
          const primary = pickPrimary(labels.length ? labels : [10]);

          return {
            ...c,
            ...st,
            labels,
            primary,
            changeGroups,
            hasClassification: Boolean(raw),
          };
        });

        ENRICHED_READY = true;
      }

      function render() {
        ensureEnriched();
        const enriched = ALL_COMMITS;
        const missing = ENRICHED_MISSING;

        // KPI
        document.getElementById("kpiCommits").textContent = fmtInt(enriched.length);
        const totalGroups = enriched.reduce((acc, c) => acc + c.changeGroups.length, 0);
        document.getElementById("kpiGroups").textContent = fmtInt(totalGroups);
        const totalLines = enriched.reduce((acc, c) => acc + c.impact, 0);
        document.getElementById("kpiLines").textContent = fmtInt(totalLines);
        document.getElementById("kpiMode").textContent = STATE.bucketMode === "primary" ? "Primary" : "Multi";
        document.getElementById("kpiIntegrity").textContent = missing.length ? `${missing.length} missing` : "OK";

        // Make the impact slider usable even with very large commits: cap at ~p99.
        {
          const impacts = enriched.map((c) => c.impact).slice().sort((a, b) => a - b);
          const idx = Math.max(0, Math.ceil(0.99 * impacts.length) - 1);
          const p99 = impacts[idx] || 200;
          const maxImpact = Math.max(200, p99);
          const impact = document.getElementById("impact");
          const impactMobile = document.getElementById("impactMobile");
          if (impact && impactMobile) {
            impact.max = String(maxImpact);
            impactMobile.max = String(maxImpact);
            if (STATE.minImpact > maxImpact) {
              STATE.minImpact = maxImpact;
              impact.value = String(maxImpact);
              impactMobile.value = String(maxImpact);
              document.getElementById("impactLabel").textContent = `${fmtInt(maxImpact)} lines`;
              document.getElementById("impactLabelMobile").textContent = `${fmtInt(maxImpact)} lines`;
            }
          }
        }

        const earliest = enriched[0];
        const latest = enriched[enriched.length - 1];
        document.getElementById("rangeLabel").textContent = `${earliest.short} → ${latest.short}`;
        document.getElementById("metaSpan").textContent = `${dayjs(earliest.dateIso).format("YYYY-MM-DD HH:mm")} to ${dayjs(latest.dateIso).format("HH:mm")}`;

        // Filter
        const filtered = enriched.filter((c) => matchesFilters(c));
        FILTERED_COMMITS = filtered;
        document.getElementById("showingCount").textContent = `${fmtInt(filtered.length)} commits`;

        // Charts + list
        renderBucketToggles();
        renderCharts(filtered);
        renderCommitList(filtered);
        syncDockAndDoc();

        // Make sure code highlighting is applied to newly inserted excerpts
        requestAnimationFrame(() => {
          document.querySelectorAll("pre code:not(.hljs)").forEach((el) => {
            try {
              hljs.highlightElement(el);
            } catch {
              // ignore
            }
          });
        });

        // Keep URL in sync with filter/view state.
        syncUrlToState();
      }

      function matchesFilters(commit) {
        if (commit.impact < STATE.minImpact) return false;
        const q = STATE.q.trim().toLowerCase();
        if (q) {
          const hay =
            `${commit.hash} ${commit.short} ${commit.author} ${commit.subject} ${commit.changeGroups
              .map((g) => `${g.summary} ${(g.changed_headings || []).join(" ")} ${(g.evidence || []).join(" ")}`)
              .join(" ")}`.toLowerCase();
          if (!hay.includes(q)) return false;
        }

        const enabled = STATE.bucketEnabled;
        if (STATE.bucketMode === "primary") {
          return enabled.has(commit.primary);
        }
        // multi-label: require overlap
        return commit.labels.some((b) => enabled.has(b));
      }

      // -----------------------------
      // Dock + Doc Evolution UI
      // -----------------------------

      const DOC_CURSOR = {
        idx: 0,
        lines: null,
      };

      function setDocTab(tab) {
        DOC.tab = tab;
        // Buttons
        const btnSpec = document.getElementById("docTabSpec");
        const btnDiff = document.getElementById("docTabDiff");
        const btnMetrics = document.getElementById("docTabMetrics");
        const btnSections = document.getElementById("docTabSections");
        const on = "focus-ring rounded-2xl bg-slate-900 px-3 py-2 text-xs font-semibold text-white hover:bg-slate-800";
        const off =
          "focus-ring rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-xs font-semibold text-slate-900 hover:bg-white";
        if (btnSpec) btnSpec.className = tab === "spec" ? on : off;
        if (btnDiff) btnDiff.className = tab === "diff" ? on : off;
        if (btnMetrics) btnMetrics.className = tab === "metrics" ? on : off;
        if (btnSections) btnSections.className = tab === "sections" ? on : off;
        updateDocPanelVisibility();
        syncUrlToState();
        // Re-render diff/spec if needed.
        void updateDocUI();
      }

      function updateDocPanelVisibility() {
        const spec = document.getElementById("docSpecView");
        const diff = document.getElementById("docDiffView");
        const metrics = document.getElementById("docMetricsView");
        const sections = document.getElementById("docSectionsView");
        if (!spec || !diff || !metrics) return;
        spec.classList.toggle("hidden", DOC.tab !== "spec");
        diff.classList.toggle("hidden", DOC.tab !== "diff");
        metrics.classList.toggle("hidden", DOC.tab !== "metrics");
        if (sections) sections.classList.toggle("hidden", DOC.tab !== "sections");
      }

      function patchForIdx(idx) {
        const d = DATASET.data;
        if (!d || !Array.isArray(d.patches)) return "";
        return d.patches[idx] || "";
      }

      // --- A/B Compare Typeahead Picker System (bd-24q.1) ---

      /** Format a commit label for the picker button. */
      function fmtPickerLabel(idx) {
        const c = ALL_COMMITS[idx];
        if (!c) return `#${idx}`;
        return `#${idx} ${c.short} – ${c.subject.slice(0, 32)}`;
      }

      /** Render commit items into a picker list element, filtered by query. */
      function renderPickerList(listEl, query, selectedIdx, onSelect) {
        listEl.innerHTML = "";
        const q = (query || "").toLowerCase().trim();
        const items = [];
        for (let i = 0; i < ALL_COMMITS.length; i++) {
          const c = ALL_COMMITS[i];
          const searchable = `#${i} ${c.short} ${c.hash} ${c.subject}`.toLowerCase();
          if (q && !searchable.includes(q)) continue;
          items.push(i);
        }
        if (items.length === 0) {
          const empty = document.createElement("div");
          empty.className = "px-3 py-4 text-center text-[11px] text-slate-400";
          empty.textContent = q ? "No matching commits" : "No commits loaded";
          listEl.appendChild(empty);
          return;
        }
        for (const i of items) {
          const c = ALL_COMMITS[i];
          const div = document.createElement("div");
          div.className = "ab-picker-item" + (i === selectedIdx ? " selected" : "");
          div.dataset.idx = i;
          div.innerHTML = `<span class="idx">#${i}</span><span class="hash">${escapeHtml(c.short)}</span><span class="subject">${escapeHtml(c.subject)}</span>`;
          div.addEventListener("click", () => onSelect(i));
          listEl.appendChild(div);
        }
      }

      /** Open a typeahead picker dropdown. */
      function openPicker(pickerId) {
        const dropdown = document.getElementById(pickerId + "Dropdown");
        const search = document.getElementById(pickerId + "Search");
        if (!dropdown) return;
        // Close any other open picker first.
        document.querySelectorAll(".ab-picker-dropdown").forEach(d => { if (d.id !== pickerId + "Dropdown") d.classList.add("hidden"); });
        dropdown.classList.remove("hidden");
        if (search) { search.value = ""; search.focus(); }
        const isA = pickerId === "pickerA";
        const selectedIdx = isA ? DOC.compareFromIdx : DOC.compareToIdx;
        const listEl = document.getElementById(pickerId + "List");
        if (listEl) {
          renderPickerList(listEl, "", selectedIdx, (idx) => {
            if (isA) DOC.compareFromIdx = idx; else DOC.compareToIdx = idx;
            closePicker(pickerId);
            populateCompareSelects();
            syncUrlToState();
            void updateDocUI();
          });
          // Scroll selected item into view.
          requestAnimationFrame(() => {
            const sel = listEl.querySelector(".selected");
            if (sel) sel.scrollIntoView({ block: "center", behavior: "instant" });
          });
        }
      }

      /** Close a typeahead picker dropdown. */
      function closePicker(pickerId) {
        const dropdown = document.getElementById(pickerId + "Dropdown");
        if (dropdown) dropdown.classList.add("hidden");
      }

      /** Wire up search input filtering for a picker. */
      function wirePickerSearch(pickerId) {
        const search = document.getElementById(pickerId + "Search");
        const listEl = document.getElementById(pickerId + "List");
        if (!search || !listEl) return;
        let debounce = 0;
        const isA = pickerId === "pickerA";
        search.addEventListener("input", () => {
          clearTimeout(debounce);
          debounce = setTimeout(() => {
            const selectedIdx = isA ? DOC.compareFromIdx : DOC.compareToIdx;
            renderPickerList(listEl, search.value, selectedIdx, (idx) => {
              if (isA) DOC.compareFromIdx = idx; else DOC.compareToIdx = idx;
              closePicker(pickerId);
              populateCompareSelects();
              syncUrlToState();
              void updateDocUI();
            });
          }, 80);
        });
        // Keyboard navigation.
        search.addEventListener("keydown", (e) => {
          if (e.key === "Escape") { closePicker(pickerId); return; }
          if (e.key === "ArrowDown" || e.key === "ArrowUp") {
            e.preventDefault();
            const items = listEl.querySelectorAll(".ab-picker-item");
            if (!items.length) return;
            let active = listEl.querySelector(".keyboard-active");
            let nextIdx = 0;
            if (active) {
              active.classList.remove("keyboard-active");
              const arr = Array.from(items);
              const cur = arr.indexOf(active);
              nextIdx = e.key === "ArrowDown" ? Math.min(cur + 1, arr.length - 1) : Math.max(cur - 1, 0);
            }
            items[nextIdx].classList.add("keyboard-active");
            items[nextIdx].scrollIntoView({ block: "nearest" });
          }
          if (e.key === "Enter") {
            const active = listEl.querySelector(".keyboard-active") || listEl.querySelector(".ab-picker-item");
            if (active) active.click();
          }
        });
      }

      /** Populate the A/B compare typeahead pickers from ALL_COMMITS. */
      function populateCompareSelects() {
        if (!ALL_COMMITS.length) return;
        // Update picker button labels.
        const btnA = document.getElementById("pickerABtn");
        const btnB = document.getElementById("pickerBBtn");
        if (btnA) btnA.textContent = fmtPickerLabel(DOC.compareFromIdx);
        if (btnB) btnB.textContent = fmtPickerLabel(DOC.compareToIdx);
      }

      /**
       * Generate a unified diff between two arbitrary commit snapshots.
       * Uses jsdiff (Diff library) to compute and returns a unified-diff string
       * suitable for Diff2Html rendering.
       */
      async function generateABDiff(fromIdx, toIdx) {
        const loading = document.getElementById("abDiffLoading");
        if (loading) loading.classList.remove("hidden");
        try {
          const [textA, textB] = await Promise.all([
            docTextAt(fromIdx),
            docTextAt(toIdx),
          ]);
          const commitA = ALL_COMMITS[fromIdx];
          const commitB = ALL_COMMITS[toIdx];
          const nameA = commitA ? `${commitA.short} (${commitA.subject.slice(0, 30)})` : `commit #${fromIdx}`;
          const nameB = commitB ? `${commitB.short} (${commitB.subject.slice(0, 30)})` : `commit #${toIdx}`;
          // Diff.createTwoFilesPatch(oldFileName, newFileName, oldStr, newStr, oldHeader, newHeader)
          const patch = Diff.createTwoFilesPatch(
            nameA, nameB,
            textA, textB,
            `commit #${fromIdx}`, `commit #${toIdx}`
          );
          return patch;
        } finally {
          if (loading) loading.classList.add("hidden");
        }
      }

      const AB_METRICS_MAIN_CACHE = new Map();
      let AB_METRICS_ABORT = null;
      async function computeAndShowABMetrics(aIdx, bIdx) {
        const bar = document.getElementById("abMetricsBar");
        if (!bar) return;
        if (aIdx === bIdx) { bar.classList.add("hidden"); return; }
        bar.classList.remove("hidden");
        const chips = ["abmLines", "abmTokens", "abmLev", "abmHunks", "abmBytes"];
        for (const id of chips) { const el = document.getElementById(id); if (el) el.textContent = "..."; }
        if (AB_METRICS_ABORT) AB_METRICS_ABORT.abort();
        AB_METRICS_ABORT = new AbortController();
        const cacheKey = `${WORKER_STATE.datasetHash}:${aIdx}:${bIdx}`;
        const cached = AB_METRICS_MAIN_CACHE.get(cacheKey);
        if (cached) { renderABMetricChips(cached); return; }
        try {
          let metrics;
          if (WORKER_STATE.ready) {
            metrics = await workerRequest("ab_metrics", { aIdx, bIdx }, { timeoutMs: 120000, signal: AB_METRICS_ABORT.signal });
          } else {
            const patch = await generateABDiff(aIdx, bIdx);
            metrics = quickMetricsFromPatch(patch);
            metrics.lev = null;
          }
          AB_METRICS_MAIN_CACHE.set(cacheKey, metrics);
          renderABMetricChips(metrics);
        } catch (e) {
          if (e?.name !== "AbortError") {
            console.error("A/B metrics failed:", e);
            for (const id of chips) { const el = document.getElementById(id); if (el) el.textContent = "ERR"; }
          }
        }
      }
      function renderABMetricChips(m) {
        const sign = (v) => v > 0 ? "+" + fmtInt(v) : v < 0 ? fmtInt(v) : "0";
        const el = (id) => document.getElementById(id);
        if (el("abmLines")) el("abmLines").textContent = `\u0394lines ${sign(m.deltaLines || 0)} (+${fmtInt(m.addLines || 0)} -${fmtInt(m.delLines || 0)})`;
        if (el("abmTokens")) el("abmTokens").textContent = `\u0394tokens ${sign(m.tokensDelta || 0)} (${fmtInt(m.tokensChanged || 0)} touched)`;
        if (el("abmLev")) el("abmLev").textContent = `Lev ${m.lev != null ? fmtInt(m.lev) : "?"}`;
        if (el("abmHunks")) el("abmHunks").textContent = `Hunks ${fmtInt(m.hunks || 0)}`;
        if (el("abmBytes")) el("abmBytes").textContent = `\u0394bytes ${sign(m.bytesDelta || 0)} (${fmtInt(m.bytesChanged || 0)} touched)`;
      }

      function applyPatchLines(prevLines, patch) {
        const hunks = parseUnifiedHunks(patch);
        let out = prevLines.slice();
        let offset = 0;
        for (const h of hunks) {
          let at = (h.oldStart - 1) + offset;
          at = clamp(at, 0, out.length);
          let cursor = at;
          const next = [];
          for (const hl of h.lines) {
            if (!hl) continue;
            const p = hl[0];
            const content = hl.slice(1);
            if (p === " ") {
              next.push(content);
              cursor += 1;
            } else if (p === "-") {
              cursor += 1;
            } else if (p === "+") {
              next.push(content);
            } else {
              // "\ No newline at end of file" or other metadata
            }
          }
          out.splice(at, cursor - at, ...next);
          offset += next.length - (cursor - at);
        }
        return out;
      }

      async function docTextAtLocal(idx) {
        const d = DATASET.data;
        if (!d) return "";
        if (idx <= 0) return String(d.base_doc || "");

        const cached = DOC_CACHE.get(idx);
        if (cached?.text) return cached.text;

        // Fast path: sequential scrub.
        if (DOC_CURSOR.lines && idx === DOC_CURSOR.idx + 1) {
          const nextLines = applyPatchLines(DOC_CURSOR.lines, patchForIdx(idx));
          DOC_CURSOR.idx = idx;
          DOC_CURSOR.lines = nextLines;
          const text = nextLines.join("\n");
          DOC_CACHE.set(idx, { text });
          return text;
        }

        // Rebuild from nearest cached anchor.
        let anchor = 0;
        for (let j = idx - 1; j > 0; j--) {
          if (DOC_CACHE.has(j)) {
            anchor = j;
            break;
          }
        }

        let lines = String(d.base_doc || "").split("\n");
        if (anchor > 0) lines = String(DOC_CACHE.get(anchor)?.text || "").split("\n");

        for (let k = Math.max(1, anchor + 1); k <= idx; k++) {
          lines = applyPatchLines(lines, patchForIdx(k));
          // Keep sparse anchors to accelerate non-linear scrubs.
          if (k === idx || k % 10 === 0) DOC_CACHE.set(k, { text: lines.join("\n") });
        }

        DOC_CURSOR.idx = idx;
        DOC_CURSOR.lines = lines;
        return DOC_CACHE.get(idx)?.text || lines.join("\n");
      }

      async function docTextAt(idx) {
        if (WORKER_STATE.ready) {
          try {
            const res = await workerRequest("snapshot_at", { idx }, { timeoutMs: 45000 });
            if (typeof res?.text === "string") return res.text;
          } catch (e) {
            console.error("Worker snapshot reconstruction failed; falling back to local path:", e);
          }
        }
        return docTextAtLocal(idx);
      }

      // -----------------------------
      // Heading Outline Extraction (bd-24q.8.1)
      // -----------------------------

      /**
       * Generate a URL-safe slug from heading text.
       * Deterministic: same input always yields same output.
       */
      function slugifyHeading(text) {
        return String(text || "")
          .toLowerCase()
          .replace(/[^\p{L}\p{N}]+/gu, "-")
          .replace(/^-+|-+$/g, "")
          || "heading";
      }

      /**
       * Extract an outline from markdown text using markdown-it's token stream.
       * Returns: [{ text: string, level: number, id: string }]
       *
       * Anchor IDs are deterministic and disambiguated: if two headings
       * produce the same slug, a numeric suffix (-1, -2, ...) is appended.
       */
      function extractOutline(markdownText) {
        if (!window._mdSingleton) {
          window._mdSingleton = markdownit({
            html: false,
            linkify: true,
            typographer: true,
            highlight: (str, lang) => {
              try {
                if (lang && hljs.getLanguage(lang)) {
                  return `<pre class="hljs"><code>${hljs.highlight(str, { language: lang }).value}</code></pre>`;
                }
                return `<pre class="hljs"><code>${hljs.highlightAuto(str).value}</code></pre>`;
              } catch {
                return `<pre class="hljs"><code>${escapeHtml(str)}</code></pre>`;
              }
            },
          });
        }
        const tokens = window._mdSingleton.parse(markdownText || "", {});
        const outline = [];
        const slugCounts = new Map(); // slug -> count for disambiguation

        for (let i = 0; i < tokens.length; i++) {
          const tok = tokens[i];
          if (tok.type !== "heading_open") continue;

          // Level from tag: "h1" -> 1, "h2" -> 2, etc.
          const level = Number(tok.tag?.slice(1)) || 1;

          // Source line (1-based) from markdown-it token map.
          const line = tok.map ? tok.map[0] + 1 : null;

          // Next token should be the inline content.
          const inlineTok = tokens[i + 1];
          let text = "";
          if (inlineTok?.type === "inline") {
            // Collect all text children (handles bold, code, etc. inside headings).
            text = (inlineTok.children || [])
              .filter((c) => c.type === "text" || c.type === "code_inline")
              .map((c) => c.content)
              .join("");
            if (!text) text = inlineTok.content || "";
          }

          // Generate stable, disambiguated slug.
          const baseSlug = slugifyHeading(text);
          const count = slugCounts.get(baseSlug) || 0;
          slugCounts.set(baseSlug, count + 1);
          const id = count === 0 ? baseSlug : `${baseSlug}-${count}`;

          outline.push({ text: text.trim(), level, id, line });
        }
        return outline;
      }

      /**
       * Get the heading outline for a given commit index.
       * Caches results; uses docTextAt to reconstruct the spec.
       *
       * Returns: Promise<[{ text: string, level: number, id: string }]>
       */
      async function getOutline(commitIdx) {
        const idx = Number(commitIdx || 0);
        if (OUTLINE_CACHE.has(idx)) return OUTLINE_CACHE.get(idx);

        const text = await docTextAt(idx);
        const outline = extractOutline(text);
        OUTLINE_CACHE.set(idx, outline);
        return outline;
      }

      /**
       * Resolve offsetTop values for outline entries by querying the DOM.
       * Call this AFTER rendering markdown to get accurate positions.
       * Returns: [{ text, level, id, offsetTop }]
       */
      function resolveOutlineOffsets(outline, containerEl) {
        if (!containerEl || !outline?.length) return outline || [];
        return outline.map((entry) => {
          const el = containerEl.querySelector(`#${CSS.escape(entry.id)}`);
          return {
            ...entry,
            offsetTop: el ? el.offsetTop : null,
          };
        });
      }

      // -----------------------------
      // Per-Heading Change Metrics (bd-24q.8.2)
      // -----------------------------

      const HEADING_METRICS_CACHE = new Map(); // idx -> Map<headingId, metrics>

      /**
       * Build a lookup: 1-based line number -> heading ID.
       * Lines before the first heading map to "__preamble__".
       */
      function buildLineToHeadingMap(totalLines, outline) {
        const map = new Array(totalLines + 1);
        map[0] = "__preamble__";

        // Determine the heading boundary for each line by walking
        // through the outline's sorted line numbers.
        let ptr = 0;
        let currentId = "__preamble__";
        for (let ln = 1; ln <= totalLines; ln++) {
          while (ptr < outline.length && outline[ptr].line != null && outline[ptr].line <= ln) {
            currentId = outline[ptr].id;
            ptr++;
          }
          map[ln] = currentId;
        }
        return map;
      }

      /**
       * Attribute each added/removed line in a unified diff to the
       * nearest preceding heading in the new (current) snapshot.
       *
       * Returns: Map<headingId, { addLines, delLines, tokensAdded, tokensDeleted }>
       */
      function attributeHunksToHeadings(patch, lineToHeading) {
        const hunks = parseUnifiedHunks(patch);
        const metrics = new Map();

        const ensure = (id) => {
          if (!metrics.has(id)) {
            metrics.set(id, { addLines: 0, delLines: 0, tokensAdded: 0, tokensDeleted: 0 });
          }
          return metrics.get(id);
        };

        for (const hunk of hunks) {
          let newLineNum = hunk.newStart;
          for (const hl of hunk.lines) {
            if (!hl) continue;
            const p = hl[0];
            const content = hl.slice(1);
            const headingId = lineToHeading[newLineNum] || "__preamble__";

            if (p === "+") {
              const m = ensure(headingId);
              m.addLines++;
              m.tokensAdded += countRoughTokens(content);
              newLineNum++;
            } else if (p === "-") {
              const m = ensure(headingId);
              m.delLines++;
              m.tokensDeleted += countRoughTokens(content);
              // Deleted lines don't advance the new-file cursor.
            } else if (p === " ") {
              newLineNum++;
            }
          }
        }
        return metrics;
      }

      /**
       * Get per-heading change metrics for a commit.
       *
       * Returns: Promise<Map<headingId, { addLines, delLines, tokensAdded, tokensDeleted }>>
       */
      async function getHeadingMetrics(commitIdx) {
        const idx = Number(commitIdx || 0);
        if (HEADING_METRICS_CACHE.has(idx)) return HEADING_METRICS_CACHE.get(idx);

        const outline = await getOutline(idx);
        const text = await docTextAt(idx);
        const patch = patchForIdx(idx);

        if (!patch || !outline.length) {
          const empty = new Map();
          HEADING_METRICS_CACHE.set(idx, empty);
          return empty;
        }

        const totalLines = text.split("\n").length;
        const lineToHeading = buildLineToHeadingMap(totalLines, outline);
        const metrics = attributeHunksToHeadings(patch, lineToHeading);
        HEADING_METRICS_CACHE.set(idx, metrics);
        return metrics;
      }

      /**
       * Render the mini-map outline panel for the current commit.
       *
       * Features (bd-24q.2.2):
       * - Heading hierarchy with change-heat indicators
       * - Follow-along: tracks visible heading while scrolling docRendered
       * - Search/filter headings by text
       * - Keyboard navigation (arrow keys, Enter to jump)
       */
      let _miniMapOutline = [];
      let _miniMapScrollCleanup = null;

      async function updateMiniMap() {
        const mm = document.getElementById("miniMap");
        if (!mm || mm.classList.contains("hidden")) return;

        const itemsContainer = document.getElementById("miniMapItems") || mm;
        const outline = await getOutline(DOC.idx);
        _miniMapOutline = outline;

        if (!outline.length) {
          itemsContainer.innerHTML = '<div class="text-xs text-slate-400 italic p-2">No headings</div>';
          return;
        }

        // In A/B compare mode, compute metrics between A and B; otherwise parent→current.
        let headingMetrics = null;
        if (DOC.compareMode && DOC.compareToIdx > 0) {
          try { headingMetrics = await getHeadingMetrics(DOC.compareToIdx); } catch { /* best-effort */ }
        } else if (DOC.idx > 0) {
          try { headingMetrics = await getHeadingMetrics(DOC.idx); } catch { /* best-effort */ }
        }

        // Bucket accent: use current commit's primary bucket color.
        const commitForAccent = DOC.compareMode
          ? ALL_COMMITS[DOC.compareToIdx]
          : ALL_COMMITS[DOC.idx];
        const accentColor = commitForAccent ? bucketById(commitForAccent.primary)?.color : null;

        const docEl = document.getElementById("docRendered");
        const searchInput = document.getElementById("miniMapSearch");
        const filterText = (searchInput?.value || "").toLowerCase().trim();

        const filtered = filterText
          ? outline.filter((e) => e.text.toLowerCase().includes(filterText))
          : outline;

        const items = filtered.map((entry, i) => {
          const indent = Math.max(0, entry.level - 1) * 12;
          const hasChange = headingMetrics?.has(entry.id);
          const m = hasChange ? headingMetrics.get(entry.id) : null;
          const changeTotal = m ? m.addLines + m.delLines : 0;
          const tokenTotal = m ? m.tokensAdded + m.tokensDeleted : 0;
          // Intensity: use token count for size, line count for color.
          const dotSize = tokenTotal > 50 ? 10 : tokenTotal > 10 ? 8 : 6;
          const dotColor = changeTotal > 20 ? '#dc2626' : changeTotal > 5 ? '#d97706' : '#059669';
          const borderStyle = accentColor && hasChange && changeTotal > 0
            ? `; border: 1.5px solid ${accentColor}` : "";
          const marker = hasChange && changeTotal > 0
            ? `<span class="inline-block rounded-full mr-1.5 shrink-0" style="width: ${dotSize}px; height: ${dotSize}px; background: ${dotColor}${borderStyle}" title="+${m.addLines} -${m.delLines} lines, ~${tokenTotal} tokens"></span>`
            : "";
          return `<a href="#${escapeHtml(entry.id)}" role="treeitem" tabindex="${i === 0 ? '0' : '-1'}" class="minimap-item flex items-center py-0.5 text-[11px] leading-tight text-slate-700 hover:text-slate-900 hover:bg-slate-100/60 rounded px-1 cursor-pointer no-underline transition-colors" style="padding-left: ${indent}px" data-heading-id="${escapeHtml(entry.id)}" title="${escapeHtml(entry.text)}">${marker}<span class="truncate">${escapeHtml(entry.text)}</span></a>`;
        });
        itemsContainer.innerHTML = items.join("");

        // Click handlers: smooth-scroll to heading in doc.
        itemsContainer.querySelectorAll(".minimap-item").forEach((a) => {
          a.addEventListener("click", (e) => {
            e.preventDefault();
            const hid = a.dataset.headingId;
            if (!hid || !docEl) return;
            const target = docEl.querySelector(`#${CSS.escape(hid)}`);
            if (target) target.scrollIntoView({ behavior: "smooth", block: "start" });
            miniMapSetActive(a);
          });
        });

        // Keyboard navigation on items container.
        itemsContainer.onkeydown = (e) => {
          const links = [...itemsContainer.querySelectorAll(".minimap-item")];
          if (!links.length) return;
          const cur = itemsContainer.querySelector(".minimap-item[tabindex='0']");
          const idx = cur ? links.indexOf(cur) : 0;
          if (e.key === "ArrowDown" || e.key === "ArrowUp") {
            e.preventDefault();
            const next = e.key === "ArrowDown" ? Math.min(idx + 1, links.length - 1) : Math.max(idx - 1, 0);
            if (cur) cur.tabIndex = -1;
            links[next].tabIndex = 0;
            links[next].focus();
          } else if (e.key === "Enter") {
            e.preventDefault();
            if (cur) cur.click();
          } else if (e.key === "Home") {
            e.preventDefault();
            if (cur) cur.tabIndex = -1;
            links[0].tabIndex = 0;
            links[0].focus();
          } else if (e.key === "End") {
            e.preventDefault();
            if (cur) cur.tabIndex = -1;
            links[links.length - 1].tabIndex = 0;
            links[links.length - 1].focus();
          }
        };

        // Follow-along: highlight current heading while scrolling docRendered.
        setupMiniMapFollowAlong(docEl, itemsContainer, outline);
      }

      /** Highlight a mini-map item as active and scroll it into view. */
      function miniMapSetActive(el) {
        const container = el?.closest("#miniMapItems") || el?.closest("#miniMap");
        if (!container) return;
        container.querySelectorAll(".minimap-item.minimap-active").forEach((a) => {
          a.classList.remove("minimap-active", "bg-slate-200/80", "font-semibold", "text-slate-900");
        });
        if (el) {
          el.classList.add("minimap-active", "bg-slate-200/80", "font-semibold", "text-slate-900");
          el.scrollIntoView({ block: "nearest", behavior: "smooth" });
        }
      }

      /** Attach a scroll listener on docRendered to track the current heading. */
      function setupMiniMapFollowAlong(docEl, itemsContainer, outline) {
        if (_miniMapScrollCleanup) { _miniMapScrollCleanup(); _miniMapScrollCleanup = null; }
        if (!docEl || !outline.length) return;

        // Cache heading offsets to avoid O(N) DOM queries per scroll.
        const headingOffsets = [];
        for (const entry of outline) {
          const el = docEl.querySelector(`#${CSS.escape(entry.id)}`);
          headingOffsets.push({ id: entry.id, top: el ? el.offsetTop : 0 });
        }

        let rafId = 0;
        const onScroll = () => {
          cancelAnimationFrame(rafId);
          rafId = requestAnimationFrame(() => {
            const viewportMid = docEl.scrollTop + docEl.clientHeight * 0.25;
            let activeId = headingOffsets[0]?.id;
            for (const h of headingOffsets) {
              if (h.top <= viewportMid) activeId = h.id;
            }
            const activeLink = activeId ? itemsContainer.querySelector(`[data-heading-id="${CSS.escape(activeId)}"]`) : null;
            if (activeLink && !activeLink.classList.contains("minimap-active")) miniMapSetActive(activeLink);
          });
        };
        docEl.addEventListener("scroll", onScroll, { passive: true });
        onScroll();
        _miniMapScrollCleanup = () => { docEl.removeEventListener("scroll", onScroll); cancelAnimationFrame(rafId); };
      }

      // ---- Section Summary Panel (bd-24q.8.3) ----
      const SECTION_SORT = { key: "impact", asc: false };
      function sectionSparkline(headingId, currentIdx) {
        const range = 5, startIdx = Math.max(1, currentIdx - range + 1), vals = [];
        for (let i = startIdx; i <= currentIdx; i++) {
          const cached = HEADING_METRICS_CACHE.get(i);
          if (cached && cached.has(headingId)) { const m = cached.get(headingId); vals.push(m.addLines + m.delLines); }
          else vals.push(0);
        }
        const maxVal = Math.max(1, ...vals);
        return vals.map(v => {
          const h = Math.max(1, Math.round((v / maxVal) * 14));
          const color = v === 0 ? "#cbd5e1" : v > 20 ? "#dc2626" : v > 5 ? "#d97706" : "#059669";
          return `<span class="sparkline-bar" style="width:3px;height:${h}px;background:${color};"><\/span>`;
        }).join("");
      }
      async function updateSectionSummary() {
        const view = document.getElementById("docSectionsView");
        if (!view || view.classList.contains("hidden")) return;
        const tbody = document.getElementById("sectionTableBody");
        const empty = document.getElementById("sectionEmpty");
        const wrap = document.getElementById("sectionTableWrap");
        if (!tbody) return;
        const outline = await getOutline(DOC.idx);
        const metrics = DOC.idx > 0 ? await getHeadingMetrics(DOC.idx) : new Map();
        const filter = (document.getElementById("sectionFilter")?.value || "").trim().toLowerCase();
        const rows = [];
        for (const entry of outline) {
          if (filter && !entry.text.toLowerCase().includes(filter)) continue;
          const m = metrics.get(entry.id) || { addLines: 0, delLines: 0, tokensAdded: 0, tokensDeleted: 0 };
          rows.push({ id: entry.id, text: entry.text, level: entry.level, add: m.addLines, del: m.delLines, tokens: m.tokensAdded + m.tokensDeleted, impact: m.addLines + m.delLines });
        }
        const sortKey = SECTION_SORT.key, dir = SECTION_SORT.asc ? 1 : -1;
        rows.sort((a, b) => sortKey === "name" ? dir * a.text.localeCompare(b.text) : dir * ((a[sortKey] || 0) - (b[sortKey] || 0)));
        if (!rows.length) { tbody.innerHTML = ""; if (wrap) wrap.classList.add("hidden"); if (empty) empty.classList.remove("hidden"); return; }
        if (wrap) wrap.classList.remove("hidden");
        if (empty) empty.classList.add("hidden");
        const maxImpact = Math.max(1, ...rows.map(r => r.impact));
        tbody.innerHTML = rows.map(r => {
          const indent = Math.max(0, r.level - 1) * 10;
          const barW = Math.round((r.impact / maxImpact) * 100);
          const spark = sectionSparkline(r.id, DOC.idx);
          const ic = r.impact === 0 ? "text-slate-400" : r.impact > 20 ? "text-red-600" : r.impact > 5 ? "text-amber-600" : "text-emerald-600";
          return `<tr class="border-b border-slate-100 hover:bg-slate-50/60 cursor-pointer transition-colors" data-heading-id="${escapeHtml(r.id)}"><td class="px-3 py-2 text-left"><div class="flex items-center gap-1.5" style="padding-left:${indent}px"><span class="truncate max-w-[240px]" title="${escapeHtml(r.text)}">${escapeHtml(r.text)}<\/span><span class="flex items-end gap-px ml-1">${spark}<\/span><\/div><\/td><td class="px-3 py-2 text-right mono text-emerald-600">${r.add > 0 ? "+" + r.add : ""}<\/td><td class="px-3 py-2 text-right mono text-red-500">${r.del > 0 ? "-" + r.del : ""}<\/td><td class="px-3 py-2 text-right mono text-slate-600">${r.tokens || ""}<\/td><td class="px-3 py-2 text-right"><div class="flex items-center justify-end gap-1.5"><span class="mono ${ic} font-semibold">${r.impact || ""}<\/span><span class="inline-block h-1.5 rounded-full bg-slate-200" style="width:40px"><span class="inline-block h-1.5 rounded-full" style="width:${barW}%;background:${r.impact > 20 ? '#dc2626' : r.impact > 5 ? '#d97706' : '#059669'}"><\/span><\/span><\/div><\/td><\/tr>`;
        }).join("");
        tbody.querySelectorAll("tr[data-heading-id]").forEach(tr => {
          tr.addEventListener("click", () => {
            const hid = tr.dataset.headingId;
            if (!hid) return;
            if (DOC.tab !== "spec") setDocTab("spec");
            requestAnimationFrame(() => {
              const docEl = document.getElementById("docRendered");
              if (!docEl) return;
              const target = docEl.querySelector(`#${CSS.escape(hid)}`);
              if (target) { target.scrollIntoView({ behavior: "smooth", block: "start" }); target.classList.remove("section-highlight"); void target.offsetWidth; target.classList.add("section-highlight"); }
            });
          });
        });
      }
      document.addEventListener("click", e => {
        const th = e.target.closest("#sectionTable th[data-sort]");
        if (!th) return;
        const key = th.dataset.sort;
        if (SECTION_SORT.key === key) SECTION_SORT.asc = !SECTION_SORT.asc;
        else { SECTION_SORT.key = key; SECTION_SORT.asc = key === "name"; }
        th.closest("thead")?.querySelectorAll("th[data-sort]").forEach(h => {
          const arrow = SECTION_SORT.key === h.dataset.sort ? (SECTION_SORT.asc ? " \u25B2" : " \u25BC") : "";
          h.textContent = h.textContent.replace(/\s*[\u25B2\u25BC]$/, "") + arrow;
        });
        void updateSectionSummary();
      });
      document.addEventListener("input", e => {
        if (e.target?.id === "sectionFilter") { clearTimeout(e.target._debounce); e.target._debounce = setTimeout(() => void updateSectionSummary(), 150); }
      });
      // Mobile section sheet open/close.
      function openSectionSheet() {
        const sheet = document.getElementById("sectionSheet");
        const overlay = document.getElementById("sectionSheetOverlay");
        if (sheet) { sheet.classList.remove("hidden"); requestAnimationFrame(() => sheet.classList.add("open")); }
        if (overlay) overlay.classList.remove("hidden");
        void updateSectionSheetList();
      }
      function closeSectionSheet() {
        const sheet = document.getElementById("sectionSheet");
        const overlay = document.getElementById("sectionSheetOverlay");
        if (sheet) { sheet.classList.remove("open"); setTimeout(() => sheet.classList.add("hidden"), 220); }
        if (overlay) overlay.classList.add("hidden");
      }
      async function updateSectionSheetList() {
        const list = document.getElementById("sectionSheetList");
        if (!list) return;
        const outline = await getOutline(DOC.idx);
        const metrics = DOC.idx > 0 ? await getHeadingMetrics(DOC.idx) : new Map();
        const filter = (document.getElementById("sectionFilterMobile")?.value || "").trim().toLowerCase();
        const items = [];
        for (const entry of outline) {
          if (filter && !entry.text.toLowerCase().includes(filter)) continue;
          const m = metrics.get(entry.id) || { addLines: 0, delLines: 0, tokensAdded: 0, tokensDeleted: 0 };
          const total = m.addLines + m.delLines;
          const color = total === 0 ? "bg-slate-100 text-slate-400" : total > 20 ? "bg-red-50 text-red-700" : total > 5 ? "bg-amber-50 text-amber-700" : "bg-emerald-50 text-emerald-700";
          const indent = Math.max(0, entry.level - 1) * 12;
          items.push(`<button class="w-full text-left rounded-2xl px-4 py-3 ${color} hover:brightness-95 transition-colors" data-heading-id="${escapeHtml(entry.id)}" style="padding-left:${indent + 16}px"><div class="text-sm font-semibold">${escapeHtml(entry.text)}<\/div><div class="mt-0.5 text-[11px]">+${m.addLines} -${m.delLines} lines \u00b7 ${m.tokensAdded + m.tokensDeleted} tokens<\/div><\/button>`);
        }
        list.innerHTML = items.length ? items.join("") : '<div class="text-xs text-slate-400 italic p-3">No section changes<\/div>';
        list.querySelectorAll("button[data-heading-id]").forEach(btn => {
          btn.addEventListener("click", () => {
            closeSectionSheet();
            setDocTab("spec");
            setTimeout(() => {
              const docEl = document.getElementById("docRendered");
              if (!docEl) return;
              const target = docEl.querySelector(`#${CSS.escape(btn.dataset.headingId)}`);
              if (target) { target.scrollIntoView({ behavior: "smooth", block: "start" }); target.classList.remove("section-highlight"); void target.offsetWidth; target.classList.add("section-highlight"); }
            }, 300);
          });
        });
      }
      document.getElementById("btnOpenSectionSheet")?.addEventListener("click", openSectionSheet);
      document.getElementById("btnCloseSectionSheet")?.addEventListener("click", closeSectionSheet);
      document.getElementById("sectionSheetOverlay")?.addEventListener("click", closeSectionSheet);
      document.getElementById("sectionFilterMobile")?.addEventListener("input", e => {
        clearTimeout(e.target._debounce);
        e.target._debounce = setTimeout(() => void updateSectionSheetList(), 150);
      });

      // ---- End Section Summary Panel ----

      // ---- Section Summary Unit Tests (bd-24q.8.4) ----
      // Run via console: window.__runSectionTests()
      window.__runSectionTests = function () {
        const results = [];
        const assert = (cond, msg, ctx) => {
          if (!cond) { results.push({ pass: false, msg, ctx }); console.error("FAIL:", msg, ctx || ""); }
          else results.push({ pass: true, msg });
        };

        // --- extractOutline tests ---
        (function testOutline_basic() {
          const md = "# Title\n\nSome text\n\n## Section A\n\nBody\n\n## Section B\n\nMore body\n\n### Subsection B.1\n";
          const o = extractOutline(md);
          assert(o.length === 4, "outline: 4 headings", { got: o.length });
          assert(o[0].text === "Title" && o[0].level === 1, "outline: h1 Title");
          assert(o[1].text === "Section A" && o[1].level === 2, "outline: h2 Section A");
          assert(o[2].text === "Section B" && o[2].level === 2, "outline: h2 Section B");
          assert(o[3].text === "Subsection B.1" && o[3].level === 3, "outline: h3 Subsection B.1");
        })();

        (function testOutline_duplicateSlugs() {
          const md = "## Intro\n\nText\n\n## Intro\n\nText\n\n## Intro\n";
          const o = extractOutline(md);
          assert(o.length === 3, "duplicate: 3 headings");
          assert(o[0].id === "intro", "duplicate: first is 'intro'", { got: o[0].id });
          assert(o[1].id === "intro-1", "duplicate: second is 'intro-1'", { got: o[1].id });
          assert(o[2].id === "intro-2", "duplicate: third is 'intro-2'", { got: o[2].id });
        })();

        (function testOutline_weirdPunctuation() {
          const md = "# Hello, World! (v2.0)\n\n## 日本語の見出し\n\n## `code_heading`\n";
          const o = extractOutline(md);
          assert(o.length === 3, "punctuation: 3 headings");
          assert(o[0].id.length > 0, "punctuation: non-empty slug for punctuated heading");
          assert(o[1].id.length > 0, "punctuation: non-empty slug for Unicode heading");
        })();

        (function testOutline_emptyHeading() {
          const md = "## \n\n## Real\n";
          const o = extractOutline(md);
          assert(o.length === 2, "empty: 2 headings (one empty)");
          assert(o[0].id === "heading", "empty: empty heading gets fallback slug 'heading'", { got: o[0].id });
        })();

        (function testOutline_deepNesting() {
          const md = "# H1\n## H2\n### H3\n#### H4\n##### H5\n###### H6\n";
          const o = extractOutline(md);
          assert(o.length === 6, "deep: 6 levels");
          assert(o[5].level === 6, "deep: h6 detected", { got: o[5].level });
        })();

        // --- slugifyHeading tests ---
        (function testSlug_basic() {
          assert(slugifyHeading("Hello World") === "hello-world", "slug: basic");
          assert(slugifyHeading("") === "heading", "slug: empty string fallback");
          assert(slugifyHeading("  ") === "heading", "slug: whitespace fallback");
          assert(slugifyHeading("CamelCase") === "camelcase", "slug: lowercased");
          assert(slugifyHeading("a--b") === "a-b", "slug: collapsed hyphens");
        })();

        // --- buildLineToHeadingMap tests ---
        (function testLineMap_basic() {
          const outline = [
            { text: "A", level: 1, id: "a", line: 1 },
            { text: "B", level: 2, id: "b", line: 5 },
            { text: "C", level: 2, id: "c", line: 10 },
          ];
          const map = buildLineToHeadingMap(15, outline);
          assert(map[1] === "a", "linemap: line 1 -> a", { got: map[1] });
          assert(map[4] === "a", "linemap: line 4 -> a (before B)", { got: map[4] });
          assert(map[5] === "b", "linemap: line 5 -> b", { got: map[5] });
          assert(map[9] === "b", "linemap: line 9 -> b", { got: map[9] });
          assert(map[10] === "c", "linemap: line 10 -> c", { got: map[10] });
          assert(map[15] === "c", "linemap: line 15 -> c", { got: map[15] });
        })();

        (function testLineMap_preamble() {
          const outline = [{ text: "A", level: 1, id: "a", line: 5 }];
          const map = buildLineToHeadingMap(10, outline);
          assert(map[1] === "__preamble__", "preamble: line 1 before any heading", { got: map[1] });
          assert(map[4] === "__preamble__", "preamble: line 4 before first heading", { got: map[4] });
          assert(map[5] === "a", "preamble: line 5 at first heading", { got: map[5] });
        })();

        // --- attributeHunksToHeadings tests ---
        (function testAttribution_basic() {
          const patch = `--- a/doc.md
+++ b/doc.md
@@ -3,2 +3,3 @@
 unchanged
+new line in section A
+another new line
@@ -8,1 +9,2 @@
 unchanged
+added in section B`;
          const lineToHeading = new Array(12);
          for (let i = 0; i <= 11; i++) lineToHeading[i] = i < 7 ? "a" : "b";
          const m = attributeHunksToHeadings(patch, lineToHeading);
          assert(m.has("a"), "attr: section a has metrics");
          assert(m.get("a").addLines === 2, "attr: section a +2 lines", { got: m.get("a")?.addLines });
          assert(m.has("b"), "attr: section b has metrics");
          assert(m.get("b").addLines === 1, "attr: section b +1 line", { got: m.get("b")?.addLines });
        })();

        (function testAttribution_deletions() {
          const patch = `--- a/doc.md
+++ b/doc.md
@@ -3,3 +3,1 @@
 unchanged
-removed line 1
-removed line 2`;
          const lineToHeading = new Array(8);
          for (let i = 0; i <= 7; i++) lineToHeading[i] = "section";
          const m = attributeHunksToHeadings(patch, lineToHeading);
          assert(m.has("section"), "del: section has metrics");
          assert(m.get("section").delLines === 2, "del: 2 lines deleted", { got: m.get("section")?.delLines });
          assert(m.get("section").addLines === 0, "del: 0 lines added", { got: m.get("section")?.addLines });
        })();

        (function testAttribution_empty_patch() {
          const m = attributeHunksToHeadings("", []);
          assert(m.size === 0, "empty: no metrics for empty patch");
        })();

        // --- countRoughTokens tests ---
        (function testTokens() {
          assert(countRoughTokens("hello world") === 2, "tokens: 2 words");
          assert(countRoughTokens("a + b = c") === 6, "tokens: operators count", { got: countRoughTokens("a + b = c") });
          assert(countRoughTokens("") === 0, "tokens: empty string");
          assert(countRoughTokens("single") === 1, "tokens: single word");
        })();

        // --- sectionSparkline tests ---
        (function testSparkline() {
          const html = sectionSparkline("nonexistent", 3);
          assert(html.includes("sparkline-bar"), "sparkline: contains bar spans");
          assert(!html.includes("undefined"), "sparkline: no undefined in output");
        })();

        // --- Performance sanity ---
        (function testPerformance_attribution() {
          // Generate a large synthetic diff.
          const lines = [];
          lines.push("--- a/big.md", "+++ b/big.md", "@@ -1,500 +1,600 @@");
          for (let i = 0; i < 500; i++) lines.push(" unchanged line " + i);
          for (let i = 0; i < 100; i++) lines.push("+added line " + i);
          const bigPatch = lines.join("\n");
          const bigMap = new Array(700);
          for (let i = 0; i < 700; i++) bigMap[i] = "section-" + Math.floor(i / 100);
          const t0 = performance.now();
          attributeHunksToHeadings(bigPatch, bigMap);
          const elapsed = performance.now() - t0;
          assert(elapsed < 100, "perf: 600-line attribution < 100ms", { elapsed: elapsed.toFixed(1) + "ms" });
        })();

        // Summary
        const passed = results.filter(r => r.pass).length;
        const failed = results.filter(r => !r.pass).length;
        const summary = `Section Summary Tests: ${passed} passed, ${failed} failed, ${results.length} total`;
        if (failed > 0) {
          console.warn(summary);
          results.filter(r => !r.pass).forEach(r => console.error("  FAIL:", r.msg, r.ctx || ""));
        } else {
          console.log("%c" + summary, "color: green; font-weight: bold");
        }
        return { passed, failed, total: results.length, results };
      };
      // ---- End Section Summary Unit Tests ----

      // ---- Section Summary E2E Tests (bd-24q.8.5) ----
      // Run via console: window.__runSectionE2ETests()
      // Requires dataset to be loaded and at least one commit with changes.
      window.__runSectionE2ETests = async function () {
        const results = [];
        const log = (msg, ctx) => console.log("  [E2E]", msg, ctx || "");
        const assert = (cond, msg, ctx) => {
          if (!cond) { results.push({ pass: false, msg, ctx }); console.error("  FAIL:", msg, ctx || ""); }
          else { results.push({ pass: true, msg }); log("PASS: " + msg); }
        };

        if (!DATASET.loaded || !ALL_COMMITS.length) {
          console.error("E2E: Dataset not loaded. Cannot run tests.");
          return { passed: 0, failed: 1, total: 1, results: [{ pass: false, msg: "Dataset not loaded" }] };
        }

        // Find a commit with actual changes for meaningful testing.
        let testIdx = -1;
        for (let i = 1; i < ALL_COMMITS.length; i++) {
          if (ALL_COMMITS[i].impact > 10) { testIdx = i; break; }
        }
        if (testIdx < 0) testIdx = Math.min(1, ALL_COMMITS.length - 1);
        log("Using commit idx=" + testIdx + " (" + ALL_COMMITS[testIdx].short + ")");

        // E2E 1: Switch to sections tab and verify table renders.
        await (async function testE2E_sectionsTabRenders() {
          selectCommitIdx(testIdx);
          setDocTab("sections");
          // Give async updateSectionSummary time to complete.
          await new Promise(r => setTimeout(r, 500));
          const view = document.getElementById("docSectionsView");
          assert(view && !view.classList.contains("hidden"), "e2e: sections view is visible");
          const tbody = document.getElementById("sectionTableBody");
          const rowCount = tbody ? tbody.querySelectorAll("tr").length : 0;
          log("Table has " + rowCount + " rows");
          assert(rowCount > 0, "e2e: section table has rows", { rowCount });
        })();

        // E2E 2: Sort by impact and verify order changes.
        await (async function testE2E_sortByImpact() {
          const th = document.querySelector('#sectionTable th[data-sort="impact"]');
          if (th) {
            th.click();
            await new Promise(r => setTimeout(r, 300));
            const rows = document.querySelectorAll("#sectionTableBody tr");
            if (rows.length >= 2) {
              const first = rows[0].querySelector("td:last-child .mono")?.textContent?.trim() || "0";
              const last = rows[rows.length - 1].querySelector("td:last-child .mono")?.textContent?.trim() || "0";
              log("First row impact: " + first + ", last row: " + last);
              assert(true, "e2e: sort by impact executed without error");
            } else {
              assert(true, "e2e: sort by impact (not enough rows to verify order)");
            }
          } else {
            assert(false, "e2e: impact sort header not found");
          }
        })();

        // E2E 3: Filter headings.
        await (async function testE2E_filterHeadings() {
          const input = document.getElementById("sectionFilter");
          if (input) {
            const tbody = document.getElementById("sectionTableBody");
            const beforeCount = tbody ? tbody.querySelectorAll("tr").length : 0;
            input.value = "xyznonexistent";
            input.dispatchEvent(new Event("input", { bubbles: true }));
            await new Promise(r => setTimeout(r, 300));
            const afterCount = tbody ? tbody.querySelectorAll("tr").length : 0;
            log("Before filter: " + beforeCount + " rows, after: " + afterCount);
            assert(afterCount <= beforeCount, "e2e: filter reduces or maintains row count");
            // Reset filter.
            input.value = "";
            input.dispatchEvent(new Event("input", { bubbles: true }));
            await new Promise(r => setTimeout(r, 300));
          } else {
            assert(false, "e2e: filter input not found");
          }
        })();

        // E2E 4: Click row jumps to spec tab with heading.
        await (async function testE2E_clickRowJumps() {
          setDocTab("sections");
          await new Promise(r => setTimeout(r, 500));
          const firstRow = document.querySelector("#sectionTableBody tr[data-heading-id]");
          if (firstRow) {
            const hid = firstRow.dataset.headingId;
            log("Clicking row for heading: " + hid);
            firstRow.click();
            await new Promise(r => setTimeout(r, 600));
            assert(DOC.tab === "spec", "e2e: after click, tab switched to spec", { got: DOC.tab });
            const docEl = document.getElementById("docRendered");
            const target = docEl ? docEl.querySelector("#" + CSS.escape(hid)) : null;
            assert(target !== null, "e2e: heading element exists in rendered doc", { hid });
            if (target) {
              const hasHighlight = target.classList.contains("section-highlight");
              log("Heading has highlight class: " + hasHighlight);
              assert(true, "e2e: heading element found (highlight animation triggered)");
            }
          } else {
            assert(false, "e2e: no rows in section table to click");
          }
        })();

        // E2E 5: Mobile sheet open/close.
        await (async function testE2E_mobileSheet() {
          const sheet = document.getElementById("sectionSheet");
          const overlay = document.getElementById("sectionSheetOverlay");
          if (sheet && overlay) {
            // Open.
            if (typeof openSectionSheet === "function") {
              openSectionSheet();
              await new Promise(r => setTimeout(r, 300));
              assert(!sheet.classList.contains("hidden"), "e2e: mobile sheet opened");
              const list = document.getElementById("sectionSheetList");
              const itemCount = list ? list.querySelectorAll("button[data-heading-id]").length : 0;
              log("Mobile sheet has " + itemCount + " items");
              assert(itemCount > 0, "e2e: mobile sheet has items", { itemCount });
              // Close.
              closeSectionSheet();
              await new Promise(r => setTimeout(r, 300));
              assert(overlay.classList.contains("hidden"), "e2e: mobile sheet overlay hidden after close");
            } else {
              assert(false, "e2e: openSectionSheet function not found");
            }
          } else {
            assert(false, "e2e: mobile sheet elements not found");
          }
        })();

        // E2E 6: Mobile sheet tap navigates.
        await (async function testE2E_mobileSheetTap() {
          if (typeof openSectionSheet === "function") {
            openSectionSheet();
            await new Promise(r => setTimeout(r, 300));
            const firstBtn = document.querySelector("#sectionSheetList button[data-heading-id]");
            if (firstBtn) {
              const hid = firstBtn.dataset.headingId;
              log("Mobile: tapping heading " + hid);
              firstBtn.click();
              await new Promise(r => setTimeout(r, 600));
              assert(DOC.tab === "spec", "e2e: mobile tap switched to spec", { got: DOC.tab });
              const sheet = document.getElementById("sectionSheet");
              // Sheet may still be animating closed.
              log("Sheet hidden: " + sheet?.classList.contains("hidden"));
              assert(true, "e2e: mobile tap navigation executed");
            } else {
              assert(false, "e2e: no mobile sheet items to tap");
            }
          }
        })();

        // E2E 7: No dock overlap check.
        await (async function testE2E_noDockOverlap() {
          const sheet = document.getElementById("sectionSheet");
          const dock = document.getElementById("dock");
          if (sheet && dock) {
            if (typeof openSectionSheet === "function") {
              openSectionSheet();
              await new Promise(r => setTimeout(r, 300));
              const sheetRect = sheet.getBoundingClientRect();
              const dockRect = dock.getBoundingClientRect();
              // Sheet z-index (50) should be above dock z-index (30).
              const sheetZ = parseInt(getComputedStyle(sheet).zIndex) || 0;
              const dockZ = parseInt(getComputedStyle(dock).zIndex) || 0;
              log("Sheet z-index: " + sheetZ + ", dock z-index: " + dockZ);
              assert(sheetZ > dockZ, "e2e: sheet z-index above dock", { sheetZ, dockZ });
              closeSectionSheet();
              await new Promise(r => setTimeout(r, 300));
            }
          }
        })();

        // Summary.
        const passed = results.filter(r => r.pass).length;
        const failed = results.filter(r => !r.pass).length;
        const summary = `Section E2E Tests: ${passed} passed, ${failed} failed, ${results.length} total`;
        if (failed > 0) {
          console.warn(summary);
          results.filter(r => !r.pass).forEach(r => console.error("  FAIL:", r.msg, r.ctx || ""));
        } else {
          console.log("%c" + summary, "color: green; font-weight: bold");
        }
        return { passed, failed, total: results.length, results };
      };
      // ---- End Section Summary E2E Tests ----

      // ---- A/B Compare Unit Tests (bd-24q.1.5) ----
      window.__runABCompareTests = async function () {
        const R = [];
        const A = (c, m, x) => { R.push(c ? { pass: true, msg: m } : { pass: false, msg: m, ctx: x }); if (!c) console.error("FAIL:", m, x || ""); };
        A(quickMetricsFromPatch("").hunks === 0, "empty patch: 0 hunks");
        let m = quickMetricsFromPatch("--- a/f\n+++ b/f\n@@ -1,2 +1,3 @@\n l1\n+added\n l2\n");
        A(m.hunks === 1 && m.addLines === 1 && m.delLines === 0, "simple add", m);
        m = quickMetricsFromPatch("--- a/f\n+++ b/f\n@@ -1,3 +1,3 @@\n ctx\n-old\n+new\n ctx2\n");
        A(m.addLines === 1 && m.delLines === 1, "replace 1+1", m);
        m = quickMetricsFromPatch("--- a/f\n+++ b/f\n@@ -1 +1 @@\n-x\n+y\n@@ -5 +5 @@\n-a\n+b\n");
        A(m.hunks === 2, "multi-hunk", m);
        let h = parseUnifiedHunks("@@ -1,3 +1,4 @@\n l1\n+ins\n l2\n l3\n");
        A(h.length === 1 && h[0].oldStart === 1, "hunks basic");
        h = parseUnifiedHunks("@@ -5 +5 @@\n-old\n+new\n");
        A(h.length === 1 && h[0].oldCount === 1, "hunks no-count");
        let r = applyPatchLines(["a","b","c"], "@@ -2,2 +2,3 @@\n b\n+ins\n c\n");
        A(r.length === 4 && r[2] === "ins", "apply add");
        r = applyPatchLines(["a","b","c"], "@@ -1,3 +1,2 @@\n a\n-b\n c\n");
        A(r.length === 2 && r[1] === "c", "apply del");
        r = applyPatchLines(["a","b","c"], "@@ -2,1 +2,1 @@\n-b\n+B\n");
        A(r.length === 3 && r[1] === "B", "apply replace");
        if (DATASET.loaded && DATASET.data) {
          const base = String(DATASET.data.base_doc || "");
          A((await docTextAtLocal(0)) === base, "snapshot 0==base");
          if (ALL_COMMITS.length > 1) { DOC_CACHE.delete(1); DOC_CURSOR.idx=-1; DOC_CURSOR.lines=null; const a=await docTextAtLocal(1); DOC_CACHE.delete(1); DOC_CURSOR.idx=-1; DOC_CURSOR.lines=null; A(a===(await docTextAtLocal(1)), "snapshot 1 deterministic"); }
          if (ALL_COMMITS.length > 5) { DOC_CACHE.clear(); DOC_CURSOR.idx=-1; DOC_CURSOR.lines=null; const d=await docTextAtLocal(5); DOC_CACHE.clear(); DOC_CURSOR.idx=-1; DOC_CURSOR.lines=null; let ln=base.split("\n"); for(let i=1;i<=5;i++) ln=applyPatchLines(ln,patchForIdx(i)); A(d===ln.join("\n"), "snapshot 5 far-jump"); }
          if (typeof Diff!=="undefined" && ALL_COMMITS.length>3) {
            A((await generateABDiff(0,2))===(await generateABDiff(0,2)), "diff deterministic");
            const ms=quickMetricsFromPatch(await generateABDiff(1,1)); A(ms.addLines===0&&ms.delLines===0, "diff same: 0");
            const mAB=quickMetricsFromPatch(await generateABDiff(0,3)); const mBA=quickMetricsFromPatch(await generateABDiff(3,0));
            A(mAB.addLines===mBA.delLines&&mAB.delLines===mBA.addLines, "diff swap symmetry");
            A(quickMetricsFromPatch(await generateABDiff(0,ALL_COMMITS.length-1)).tokensDelta>0, "spec grew");
          }
        }
        (() => { const c=document.createElement("div"); c.innerHTML='<span id="abmLines"></span><span id="abmTokens"></span><span id="abmLev"></span><span id="abmHunks"></span><span id="abmBytes"></span>'; document.body.appendChild(c); renderABMetricChips({addLines:100,delLines:50,deltaLines:50,tokensChanged:300,tokensDelta:100,bytesChanged:5000,bytesDelta:2000,hunks:5,lev:42}); A(document.getElementById("abmLines").textContent.includes("+100"),"chip +100"); A(document.getElementById("abmLev").textContent.includes("42"),"chip lev"); document.body.removeChild(c); })();
        (() => { const c=document.createElement("div"); c.innerHTML='<span id="abmLines"></span><span id="abmTokens"></span><span id="abmLev"></span><span id="abmHunks"></span><span id="abmBytes"></span>'; document.body.appendChild(c); renderABMetricChips({addLines:0,delLines:0,deltaLines:0,tokensChanged:0,tokensDelta:0,bytesChanged:0,bytesDelta:0,hunks:0,lev:null}); A(document.getElementById("abmLev").textContent.includes("?"),"chip null lev"); document.body.removeChild(c); })();
        const p=R.filter(x=>x.pass).length, f=R.filter(x=>!x.pass).length;
        const s=`A/B Compare Tests: ${p} passed, ${f} failed, ${R.length} total`;
        if(f>0){console.warn(s);R.filter(x=>!x.pass).forEach(x=>console.error("  FAIL:",x.msg,x.ctx||""));}else console.log("%c"+s,"color:green;font-weight:bold");
        return{passed:p,failed:f,total:R.length,results:R};
      };
      // ---- End A/B Compare Unit Tests ----


      async function levenshteinForPatchLocal(patch) {
        const hunks = parseUnifiedHunks(patch);
        const enc = new TextEncoder();
        let sum = 0;
        for (const h of hunks) {
          const oldLines = [];
          const newLines = [];
          for (const hl of h.lines) {
            if (!hl) continue;
            const p = hl[0];
            const content = hl.slice(1);
            if (p === "-") oldLines.push(content);
            if (p === "+") newLines.push(content);
          }
          if (!oldLines.length && !newLines.length) continue;
          const a = enc.encode(oldLines.join("\n"));
          const b = enc.encode(newLines.join("\n"));
          if (a.length > 20000 || b.length > 20000) {
            // Worst-case guard: fallback to a cheap upper bound.
            sum += a.length + b.length;
          } else {
            sum += await levenshteinBytes(a, b);
          }
        }
        return sum;
      }

      async function levenshteinForPatch(patch, opts = {}) {
        if (WORKER_STATE.ready && !opts.localOnly) {
          try {
            const res = await workerRequest("levenshtein_patch", { patch }, { timeoutMs: 30000, signal: opts.signal });
            if (Number.isFinite(Number(res?.lev))) return Number(res.lev);
          } catch (e) {
            if (e?.name === "AbortError") throw e;
            console.error("Worker Levenshtein failed; using local fallback:", e);
          }
        }
        return levenshteinForPatchLocal(patch);
      }

      function selectCommitIdx(idx, opts = {}) {
        const max = Math.max(0, (ALL_COMMITS.length || 1) - 1);
        const next = clamp(Number(idx || 0), 0, max);
        DOC.idx = next;

        const slider = document.getElementById("dockSlider");
        if (slider) slider.value = String(next);

        syncDockAndDoc();
        if (!opts._fromPopstate) syncUrlToState();

        if (opts.scrollToCommitList) {
          const c = ALL_COMMITS[next];
          const el = c ? document.getElementById(`commit-${c.hash}`) : null;
          if (el) {
            el.scrollIntoView({ behavior: "smooth", block: "start" });
            el.open = true;
          }
        }
      }

      // -------------------------------------------------------
      // Playback: Core Scheduler + State Machine (bd-24q.7.1)
      // -------------------------------------------------------
      // States: "paused" | "playing" | "seeking"
      // Transitions:
      //   paused  -> playing  (play)
      //   playing -> paused   (pause / reach end without loop / visibility hidden)
      //   playing -> seeking  (manual scrub during playback)
      //   seeking -> playing  (scrub ends, resume)
      //   seeking -> paused   (scrub ends, was paused before)
      //   any     -> paused   (stop)

      const PLAYBACK = {
        state: "paused",         // "paused" | "playing" | "seeking"
        speed: 1,                // commits per second (0.25, 0.5, 1, 2, 4)
        loop: false,             // loop back to start at end
        _preSeekState: "paused", // state to restore after seeking
        _rafId: 0,               // requestAnimationFrame id
        _lastTickTime: 0,        // performance.now() of last tick
        _accumMs: 0,             // accumulated sub-tick milliseconds (drift correction)
        _visCleanup: null,       // cleanup for visibilitychange listener
      };

      const PLAYBACK_SPEEDS = [0.25, 0.5, 1, 2, 4];

      /** Pure: compute how many commits to advance given elapsed ms and speed. */
      function playbackTicksForElapsed(elapsedMs, speed, accumMs) {
        const totalMs = accumMs + elapsedMs;
        const intervalMs = 1000 / speed;
        const ticks = Math.floor(totalMs / intervalMs);
        const remainder = totalMs - ticks * intervalMs;
        return { ticks, remainder };
      }

      /** Pure: compute next commit index given current, ticks, max, and loop flag. */
      function playbackNextIndex(currentIdx, ticks, maxIdx, loop) {
        if (ticks <= 0) return { idx: currentIdx, stopped: false };
        let next = currentIdx + ticks;
        if (next > maxIdx) {
          if (loop) {
            next = next % (maxIdx + 1);
          } else {
            return { idx: maxIdx, stopped: true };
          }
        }
        return { idx: next, stopped: false };
      }

      /** Pure: validate and return a transition, or null if invalid. */
      function playbackTransition(currentState, action) {
        const transitions = {
          paused:  { play: "playing", seek: "seeking" },
          playing: { pause: "paused", seek: "seeking", stop: "paused", end: "paused" },
          seeking: { resume: null, stop: "paused" }, // resume target computed at runtime
        };
        const t = transitions[currentState];
        if (!t || !(action in t)) return null;
        if (currentState === "seeking" && action === "resume") {
          return PLAYBACK._preSeekState === "playing" ? "playing" : "paused";
        }
        return t[action];
      }

      function playbackPlay() {
        // Respect prefers-reduced-motion (a11y).
        if (window.matchMedia?.("(prefers-reduced-motion: reduce)")?.matches) return;
        const next = playbackTransition(PLAYBACK.state, "play");
        if (!next) return;
        PLAYBACK.state = next;
        PLAYBACK._lastTickTime = performance.now();
        PLAYBACK._accumMs = 0;
        _playbackInstallVisibility();
        _playbackSchedule();
      }

      function playbackPause() {
        const next = playbackTransition(PLAYBACK.state, "pause")
          || playbackTransition(PLAYBACK.state, "stop");
        if (!next) return;
        PLAYBACK.state = next;
        _playbackCancel();
      }

      function playbackStop() {
        PLAYBACK.state = "paused";
        PLAYBACK._accumMs = 0;
        _playbackCancel();
        _playbackRemoveVisibility();
        _syncPlaybackUI();
      }

      function playbackToggle() {
        if (PLAYBACK.state === "playing") playbackPause();
        else playbackPlay();
        _syncPlaybackUI();
      }

      function playbackSetSpeed(speed) {
        PLAYBACK.speed = Math.max(0.1, Number(speed) || 1);
        PLAYBACK._accumMs = 0;
        PLAYBACK._lastTickTime = performance.now();
      }

      function playbackSetLoop(loop) {
        PLAYBACK.loop = Boolean(loop);
      }

      /** Call when user manually scrubs the slider during playback. */
      function playbackOnManualScrub() {
        if (PLAYBACK.state === "paused") return; // no-op
        PLAYBACK._preSeekState = PLAYBACK.state;
        PLAYBACK.state = "seeking";
        _playbackCancel();
      }

      /** Call when manual scrub ends to optionally resume playback. */
      function playbackOnScrubEnd() {
        if (PLAYBACK.state !== "seeking") return;
        const next = playbackTransition("seeking", "resume");
        PLAYBACK.state = next || "paused";
        if (PLAYBACK.state === "playing") {
          PLAYBACK._lastTickTime = performance.now();
          PLAYBACK._accumMs = 0;
          _playbackSchedule();
        }
      }

      function _playbackSchedule() {
        if (PLAYBACK.state !== "playing") return;
        PLAYBACK._rafId = requestAnimationFrame(_playbackFrame);
      }

      function _playbackFrame(now) {
        if (PLAYBACK.state !== "playing") return;
        const elapsed = now - PLAYBACK._lastTickTime;
        PLAYBACK._lastTickTime = now;

        const { ticks, remainder } = playbackTicksForElapsed(elapsed, PLAYBACK.speed, PLAYBACK._accumMs);
        PLAYBACK._accumMs = remainder;

        if (ticks > 0) {
          const maxIdx = Math.max(0, (ALL_COMMITS.length || 1) - 1);
          const { idx, stopped } = playbackNextIndex(DOC.idx, ticks, maxIdx, PLAYBACK.loop);
          selectCommitIdx(idx);
          if (stopped) {
            playbackStop();
            return;
          }
        }
        _playbackSchedule();
      }

      function _playbackCancel() {
        if (PLAYBACK._rafId) {
          cancelAnimationFrame(PLAYBACK._rafId);
          PLAYBACK._rafId = 0;
        }
      }

      /** Update playback UI elements to match PLAYBACK state. */
      function _syncPlaybackUI() {
        const btn = document.getElementById("dockPlayPause");
        if (btn) {
          btn.innerHTML = PLAYBACK.state === "playing" ? "&#9646;&#9646;" : "&#9654;";
          btn.classList.toggle("bg-slate-900", PLAYBACK.state === "playing");
          btn.classList.toggle("text-white", PLAYBACK.state === "playing");
          btn.classList.toggle("bg-white/70", PLAYBACK.state !== "playing");
          btn.classList.toggle("text-slate-900", PLAYBACK.state !== "playing");
          btn.setAttribute("aria-label", PLAYBACK.state === "playing" ? "Pause playback" : "Play timeline");
        }
        const speedSel = document.getElementById("dockSpeed");
        if (speedSel) speedSel.value = String(PLAYBACK.speed);
        const loopBtn = document.getElementById("dockLoop");
        if (loopBtn) {
          loopBtn.classList.toggle("bg-slate-900", PLAYBACK.loop);
          loopBtn.classList.toggle("text-white", PLAYBACK.loop);
          loopBtn.classList.toggle("text-slate-500", !PLAYBACK.loop);
        }
      }

      function _playbackInstallVisibility() {
        if (PLAYBACK._visCleanup) return;
        const handler = () => {
          if (document.hidden && PLAYBACK.state === "playing") {
            PLAYBACK._preSeekState = "playing";
            PLAYBACK.state = "paused";
            _playbackCancel();
          } else if (!document.hidden && PLAYBACK._preSeekState === "playing" && PLAYBACK.state === "paused") {
            playbackPlay();
          }
        };
        document.addEventListener("visibilitychange", handler);
        PLAYBACK._visCleanup = () => document.removeEventListener("visibilitychange", handler);
      }

      function _playbackRemoveVisibility() {
        if (PLAYBACK._visCleanup) {
          PLAYBACK._visCleanup();
          PLAYBACK._visCleanup = null;
        }
      }

      function initDockIfNeeded() {
        if (DOCK_READY) return;
        const slider = document.getElementById("dockSlider");
        if (!slider || !ALL_COMMITS.length) return;
        slider.min = "0";
        slider.max = String(ALL_COMMITS.length - 1);
        slider.step = "1";
        document.getElementById("dockLeftLabel").textContent = ALL_COMMITS[0]?.short || "-";
        document.getElementById("dockRightLabel").textContent = ALL_COMMITS[ALL_COMMITS.length - 1]?.short || "-";
        DOCK_READY = true;
      }

      function drawDock() {
        const canvas = document.getElementById("dockCanvas");
        if (!canvas || !ALL_COMMITS.length) return;
        const dpr = window.devicePixelRatio || 1;
        const w = Math.max(1, Math.floor(canvas.clientWidth * dpr));
        const h = Math.max(1, Math.floor(canvas.clientHeight * dpr));
        if (canvas.width !== w || canvas.height !== h) {
          canvas.width = w;
          canvas.height = h;
        }
        const ctx = canvas.getContext("2d");
        if (!ctx) return;
        ctx.clearRect(0, 0, w, h);

        const maxImpact = Math.max(1, ...ALL_COMMITS.map((c) => Number(c.impact || 0)));
        const barW = w / ALL_COMMITS.length;
        const pad = 2 * dpr;

        for (let i = 0; i < ALL_COMMITS.length; i++) {
          const c = ALL_COMMITS[i];
          const v = Number(c.impact || 0);
          const t = Math.sqrt(v / maxImpact);
          const bh = Math.max(1, (h - pad * 2) * t);
          const x = i * barW;
          const y = h - pad - bh;
          ctx.globalAlpha = 0.9;
          ctx.fillStyle = bucketById(c.primary).color;
          ctx.fillRect(x, y, Math.max(1, barW), bh);
        }

        // Selected marker.
        const sx = DOC.idx * barW;
        ctx.globalAlpha = 1;
        ctx.lineWidth = Math.max(1, dpr);
        ctx.strokeStyle = "rgba(2,6,23,0.85)";
        ctx.strokeRect(sx + 0.5, 0.5, Math.max(1, barW - 1), h - 1);

        // Inset glow
        ctx.globalAlpha = 1;
        ctx.strokeStyle = "rgba(37,99,235,0.35)";
        ctx.strokeRect(sx + 0.5, 1.5, Math.max(1, barW - 1), h - 3);
      }

      async function updateDocUI() {
        const loading = document.getElementById("docLoading");
        const main = document.getElementById("docMain");
        if (!loading || !main) return;

        if (!DATASET.loaded) {
          loading.classList.remove("hidden");
          main.classList.add("hidden");
          loading.textContent = DATASET.error
            ? `Spec evolution dataset unavailable: ${DATASET.error}`
            : "Loading spec evolution dataset... (local gzip JSON; no GitHub API)";
          return;
        }

        loading.classList.add("hidden");
        main.classList.remove("hidden");
        updateDocPanelVisibility();

        const c = ALL_COMMITS[DOC.idx];
        if (!c) return;

        document.getElementById("dockTitle").textContent = `${c.short} · ${c.subject}`;

        const title = document.getElementById("docCommitTitle");
        const meta = document.getElementById("docCommitMeta");
        const link = document.getElementById("docCommitLink");
        if (title) title.textContent = c.subject;
        if (meta) meta.textContent = `${c.short} · ${dayjs(c.dateIso).format("YYYY-MM-DD HH:mm:ss")} · +${fmtInt(c.add)} -${fmtInt(c.del)}`;
        if (link) link.href = c.url;

        // Summary box.
        const sum = document.getElementById("docSummary");
        const tok = METRICS.tokensChanged.get(c.hash);
        const byt = METRICS.bytesChanged.get(c.hash);
        const hn = METRICS.hunks.get(c.hash);
        const lv = METRICS.lev.get(c.hash);
        if (sum) {
          sum.innerHTML = `
            <div class="flex flex-wrap gap-2">
              <span class="chip mono rounded-full px-2.5 py-1 text-[11px] text-slate-700">Impact ${fmtInt(c.impact)}</span>
              <span class="chip mono rounded-full px-2.5 py-1 text-[11px] text-slate-700">Hunks ${fmtInt(hn ?? 0)}</span>
              <span class="chip mono rounded-full px-2.5 py-1 text-[11px] text-slate-700">Tokens ${fmtInt(tok ?? 0)}</span>
              <span class="chip mono rounded-full px-2.5 py-1 text-[11px] text-slate-700">Bytes ${fmtInt(byt ?? 0)}</span>
              <span class="chip mono rounded-full px-2.5 py-1 text-[11px] text-slate-700">Lev ${(lv === null || lv === undefined) ? "?" : fmtInt(lv)}</span>
            </div>
            <div class="mt-2 text-xs text-slate-600">Bucket: <span class="font-semibold">${escapeHtml(bucketById(c.primary).name)}</span></div>
          `;
        }

        // Metrics cards
        document.getElementById("mTokens").textContent = (tok === null || tok === undefined) ? "-" : fmtInt(tok);
        document.getElementById("mBytes").textContent = (byt === null || byt === undefined) ? "-" : fmtInt(byt);
        document.getElementById("mHunks").textContent = (hn === null || hn === undefined) ? "-" : fmtInt(hn);
        document.getElementById("mLev").textContent = (lv === null || lv === undefined) ? "-" : fmtInt(lv);

        const patch = patchForIdx(DOC.idx);

        if (DOC.tab === "diff") {
          const raw = document.getElementById("diffRaw");
          const pretty = document.getElementById("diffPretty");
          // A/B compare mode: generate diff between two arbitrary commits.
          let diffText = patch || "";
          let abDiffError = false;
          if (DOC.compareMode) {
            try {
              diffText = await generateABDiff(DOC.compareFromIdx, DOC.compareToIdx);
            } catch (e) {
              abDiffError = true;
              diffText = "";
              if (pretty) pretty.innerHTML = `<div class="p-3 text-xs text-red-600">A/B diff failed: ${escapeHtml(String(e?.message || e))}</div>`;
            }
          }
          if (raw) raw.textContent = diffText;

          if (pretty && !abDiffError) {
            if (DOC.diffMode === "pretty") {
              try {
                const outputFmt = DOC.compareMode ? DOC.diffLayout : "side-by-side";
                pretty.innerHTML = Diff2Html.html(diffText, {
                  drawFileList: false,
                  matching: "lines",
                  outputFormat: outputFmt,
                });
              } catch (e) {
                pretty.innerHTML = `<div class="p-3 text-xs text-slate-700">Diff2Html failed: ${escapeHtml(String(e?.message || e))}</div>`;
              }
            } else {
              pretty.innerHTML = "";
            }
          }

          document.getElementById("diffPretty")?.classList.toggle("hidden", DOC.diffMode !== "pretty");
          document.getElementById("diffRaw")?.classList.toggle("hidden", DOC.diffMode !== "raw");

          // Update diff label for compare mode.
          const diffLabel = document.getElementById("diffLabel");
          if (diffLabel) {
            if (DOC.compareMode) {
              const cA = ALL_COMMITS[DOC.compareFromIdx];
              const cB = ALL_COMMITS[DOC.compareToIdx];
              diffLabel.textContent = `Diff (${cA?.short || "#" + DOC.compareFromIdx} → ${cB?.short || "#" + DOC.compareToIdx})`;
            } else {
              diffLabel.textContent = "Diff (parent → selected)";
            }
          }

          // A/B Compare metrics: show chips when in compare mode.
          const abMetricsBar = document.getElementById("abMetricsBar");
          if (abMetricsBar) {
            if (DOC.compareMode) {
              void computeAndShowABMetrics(DOC.compareFromIdx, DOC.compareToIdx);
            } else {
              abMetricsBar.classList.add("hidden");
            }
          }
        }

        if (DOC.tab === "spec") {
          const raw = document.getElementById("docRaw");
          const rendered = document.getElementById("docRendered");
          const specView = document.getElementById("docSpecView");
          if (specView) specView.classList.remove("hidden");

          const text = await docTextAt(DOC.idx);
          if (raw) raw.textContent = text;

          if (rendered) {
            // Extract outline and render markdown with anchor IDs injected.
            const outline = extractOutline(text);
            OUTLINE_CACHE.set(DOC.idx, outline);

            if (!window._mdSingleton) {
              window._mdSingleton = markdownit({
                html: false,
                linkify: true,
                typographer: true,
                highlight: (str, lang) => {
                  try {
                    if (lang && hljs.getLanguage(lang)) {
                      return `<pre class="hljs"><code>${hljs.highlight(str, { language: lang }).value}</code></pre>`;
                    }
                    return `<pre class="hljs"><code>${hljs.highlightAuto(str).value}</code></pre>`;
                  } catch {
                    return `<pre class="hljs"><code>${escapeHtml(str)}</code></pre>`;
                  }
                },
              });
            }

            // Install heading_open renderer to inject stable anchor IDs.
            let outlineIdx = 0;
            const origHeadingOpen = window._mdSingleton.renderer.rules.heading_open;
            window._mdSingleton.renderer.rules.heading_open = (tokens, idx, options, env, self) => {
              const entry = outline[outlineIdx++];
              if (entry) {
                tokens[idx].attrSet("id", entry.id);
              }
              return self.renderToken(tokens, idx, options);
            };

            const html = window._mdSingleton.render(text || "");
            rendered.innerHTML = DOMPurify.sanitize(html, { ADD_ATTR: ["id"] });

            // Restore original renderer rule.
            if (origHeadingOpen) {
              window._mdSingleton.renderer.rules.heading_open = origHeadingOpen;
            } else {
              delete window._mdSingleton.renderer.rules.heading_open;
            }
          }

          document.getElementById("docRendered").classList.toggle("hidden", DOC.rawSpec);
          document.getElementById("docRaw").classList.toggle("hidden", !DOC.rawSpec);

          // Refresh mini-map outline (non-blocking).
          void updateMiniMap();
        }

        // Refresh section summary panel (non-blocking).
        if (DOC.tab === "sections") {
          void updateSectionSummary();
        }

        // On-demand Levenshtein for selected commit.
        if (DATASET.loaded && DOC.idx > 0 && !METRICS.lev.has(c.hash)) {
          // Compute async, don't block UI.
          void (async () => {
            try {
              const d = await levenshteinForPatch(patch);
              METRICS.lev.set(c.hash, d);
              // Update only if still selected.
              if (ALL_COMMITS[DOC.idx]?.hash === c.hash) {
                document.getElementById("mLev").textContent = fmtInt(d);
                syncDockAndDoc();
              }
            } catch (e) {
              if (e?.name !== "AbortError") {
                console.error("On-demand Levenshtein failed:", e);
                if (ALL_COMMITS[DOC.idx]?.hash === c.hash) {
                  const levNode = document.getElementById("mLev");
                  if (levNode) {
                    levNode.textContent = "ERR";
                    levNode.title = formatErr(e);
                  }
                }
              }
            }
          })();
        }
      }

      let COMPUTE_RUNNING = false;

      function applyMetricsPayload(payload) {
        if (!payload) return;
        const tokens = payload.tokensChanged || {};
        const bytes = payload.bytesChanged || {};
        const hunks = payload.hunks || {};
        const lev = payload.lev || {};
        for (const [hash, value] of Object.entries(tokens)) {
          METRICS.tokensChanged.set(hash, Number(value || 0));
        }
        for (const [hash, value] of Object.entries(bytes)) {
          METRICS.bytesChanged.set(hash, Number(value || 0));
        }
        for (const [hash, value] of Object.entries(hunks)) {
          METRICS.hunks.set(hash, Number(value || 0));
        }
        for (const [hash, value] of Object.entries(lev)) {
          METRICS.lev.set(hash, Number(value || 0));
        }
      }

      async function computeAllMetricsLocal(progressCb, signal) {
        const total = ALL_COMMITS.length;
        for (let i = 0; i < total; i++) {
          if (signal?.aborted) throw new DOMException("Aborted", "AbortError");
          const c = ALL_COMMITS[i];
          const patch = patchForIdx(i);
          if (!METRICS.tokensChanged.has(c.hash) || !METRICS.bytesChanged.has(c.hash) || !METRICS.hunks.has(c.hash)) {
            const qm = quickMetricsFromPatch(patch);
            METRICS.tokensChanged.set(c.hash, qm.tokensChanged);
            METRICS.bytesChanged.set(c.hash, qm.bytesChanged);
            METRICS.hunks.set(c.hash, qm.hunks);
          }
          if (i > 0 && !METRICS.lev.has(c.hash)) {
            const d = await levenshteinForPatchLocal(patch);
            METRICS.lev.set(c.hash, d);
          }
          progressCb?.({
            stage: "metrics",
            done: i + 1,
            total,
            haveLev: METRICS.lev.size,
            message: "Computing per-commit metrics",
          });
          if (i % 4 === 0) {
            await new Promise((r) => setTimeout(r, 0));
            if (i % 12 === 0) render();
          }
        }
      }

      async function computeAllMetrics() {
        if (COMPUTE_RUNNING) return;
        if (!DATASET.loaded) return;
        if (!ALL_COMMITS.length) return;

        COMPUTE_RUNNING = true;
        COMPUTE_ABORT_CONTROLLER?.abort();
        COMPUTE_ABORT_CONTROLLER = new AbortController();
        const prog = document.getElementById("computeProgress");
        const cancelBtn = document.getElementById("btnCancelCompute");
        if (prog) {
          prog.classList.remove("hidden");
          prog.className = "mt-2 text-xs font-semibold text-slate-700";
          prog.textContent = WORKER_STATE.ready
            ? "Worker compute in progress..."
            : "Computing metrics (main-thread fallback)...";
          prog.title = "";
        }
        if (cancelBtn) {
          cancelBtn.classList.remove("hidden");
          cancelBtn.disabled = false;
        }

        try {
          const updateProgressText = (p) => {
            if (prog) {
              const done = Number(p?.done || 0);
              const total = Number(p?.total || ALL_COMMITS.length);
              const haveLev = Number(p?.haveLev || METRICS.lev.size);
              prog.textContent = `Computed ${done}/${total} commits · Levenshtein ready ${haveLev}/${Math.max(0, total - 1)}`;
            }
          };

          if (WORKER_STATE.ready) {
            const payload = await workerRequest(
              "compute_all_metrics",
              { includeLev: true },
              {
                signal: COMPUTE_ABORT_CONTROLLER.signal,
                timeoutMs: 180000,
                onProgress: updateProgressText,
              },
            );
            applyMetricsPayload(payload);
          } else {
            await computeAllMetricsLocal(updateProgressText, COMPUTE_ABORT_CONTROLLER.signal);
          }
          if (prog) prog.textContent = "Done. Charts updated.";
        } catch (e) {
          if (e?.name === "AbortError") {
            if (prog) {
              prog.textContent = "Compute cancelled.";
              prog.title = "";
            }
          } else {
            if (prog) {
              prog.className = "mt-2 text-xs font-semibold text-rose-700";
              prog.textContent = `Compute failed: ${e?.message || e}`;
              prog.title = formatErr(e);
            }
            console.error("computeAllMetrics failed:", e);
          }
        } finally {
          COMPUTE_RUNNING = false;
          if (cancelBtn) {
            cancelBtn.disabled = true;
            cancelBtn.classList.add("hidden");
          }
          render();
        }
      }

      function syncDockAndDoc() {
        if (!ALL_COMMITS.length) return;
        DOC.idx = clamp(DOC.idx, 0, ALL_COMMITS.length - 1);
        initDockIfNeeded();
        drawDock();
        void updateDocUI();
      }

      // --- Phase 4: Bucket toggle DOM caching ---
      let BUCKET_TOGGLES_BUILT = false;
      const BUCKET_TOGGLE_INDICATORS = new Map(); // bucketId -> [desktopSpan, mobileSpan]

      function renderBucketToggles() {
        const wrap = document.getElementById("bucketToggles");
        const wrapM = document.getElementById("bucketTogglesMobile");

        if (!BUCKET_TOGGLES_BUILT) {
          wrap.innerHTML = "";
          wrapM.innerHTML = "";
          for (const b of BUCKETS) {
            const item = bucketToggleItem(b);
            const itemM = bucketToggleItem(b, true);
            wrap.appendChild(item);
            wrapM.appendChild(itemM);
            const ind = item.querySelector("[data-toggle-ind]");
            const indM = itemM.querySelector("[data-toggle-ind]");
            BUCKET_TOGGLE_INDICATORS.set(b.id, [ind, indM]);
          }
          BUCKET_TOGGLES_BUILT = true;
        }

        // Update only the toggle indicator styles
        for (const b of BUCKETS) {
          const on = STATE.bucketEnabled.has(b.id);
          const indicators = BUCKET_TOGGLE_INDICATORS.get(b.id);
          if (!indicators) continue;
          for (const ind of indicators) {
            if (!ind) continue;
            ind.style.background = on ? b.color : "#cbd5e1";
            ind.style.transform = on ? "translateX(16px)" : "translateX(0)";
          }
        }
      }

      function bucketToggleItem(bucket, isMobile = false) {
        const id = (isMobile ? "m-" : "") + `b-${bucket.id}`;

        const btn = document.createElement("button");
        btn.type = "button";
        btn.id = id;
        btn.className =
          "focus-ring flex items-start gap-3 rounded-2xl border border-slate-900/10 bg-white/70 px-3 py-2 text-left hover:bg-white";
        btn.innerHTML = `
          <span class="mt-1 inline-block h-2.5 w-2.5 rounded-full" style="background:${bucket.color}"></span>
          <span class="min-w-0">
            <span class="block text-xs font-semibold text-slate-900">${escapeHtml(bucket.name)}</span>
            <span class="mt-0.5 block text-[11px] leading-snug text-slate-500">${escapeHtml(bucket.desc)}</span>
          </span>
          <span class="ml-auto mt-0.5 inline-flex h-6 w-10 items-center rounded-full border border-slate-900/10 bg-white/60 p-0.5">
            <span data-toggle-ind class="h-5 w-5 rounded-full transition"></span>
          </span>
        `;
        btn.addEventListener("click", () => {
          if (STATE.bucketEnabled.has(bucket.id)) {
            STATE.bucketEnabled.delete(bucket.id);
          } else {
            STATE.bucketEnabled.add(bucket.id);
          }
          render();
          syncUrlToState();
        });
        return btn;
      }

      // -----------------------------------------------------------
      // Wall-Clock Series Builder (bd-24q.12.1)
      // Builds a dense bin array with explicit empty bins so quiet
      // periods are visible. Supports UTC and local timezone modes.
      // -----------------------------------------------------------

      function wallClockBinKey(t, resolution) {
        const fmt2 = (n) => String(n).padStart(2, "0");
        if (resolution === "day") return t.format("YYYY-MM-DD");
        if (resolution === "hour") return t.format("YYYY-MM-DD HH:00");
        if (resolution === "15m") {
          const m = Math.floor(t.minute() / 15) * 15;
          return `${t.format("YYYY-MM-DD HH")}:${fmt2(m)}`;
        }
        if (resolution === "5m") {
          const m = Math.floor(t.minute() / 5) * 5;
          return `${t.format("YYYY-MM-DD HH")}:${fmt2(m)}`;
        }
        return t.format("YYYY-MM-DD HH:mm");
      }

      function wallClockBinMinutes(resolution) {
        if (resolution === "day") return 1440;
        if (resolution === "hour") return 60;
        if (resolution === "15m") return 15;
        if (resolution === "5m") return 5;
        return 1;
      }

      function wallClockFloor(t, resolution) {
        if (resolution === "day") return t.startOf("day");
        if (resolution === "hour") return t.startOf("hour");
        if (resolution === "15m") {
          const m = Math.floor(t.minute() / 15) * 15;
          return t.startOf("hour").add(m, "minute");
        }
        if (resolution === "5m") {
          const m = Math.floor(t.minute() / 5) * 5;
          return t.startOf("hour").add(m, "minute");
        }
        return t.startOf("minute");
      }

      function buildWallClockBins(commits, resolution, tzMode) {
        if (!commits.length || resolution === "commit") return null;

        const toT = (c) => (tzMode === "utc" && dayjs.utc) ? dayjs.utc(c.dateIso) : dayjs(c.dateIso);
        // Use calendar-aware units for day/hour to handle DST correctly in local tz.
        const stepUnit = resolution === "day" ? "day" : resolution === "hour" ? "hour" : "minute";
        const stepAmount = (stepUnit === "minute") ? wallClockBinMinutes(resolution) : 1;

        let minT = toT(commits[0]);
        let maxT = toT(commits[0]);
        for (const c of commits) {
          const t = toT(c);
          if (t.isBefore(minT)) minT = t;
          if (t.isAfter(maxT)) maxT = t;
        }

        const startBin = wallClockFloor(minT, resolution);
        const endBin = wallClockFloor(maxT, resolution);

        const labels = [];
        const binsMap = new Map();
        let cursor = startBin;
        const maxBins = 10000;
        let count = 0;
        while (!cursor.isAfter(endBin) && count < maxBins) {
          const key = wallClockBinKey(cursor, resolution);
          labels.push(key);
          binsMap.set(key, { key, label: key, start: cursor, commits: [] });
          cursor = cursor.add(stepAmount, stepUnit);
          count++;
        }

        for (const c of commits) {
          const t = toT(c);
          const key = wallClockBinKey(wallClockFloor(t, resolution), resolution);
          const bin = binsMap.get(key);
          if (bin) bin.commits.push(c);
        }

        return { labels, bins: labels.map((k) => binsMap.get(k)) };
      }

      function aggregateBinMetric(binCommits, metricFn, mode) {
        if (!binCommits.length) return 0;
        const vals = binCommits.map(metricFn);
        const sum = vals.reduce((a, b) => a + b, 0);
        if (mode === "sum") return sum;
        if (mode === "mean") return sum / vals.length;
        if (mode === "median") {
          const sorted = vals.slice().sort((a, b) => a - b);
          const mid = Math.floor(sorted.length / 2);
          return sorted.length % 2 ? sorted[mid] : (sorted[mid - 1] + sorted[mid]) / 2;
        }
        return sum;
      }

      function renderCharts(commits) {
        // Timeline scatter
        if (!chartTimeline) chartTimeline = echarts.init(document.getElementById("timelineChart"));
        if (!chartStack) chartStack = echarts.init(document.getElementById("stackChart"));
        if (!chartDonut) chartDonut = echarts.init(document.getElementById("donutChart"));
        if (!chartBocpd) chartBocpd = echarts.init(document.getElementById("bocpdChart"));

        const timelineData = commits.map((c) => {
          return [
            c.dateIso,
            c.impact,
            c.primary,
            c.short,
            c.subject,
            c.hash,
            c.author,
          ];
        });

        chartTimeline.setOption({
          grid: { left: 44, right: 18, top: 18, bottom: 40 },
          tooltip: {
            trigger: "item",
            borderWidth: 1,
            backgroundColor: "rgba(255,255,255,0.95)",
            textStyle: { color: "#0b1220" },
            extraCssText: "box-shadow: 0 16px 40px rgba(2,6,23,0.18); border-radius: 14px;",
            formatter: (p) => {
              const d = p.data;
              const b = bucketById(d[2]);
              return `
                <div style="min-width: 240px">
                  <div style="display:flex;align-items:center;gap:10px;">
                    <span style="width:10px;height:10px;border-radius:99px;background:${b.color};display:inline-block;"></span>
                    <div style="font-weight:700">${escapeHtml(d[3])}</div>
                    <div style="margin-left:auto;font-size:11px;color:rgba(2,6,23,.62)">${dayjs(d[0]).format(
                      "YYYY-MM-DD HH:mm:ss",
                    )}</div>
                  </div>
                  <div style="margin-top:6px;font-size:12px;color:rgba(2,6,23,.8)">${escapeHtml(d[4])}</div>
                  <div style="margin-top:6px;font-size:11px;color:rgba(2,6,23,.62)">Impact ${fmtInt(
                    d[1],
                  )} lines · Bucket ${escapeHtml(b.name)}</div>
                </div>
              `;
            },
          },
          xAxis: {
            type: "time",
            name: "time",
            nameTextStyle: { color: "rgba(2,6,23,.55)" },
            axisLabel: { color: "rgba(2,6,23,.55)" },
            axisLine: { lineStyle: { color: "rgba(2,6,23,.12)" } },
            splitLine: { lineStyle: { color: "rgba(2,6,23,.06)" } },
          },
          yAxis: {
            type: "value",
            name: "lines changed",
            nameTextStyle: { color: "rgba(2,6,23,.55)" },
            axisLabel: { color: "rgba(2,6,23,.55)" },
            axisLine: { lineStyle: { color: "rgba(2,6,23,.12)" } },
            splitLine: { lineStyle: { color: "rgba(2,6,23,.06)" } },
          },
          series: [
            {
              type: "scatter",
              data: timelineData,
              symbolSize: (d) => clamp(6 + Math.sqrt(d[1] || 0) * 0.7, 6, 40),
              itemStyle: {
                color: (p) => bucketById(p.data[2]).color,
                opacity: 0.9,
              },
              emphasis: { scale: true },
            },
          ],
        });

        chartTimeline.off("click");
        chartTimeline.on("click", (p) => {
          const hash = p.data[5];
          const idx = ALL_COMMITS.findIndex((c) => c.hash === hash);
          if (idx >= 0) selectCommitIdx(idx);
          const el = document.getElementById(`commit-${hash}`);
          if (el) {
            el.scrollIntoView({ behavior: "smooth", block: "start" });
            el.open = true;
          }
        });

        // Stacked buckets by time bin (commit/day/hour/15m/5m)
        // Uses dense wall-clock bins for time-based resolutions so
        // empty periods are visible in the chart (bd-24q.12.1).
        const resSel = document.getElementById("stackResolution");
        const metricSel = document.getElementById("stackMetric");
        const tzSel = document.getElementById("stackTimezone");
        const resolution = resSel?.value || "commit";
        const metric = metricSel?.value || "groups";
        const tzMode = tzSel?.value || "local";

        let binLabels = [];
        let binCommits = [];

        if (resolution === "commit") {
          // Commit mode: one bin per commit (no empty bins).
          const binIdx = new Map();
          for (const c of commits) {
            const k = c.short;
            let j = binIdx.get(k);
            if (j === undefined) {
              j = binLabels.length;
              binIdx.set(k, j);
              binLabels.push(k);
              binCommits.push([]);
            }
            binCommits[j].push(c);
          }
        } else {
          // Wall-clock mode: dense bins with empties filled.
          const wc = buildWallClockBins(commits, resolution, tzMode);
          if (wc) {
            binLabels = wc.labels;
            binCommits = wc.bins.map((b) => b.commits);
          }
        }

        const valuesByBucket = new Map(BUCKETS.map((b) => [b.id, new Array(binLabels.length).fill(0)]));

        const metricForCommit = (c) => {
          if (metric === "groups") return 1; // handled per-group
          if (metric === "lines") return Number(c.impact || 0);
          if (metric === "tokens") return Number(METRICS.tokensChanged.get(c.hash) || 0);
          if (metric === "lev") return Number(METRICS.lev.get(c.hash) || 0);
          return 0;
        };

        for (let bi = 0; bi < binCommits.length; bi++) {
          for (const c of binCommits[bi]) {
            if (metric === "groups") {
              if (STATE.bucketMode === "primary") {
                for (const g of c.changeGroups) {
                  const a = valuesByBucket.get(g.primary);
                  if (a) a[bi] += 1;
                }
              } else {
                for (const g of c.changeGroups) {
                  for (const b of g.labels) {
                    const a = valuesByBucket.get(b);
                    if (a) a[bi] += 1;
                  }
                }
              }
              continue;
            }

            const v = metricForCommit(c);
            const groups = c.changeGroups.length || 1;
            const perGroup = v / groups;

            if (STATE.bucketMode === "primary") {
              for (const g of c.changeGroups) {
                const a = valuesByBucket.get(g.primary);
                if (a) a[bi] += perGroup;
              }
            } else {
              for (const g of c.changeGroups) {
                const labels = g.labels?.length ? g.labels : [10];
                const perLabel = perGroup / labels.length;
                for (const b of labels) {
                  const a = valuesByBucket.get(b);
                  if (a) a[bi] += perLabel;
                }
              }
            }
          }
        }

        chartStack.setOption({
          grid: { left: 44, right: 18, top: 18, bottom: 60 },
          tooltip: {
            trigger: "axis",
            formatter: (params) => {
              if (!params?.length) return "";
              const label = params[0].axisValue + (tzMode === "utc" && resolution !== "commit" ? " UTC" : "");
              const total = params.reduce((s, p) => s + (Number(p.value) || 0), 0);
              const lines = params
                .filter((p) => p.value > 0)
                .map((p) => `${p.marker} ${escapeHtml(p.seriesName)}: ${Number(p.value).toFixed(metric === "groups" ? 0 : 1)}`)
                .join("<br>");
              return `<div style="font-weight:700;margin-bottom:4px">${escapeHtml(label)}</div>${lines || "<span style='color:#94a3b8'>No activity</span>"}${total > 0 ? `<div style="margin-top:4px;font-size:11px;color:rgba(2,6,23,.55)">Total: ${total.toFixed(metric === "groups" ? 0 : 1)}</div>` : ""}`;
            },
          },
          xAxis: {
            type: "category",
            data: binLabels,
            axisLabel: {
              color: "rgba(2,6,23,.55)",
              rotate: resolution === "commit" ? 45 : (resolution === "day" ? 0 : 45),
              interval: "auto",
            },
            axisLine: { lineStyle: { color: "rgba(2,6,23,.12)" } },
          },
          yAxis: {
            type: "value",
            axisLabel: { color: "rgba(2,6,23,.55)" },
            splitLine: { lineStyle: { color: "rgba(2,6,23,.06)" } },
          },
          legend: {
            type: "scroll",
            bottom: 0,
            textStyle: { color: "rgba(2,6,23,.62)" },
          },
          series: BUCKETS.map((b) => {
            return {
              name: `${b.id}. ${b.name}`,
              type: "bar",
              stack: "b",
              barWidth: "60%",
              emphasis: { focus: "series" },
              itemStyle: { color: b.color, opacity: 0.9 },
              data: valuesByBucket.get(b.id),
            };
          }),
        });

	        // Donut distribution by (primary or multi)
	        const totals = new Map(BUCKETS.map((b) => [b.id, 0]));
	        if (STATE.bucketMode === "primary") {
	          for (const c of commits) {
	            for (const g of c.changeGroups) {
	              totals.set(g.primary, (totals.get(g.primary) || 0) + 1);
	            }
	          }
	        } else {
	          for (const c of commits) {
	            for (const g of c.changeGroups) {
	              for (const b of g.labels) totals.set(b, (totals.get(b) || 0) + 1);
	            }
	          }
	        }
        const donutData = BUCKETS.map((b) => ({ name: `${b.id}. ${b.name}`, value: totals.get(b.id) || 0, itemStyle: { color: b.color } }));

        chartDonut.setOption({
          tooltip: { trigger: "item" },
          series: [
            {
              type: "pie",
              radius: ["45%", "70%"],
              avoidLabelOverlap: true,
              itemStyle: { borderRadius: 10, borderColor: "rgba(255,255,255,0.8)", borderWidth: 2 },
              label: { show: false },
              emphasis: { label: { show: true, fontWeight: 700, formatter: "{b}\\n{c}" } },
              labelLine: { show: false },
              data: donutData,
            },
          ],
        });

        renderBocpd(commits);
        void maybeRefreshPhaseAndOutliers(commits);
      }

      function renderBocpd(commits) {
        const H = Number(document.getElementById("hazard").value);
        document.getElementById("hazardLabel").textContent = H.toFixed(2);

        const xs = commits.map((c) => c.idx);
        const ys = commits.map((c) => Math.log1p(c.impact));

        const cp = WORKER_DERIVED.phase && WORKER_DERIVED.phaseKey === `${H}|${ys.join(",")}`
          ? WORKER_DERIVED.phase
          : bocpdChangePoints(ys, H);
        const markers = cp.changePoints.map((idx) => ({
          xAxis: xs[idx],
          label: { formatter: "CP", color: "rgba(2,6,23,.75)", fontSize: 10 },
          lineStyle: { color: "rgba(2,6,23,.25)", width: 1, type: "dashed" },
        }));

        chartBocpd.setOption({
          grid: { left: 44, right: 18, top: 18, bottom: 40 },
          tooltip: { trigger: "axis" },
          xAxis: {
            type: "value",
            name: "commit index",
            nameTextStyle: { color: "rgba(2,6,23,.55)" },
            axisLabel: { color: "rgba(2,6,23,.55)" },
            splitLine: { lineStyle: { color: "rgba(2,6,23,.06)" } },
          },
          yAxis: {
            type: "value",
            name: "log(1+impact)",
            nameTextStyle: { color: "rgba(2,6,23,.55)" },
            axisLabel: { color: "rgba(2,6,23,.55)" },
            splitLine: { lineStyle: { color: "rgba(2,6,23,.06)" } },
          },
          series: [
            {
              type: "line",
              data: xs.map((x, i) => [x, ys[i]]),
              showSymbol: false,
              lineStyle: { width: 2, color: "rgba(2,6,23,.78)" },
              markLine: {
                symbol: "none",
                data: markers,
              },
            },
            {
              type: "line",
              data: xs.map((x, i) => [x, cp.p0[i]]),
              yAxisIndex: 0,
              showSymbol: false,
              lineStyle: { width: 2, color: "rgba(37,99,235,.55)" },
            },
          ],
        });
      }

      // Minimal BOCPD over a scalar series with a crude Normal-Gamma model.
      // This is an interpretive visualization only.
      function bocpdChangePoints(y, hazard) {
        // Prior
        let mu0 = 0.0;
        let kappa0 = 0.01;
        let alpha0 = 0.5;
        let beta0 = 0.5;

        // Run-length posterior (log-space)
        let logR = [0.0]; // P(r=0)=1 at t=0
        // Sufficient stats per run length: n, mean, M2
        let stats = [{ n: 0, mean: 0.0, m2: 0.0 }];

        const p0 = [];
        const changePoints = [];

        const logHaz = Math.log(hazard);
        const log1mHaz = Math.log(1.0 - hazard);

        for (let t = 0; t < y.length; t++) {
          const x = y[t];

          // Predictive probabilities for each run length
          const logPred = [];
          for (let r = 0; r < stats.length; r++) {
            const st = stats[r];
            // Conjugate predictive approx: Student-t-ish with parameters from Normal-Gamma.
            const n = st.n;
            const mean = st.mean;
            const kappa = kappa0 + n;
            const alpha = alpha0 + n / 2;
            const beta =
              beta0 +
              0.5 * st.m2 +
              (kappa0 * n * (mean - mu0) * (mean - mu0)) / (2 * (kappa0 + n));

            const dof = 2 * alpha;
            const scale2 = (beta * (kappa + 1)) / (alpha * kappa);
            const logp = studentTLogPdf(x, mean, Math.sqrt(scale2), dof);
            logPred.push(logp);
          }

          // Growth + change point
          const newLogR = new Array(stats.length + 1).fill(-Infinity);

          // r_t = 0
          let logSumCp = -Infinity;
          for (let r = 0; r < logR.length; r++) {
            logSumCp = logAddExp(logSumCp, logR[r] + logPred[r] + logHaz);
          }
          newLogR[0] = logSumCp;

          // r_t = r_{t-1}+1
          for (let r = 0; r < logR.length; r++) {
            newLogR[r + 1] = logR[r] + logPred[r] + log1mHaz;
          }

          // Normalize
          const logZ = newLogR.reduce((a, b) => logAddExp(a, b), -Infinity);
          for (let i = 0; i < newLogR.length; i++) newLogR[i] -= logZ;

          // Record P(r_t=0)
          const p_r0 = Math.exp(newLogR[0]);
          p0.push(p_r0);
          if (p_r0 > 0.5) changePoints.push(t);

          // Update stats for next step
          const newStats = new Array(stats.length + 1);
          // For r=0, reset stats
          newStats[0] = { n: 1, mean: x, m2: 0.0 };
          for (let r = 1; r < newStats.length; r++) {
            newStats[r] = updateWelford(stats[r - 1], x);
          }

          // Prune (keep top K run lengths)
          const K = 120;
          const idxs = newLogR.map((v, i) => [v, i]).sort((a, b) => b[0] - a[0]).slice(0, K).map((x) => x[1]).sort((a,b)=>a-b);
          logR = idxs.map((i) => newLogR[i]);
          stats = idxs.map((i) => newStats[i]);

          // Renormalize after pruning
          const logZ2 = logR.reduce((a, b) => logAddExp(a, b), -Infinity);
          logR = logR.map((v) => v - logZ2);
        }

        return { p0, changePoints };
      }

      function updateWelford(st, x) {
        const n1 = st.n + 1;
        const delta = x - st.mean;
        const mean1 = st.mean + delta / n1;
        const delta2 = x - mean1;
        const m21 = st.m2 + delta * delta2;
        return { n: n1, mean: mean1, m2: m21 };
      }

      function logAddExp(a, b) {
        if (a === -Infinity) return b;
        if (b === -Infinity) return a;
        const m = Math.max(a, b);
        return m + Math.log(Math.exp(a - m) + Math.exp(b - m));
      }

      function studentTLogPdf(x, mu, sigma, nu) {
        // log Γ((ν+1)/2) - log(σ*sqrt(νπ)) - log Γ(ν/2) - (ν+1)/2 log(1 + ((x-μ)/σ)^2 / ν)
        const z = (x - mu) / (sigma || 1e-9);
        return (
          logGamma((nu + 1) / 2) -
          logGamma(nu / 2) -
          Math.log((sigma || 1e-9) * Math.sqrt(nu * Math.PI)) -
          ((nu + 1) / 2) * Math.log(1 + (z * z) / nu)
        );
      }

      // Lanczos approximation for log-gamma
      function logGamma(z) {
        const p = [
          0.99999999999980993, 676.5203681218851, -1259.1392167224028, 771.32342877765313,
          -176.61502916214059, 12.507343278686905, -0.13857109526572012, 9.9843695780195716e-6,
          1.5056327351493116e-7,
        ];
        if (z < 0.5) {
          return Math.log(Math.PI) - Math.log(Math.sin(Math.PI * z)) - logGamma(1 - z);
        }
        z -= 1;
        let x = p[0];
        for (let i = 1; i < p.length; i++) x += p[i] / (z + i);
        const t = z + 7.5;
        return 0.5 * Math.log(2 * Math.PI) + (z + 0.5) * Math.log(t) - t + Math.log(x);
      }

      // --- Phase 5: Commit list DOM caching + progressive rendering ---
      const COMMIT_LIST_NODES = new Map(); // hash -> details element
      let COMMIT_LIST_MODE = null; // tracks bucketMode used for build

      function buildCommitNode(c) {
        const details = document.createElement("details");
        details.id = `commit-${c.hash}`;
        details.className = "glass-2 rounded-3xl px-4 py-3 shadow-sm";
        details.dataset.hash = c.hash;

        const tags = (STATE.bucketMode === "primary" ? [c.primary] : c.labels).map(bucketById);

        details.innerHTML = `
          <summary class="cursor-pointer list-none">
            <div class="flex flex-col gap-3 sm:flex-row sm:items-start sm:justify-between">
              <div class="min-w-0">
                <div class="flex flex-wrap items-center gap-2">
                  <span class="mono text-xs font-semibold text-slate-600">${escapeHtml(c.short)}</span>
                  <span class="mono text-xs text-slate-500">${dayjs(c.dateIso).format("HH:mm:ss")}</span>
                  <span class="mono text-xs text-slate-500">+${fmtInt(c.add)} -${fmtInt(c.del)}</span>
                </div>
                <div class="mt-1 truncate text-sm font-semibold text-slate-900">${escapeHtml(c.subject)}</div>
                <div class="mt-1 flex flex-wrap gap-1.5">
                  ${tags
                    .map(
                      (t) => `
                        <span class="chip inline-flex items-center gap-2 rounded-full px-2.5 py-1 text-[11px] font-semibold text-slate-700">
                          <span class="inline-block h-2 w-2 rounded-full" style="background:${t.color}"></span>
                          ${escapeHtml(`${t.id}. ${t.name}`)}
                        </span>`,
                    )
                    .join("")}
                </div>
              </div>
              <div class="shrink-0">
                <a class="focus-ring chip inline-flex items-center gap-2 rounded-2xl px-3 py-2 text-xs font-semibold text-slate-700 hover:bg-white"
                   href="${escapeHtml(c.url)}" target="_blank" rel="noreferrer">View commit</a>
              </div>
            </div>
          </summary>
          <div class="mt-3 border-t border-slate-900/10 pt-3">
            ${c.hasClassification ? "" : `<div class="text-xs text-red-700">Missing classification entry for this commit.</div>`}
            ${c.changeGroups
              .map((g, i) => renderGroup(g, i))
              .join("")}
          </div>
        `;

        details.addEventListener("toggle", () => {
          if (details.open) selectCommitIdx(c.idx);
        });

        return details;
      }

      function ensureCommitNodes() {
        if (COMMIT_LIST_MODE === STATE.bucketMode && COMMIT_LIST_NODES.size === ALL_COMMITS.length) return;
        COMMIT_LIST_NODES.clear();
        COMMIT_LIST_MODE = STATE.bucketMode;
        for (const c of ALL_COMMITS) {
          COMMIT_LIST_NODES.set(c.hash, buildCommitNode(c));
        }
      }

      function renderCommitList(filteredCommits) {
        ensureCommitNodes();
        const wrap = document.getElementById("commitList");
        const filteredSet = new Set(filteredCommits.map((c) => c.hash));

        // If the DOM already has the right children, just toggle visibility
        if (wrap.children.length === ALL_COMMITS.length && wrap.dataset.mode === STATE.bucketMode) {
          for (const node of wrap.children) {
            node.classList.toggle("hidden", !filteredSet.has(node.dataset.hash));
          }
          return;
        }

        // Full rebuild: insert all nodes via DocumentFragment (single reflow),
        // toggle hidden based on filter match.
        wrap.innerHTML = "";
        wrap.dataset.mode = STATE.bucketMode;
        const frag = document.createDocumentFragment();

        for (const c of ALL_COMMITS) {
          const node = COMMIT_LIST_NODES.get(c.hash);
          if (!node) continue;
          node.classList.toggle("hidden", !filteredSet.has(c.hash));
          frag.appendChild(node);
        }
        wrap.appendChild(frag);
      }

      function renderGroup(g, i) {
        const chips = g.labels.map((id) => bucketById(id));
        const confPct = Math.round(clamp(g.confidence, 0, 1) * 100);
        const headings = (g.changed_headings || []).slice(0, 8);
        const evidence = (g.evidence || []).slice(0, 3);

        return `
          <div class="mt-3 rounded-3xl border border-slate-900/10 bg-white/60 p-4">
            <div class="flex flex-col gap-2 sm:flex-row sm:items-start sm:justify-between">
              <div class="min-w-0">
                <div class="text-xs font-semibold text-slate-500">Change group ${i + 1}</div>
                <div class="mt-1 text-sm font-semibold text-slate-900">${escapeHtml(g.summary || "")}</div>
                <div class="mt-2 flex flex-wrap gap-1.5">
                  ${chips
                    .map(
                      (c) => `
                        <span class="chip inline-flex items-center gap-2 rounded-full px-2.5 py-1 text-[11px] font-semibold text-slate-700">
                          <span class="inline-block h-2 w-2 rounded-full" style="background:${c.color}"></span>
                          ${escapeHtml(`${c.id}. ${c.name}`)}
                        </span>`,
                    )
                    .join("")}
                  <span class="chip inline-flex items-center rounded-full px-2.5 py-1 text-[11px] font-semibold text-slate-700">
                    Confidence <span class="mono ml-1">${confPct}%</span>
                  </span>
                </div>
              </div>
            </div>

            ${
              headings.length
                ? `
                  <div class="mt-3">
                    <div class="text-[11px] font-semibold text-slate-500">Touched headings</div>
                    <div class="mt-2 flex flex-wrap gap-1.5">
                      ${headings
                        .map(
                          (h) => `<span class="chip mono rounded-full px-2.5 py-1 text-[11px] text-slate-700">${escapeHtml(h)}</span>`,
                        )
                        .join("")}
                    </div>
                  </div>
                `
                : ""
            }

            ${
              evidence.length
                ? `
                  <div class="mt-3">
                    <div class="text-[11px] font-semibold text-slate-500">Diff excerpts</div>
                    <div class="mt-2 grid grid-cols-1 gap-2">
                      ${evidence
                        .map(
                          (e) => `
                            <pre class="codebox overflow-auto rounded-2xl p-3"><code class="language-diff">${escapeHtml(
                              String(e),
                            )}</code></pre>
                          `,
                        )
                        .join("")}
                    </div>
                  </div>
                `
                : ""
            }
          </div>
        `;
      }

      // -----------------------------
      // Navigation helpers
      // -----------------------------

      function scrollToSection(id) {
        const el = document.getElementById(id);
        if (!el) return;
        el.scrollIntoView({ behavior: "smooth", block: "start" });
      }

      // -----------------------------
      // UI wiring
      // -----------------------------

      function wireUI() {
        const q = document.getElementById("q");
        const impact = document.getElementById("impact");
        const impactLabel = document.getElementById("impactLabel");
        const qMobile = document.getElementById("qMobile");
        const impactMobile = document.getElementById("impactMobile");
        const impactLabelMobile = document.getElementById("impactLabelMobile");

        const setImpact = (v, skipRender) => {
          STATE.minImpact = Number(v);
          impact.value = String(v);
          impactMobile.value = String(v);
          impactLabel.textContent = `${fmtInt(v)} lines`;
          impactLabelMobile.textContent = `${fmtInt(v)} lines`;
          if (!skipRender) render();
        };
        setImpact(STATE.minImpact, true);

        const debouncedRenderSearch = debounce(() => { render(); syncUrlToState(); }, 150);
        const debouncedRenderSlider = debounce(() => { render(); syncUrlToState(); }, 50);

        q.addEventListener("input", () => {
          STATE.q = q.value;
          qMobile.value = q.value;
          debouncedRenderSearch();
        });
        qMobile.addEventListener("input", () => {
          STATE.q = qMobile.value;
          q.value = qMobile.value;
          debouncedRenderSearch();
        });

        impact.addEventListener("input", () => {
          STATE.minImpact = Number(impact.value);
          impactMobile.value = impact.value;
          impactLabel.textContent = `${fmtInt(impact.value)} lines`;
          impactLabelMobile.textContent = `${fmtInt(impact.value)} lines`;
          debouncedRenderSlider();
        });
        impactMobile.addEventListener("input", () => {
          STATE.minImpact = Number(impactMobile.value);
          impact.value = impactMobile.value;
          impactLabel.textContent = `${fmtInt(impactMobile.value)} lines`;
          impactLabelMobile.textContent = `${fmtInt(impactMobile.value)} lines`;
          debouncedRenderSlider();
        });

        const setMode = (m) => {
          STATE.bucketMode = m;
          document.getElementById("bucketModeLabel").textContent = m;
          document.getElementById("modePrimary").className =
            "focus-ring rounded-2xl border border-slate-900/10 px-3 py-2 text-xs font-semibold " +
            (m === "primary" ? "bg-slate-900 text-white" : "bg-white/70 text-slate-900");
          document.getElementById("modeMulti").className =
            "focus-ring rounded-2xl border border-slate-900/10 px-3 py-2 text-xs font-semibold " +
            (m === "multi" ? "bg-slate-900 text-white" : "bg-white/70 text-slate-900");
          document.getElementById("modePrimaryMobile").className =
            "focus-ring rounded-2xl border border-slate-900/10 px-3 py-2 text-xs font-semibold " +
            (m === "primary" ? "bg-slate-900 text-white" : "bg-white/70 text-slate-900");
          document.getElementById("modeMultiMobile").className =
            "focus-ring rounded-2xl border border-slate-900/10 px-3 py-2 text-xs font-semibold " +
            (m === "multi" ? "bg-slate-900 text-white" : "bg-white/70 text-slate-900");
          render();
          syncUrlToState();
        };

        document.getElementById("modePrimary").addEventListener("click", () => setMode("primary"));
        document.getElementById("modeMulti").addEventListener("click", () => setMode("multi"));
        document.getElementById("modePrimaryMobile").addEventListener("click", () => setMode("primary"));
        document.getElementById("modeMultiMobile").addEventListener("click", () => setMode("multi"));

        document.getElementById("btnReset").addEventListener("click", () => { resetFilters(); syncUrlToState(); });
        document.getElementById("btnResetMobile").addEventListener("click", () => { resetFilters(); syncUrlToState(); });

        // Section buttons
        document.getElementById("viewTimeline").addEventListener("click", () => scrollToSection("sectionTimeline"));
        document.getElementById("viewCommits").addEventListener("click", () => scrollToSection("sectionCommits"));
        document.getElementById("viewAlien").addEventListener("click", () => scrollToSection("sectionAlien"));

        // BOCPD hazard slider
        document.getElementById("hazard").addEventListener("input", debounce(() => render(), 50));

        // Stack chart controls
        document.getElementById("stackResolution")?.addEventListener("change", () => render());
        document.getElementById("stackMetric")?.addEventListener("change", () => render());
        document.getElementById("stackTimezone")?.addEventListener("change", () => render());

        // Doc tabs + toggles
        document.getElementById("docTabSpec")?.addEventListener("click", () => setDocTab("spec"));
        document.getElementById("docTabDiff")?.addEventListener("click", () => setDocTab("diff"));
        document.getElementById("docTabMetrics")?.addEventListener("click", () => setDocTab("metrics"));
        document.getElementById("docTabSections")?.addEventListener("click", () => setDocTab("sections"));

        document.getElementById("btnCopyLink")?.addEventListener("click", () => copyPermalink());
        document.getElementById("btnShareHelp")?.addEventListener("click", () => toggleShareHelp());
        // Close share help popover on outside click.
        document.addEventListener("click", (e) => {
          const pop = document.getElementById("shareHelpPopover");
          const btn = document.getElementById("btnShareHelp");
          if (pop && !pop.classList.contains("hidden") && !pop.contains(e.target) && e.target !== btn) {
            pop.classList.add("hidden");
          }
        });

        document.getElementById("btnRawToggle")?.addEventListener("click", () => {
          DOC.rawSpec = !DOC.rawSpec;
          syncUrlToState();
          void updateDocUI();
        });

        document.getElementById("btnMiniMapToggle")?.addEventListener("click", () => {
          const mm = document.getElementById("miniMap");
          if (mm) {
            const show = mm.classList.toggle("hidden");
            const btn = document.getElementById("btnMiniMapToggle");
            if (btn) {
              btn.classList.toggle("bg-slate-900", !show);
              btn.classList.toggle("text-white", !show);
              btn.classList.toggle("bg-white/70", show);
              btn.classList.toggle("text-slate-900", show);
            }
            if (!show) void updateMiniMap();
          }
        });

        // Mini-map search filter with debounce (bd-24q.2.2).
        {
          let searchTimer = 0;
          document.getElementById("miniMapSearch")?.addEventListener("input", () => {
            clearTimeout(searchTimer);
            searchTimer = setTimeout(() => void updateMiniMap(), 150);
          });
        }

        document.getElementById("btnPrettyDiff")?.addEventListener("click", () => {
          DOC.diffMode = "pretty";
          syncUrlToState();
          void updateDocUI();
        });
        document.getElementById("btnRawDiff")?.addEventListener("click", () => {
          DOC.diffMode = "raw";
          syncUrlToState();
          void updateDocUI();
        });

        // --- A/B Compare event listeners ---
        document.getElementById("btnCompareToggle")?.addEventListener("click", () => {
          DOC.compareMode = !DOC.compareMode;
          const btn = document.getElementById("btnCompareToggle");
          const bar = document.getElementById("abCompareBar");
          const layoutBtn = document.getElementById("btnDiffLayout");
          if (btn) { btn.classList.toggle("bg-slate-900", DOC.compareMode); btn.classList.toggle("text-white", DOC.compareMode); btn.classList.toggle("bg-white/70", !DOC.compareMode); btn.classList.toggle("text-slate-900", !DOC.compareMode); }
          if (bar) bar.classList.toggle("hidden", !DOC.compareMode);
          if (layoutBtn) layoutBtn.classList.toggle("hidden", !DOC.compareMode);
          if (DOC.compareMode) { DOC.compareFromIdx = Math.max(0, DOC.idx - 1); DOC.compareToIdx = DOC.idx; populateCompareSelects(); }
          syncUrlToState();
          void updateDocUI();
        });
        document.getElementById("btnDiffLayout")?.addEventListener("click", () => {
          DOC.diffLayout = DOC.diffLayout === "side-by-side" ? "line-by-line" : "side-by-side";
          const btn = document.getElementById("btnDiffLayout");
          if (btn) btn.textContent = DOC.diffLayout === "side-by-side" ? "Side-by-Side" : "Unified";
          syncUrlToState(); void updateDocUI();
        });
        document.getElementById("btnSwapAB")?.addEventListener("click", () => {
          const tmp = DOC.compareFromIdx; DOC.compareFromIdx = DOC.compareToIdx; DOC.compareToIdx = tmp;
          populateCompareSelects(); syncUrlToState(); void updateDocUI();
        });
        document.getElementById("compareFromSelect")?.addEventListener("change", (e) => { DOC.compareFromIdx = Number(e.target.value); syncUrlToState(); void updateDocUI(); });
        document.getElementById("compareToSelect")?.addEventListener("change", (e) => { DOC.compareToIdx = Number(e.target.value); syncUrlToState(); void updateDocUI(); });

        document.getElementById("btnComputeAll")?.addEventListener("click", () => {
          void computeAllMetrics();
        });
        document.getElementById("btnCancelCompute")?.addEventListener("click", () => {
          COMPUTE_ABORT_CONTROLLER?.abort();
        });

        // Galaxy brain button: jump to alien section
        document.getElementById("btnGalaxy").addEventListener("click", () => scrollToSection("sectionAlien"));

        // Dock collapse toggle (mobile)
        const dockBody = document.getElementById("dockBody");
        const dockToggle = document.getElementById("dockCollapseToggle");
        if (dockToggle && dockBody) {
          dockToggle.addEventListener("click", () => {
            const hidden = dockBody.classList.toggle("hidden");
            dockToggle.textContent = hidden ? "\u25BC" : "\u25B2";
            if (!hidden) drawDock(); // redraw canvas after un-collapsing
          });
        }

        // Dock controls
        document.getElementById("dockPrev")?.addEventListener("click", () => selectCommitIdx(DOC.idx - 1));
        document.getElementById("dockNext")?.addEventListener("click", () => selectCommitIdx(DOC.idx + 1));
        document.getElementById("dockSlider")?.addEventListener("input", (e) => {
          playbackOnManualScrub();
          const v = Number(e?.target?.value || 0);
          selectCommitIdx(v);
        });
        document.getElementById("dockSlider")?.addEventListener("change", () => {
          playbackOnScrubEnd();
        });

        // Playback controls (bd-24q.7.2).
        document.getElementById("dockPlayPause")?.addEventListener("click", () => playbackToggle());
        document.getElementById("dockSpeed")?.addEventListener("change", (e) => {
          playbackSetSpeed(Number(e.target.value));
          _syncPlaybackUI();
        });
        document.getElementById("dockLoop")?.addEventListener("click", () => {
          playbackSetLoop(!PLAYBACK.loop);
          _syncPlaybackUI();
        });

        document.addEventListener("keydown", (e) => {
          const t = e.target;
          const isTyping =
            t && (t.tagName === "INPUT" || t.tagName === "TEXTAREA" || t.tagName === "SELECT" || t.isContentEditable);
          if (isTyping) return;

          if (e.key === "ArrowLeft") {
            e.preventDefault();
            selectCommitIdx(DOC.idx - 1);
          }
          if (e.key === "ArrowRight") {
            e.preventDefault();
            selectCommitIdx(DOC.idx + 1);
          }
          // Playback keyboard shortcuts (bd-24q.7.2).
          if (e.key === " " || e.code === "Space") {
            e.preventDefault();
            playbackToggle();
          }
        });

        // Mobile sheet
        const overlay = document.getElementById("overlay");
        const sheet = document.getElementById("sheet");
        const openSheet = () => {
          overlay.classList.remove("hidden");
          sheet.classList.remove("hidden");
          requestAnimationFrame(() => sheet.classList.add("open"));
        };
        const closeSheet = () => {
          sheet.classList.remove("open");
          setTimeout(() => {
            overlay.classList.add("hidden");
            sheet.classList.add("hidden");
          }, 200);
        };

        document.getElementById("btnFilters").addEventListener("click", openSheet);
        overlay.addEventListener("click", closeSheet);
        document.getElementById("btnCloseSheet").addEventListener("click", closeSheet);
        document.getElementById("btnApplyMobile").addEventListener("click", () => {
          // Apply mobile search changes
          STATE.q = qMobile.value;
          q.value = qMobile.value;
          render();
          syncUrlToState();
          closeSheet();
        });
      }

      function resetFilters() {
        STATE.q = "";
        STATE.minImpact = 0;
        STATE.bucketMode = "primary";
        STATE.bucketEnabled = new Set(BUCKETS.map((b) => b.id));

        document.getElementById("q").value = "";
        document.getElementById("qMobile").value = "";
        document.getElementById("impact").value = "0";
        document.getElementById("impactMobile").value = "0";
        document.getElementById("impactLabel").textContent = "0 lines";
        document.getElementById("impactLabelMobile").textContent = "0 lines";

        render();
      }

      // -----------------------------
      // Boot
      // -----------------------------

      window.addEventListener("resize", () => {
        chartTimeline?.resize();
        chartStack?.resize();
        chartDonut?.resize();
        chartBocpd?.resize();
        drawDock();
      });

      wireUI();
      (async function boot() {
        await loadEvolutionDataset();

        if (DATASET.loaded) {
          const workerReady = await initAnalysisWorker();
          const d = DATASET.data;
          if (workerReady) {
            try {
              const metricsNoLev = await workerRequest(
                "compute_all_metrics",
                { includeLev: false },
                { timeoutMs: 120000 },
              );
              applyMetricsPayload(metricsNoLev);
            } catch (e) {
              console.error("Worker boot metric precompute failed; using local precompute:", e);
              setWorkerStatus("Worker precompute failed; local fallback active.", "error", formatErr(e));
              for (let i = 0; i < d.commits.length; i++) {
                const c = d.commits[i];
                const qm = quickMetricsFromPatch(d.patches[i]);
                METRICS.tokensChanged.set(c.hash, qm.tokensChanged);
                METRICS.bytesChanged.set(c.hash, qm.bytesChanged);
                METRICS.hunks.set(c.hash, qm.hunks);
              }
            }
          } else {
            for (let i = 0; i < d.commits.length; i++) {
              const c = d.commits[i];
              const qm = quickMetricsFromPatch(d.patches[i]);
              METRICS.tokensChanged.set(c.hash, qm.tokensChanged);
              METRICS.bytesChanged.set(c.hash, qm.bytesChanged);
              METRICS.hunks.set(c.hash, qm.hunks);
            }
          }

          // Start at the latest commit (most interesting view).
          DOC.idx = Math.max(0, d.commits.length - 1);
        }

        // Restore state from URL if present; otherwise use defaults.
        const urlState = decodeUrlState(location.search);
        if (urlState) {
          applyUrlState(urlState);
          if (DOC.compareMode) {
            populateCompareSelects();
            const _b = document.getElementById("btnCompareToggle"); if (_b) { _b.classList.add("bg-slate-900","text-white"); _b.classList.remove("bg-white/70","text-slate-900"); }
            const _r = document.getElementById("abCompareBar"); if (_r) _r.classList.remove("hidden");
            const _l = document.getElementById("btnDiffLayout"); if (_l) { _l.classList.remove("hidden"); _l.textContent = DOC.diffLayout === "side-by-side" ? "Side-by-Side" : "Unified"; }
          }
        }

        // Set tab (may have been overridden by URL state).
        setDocTab(DOC.tab);
        render();
        // Record initial URL state (replaceState so back goes to referrer, not blank).
        syncUrlToState({ immediate: true, replace: true });
        void warmDerivedWorkerArtifacts();
      })();

      // Browser back/forward: restore state from URL.
      window.addEventListener("popstate", () => {
        const s = decodeUrlState(location.search);
        if (s) {
          applyUrlState(s);
          // Restore compare UI elements.
          const cmpBtn = document.getElementById("btnCompareToggle");
          const cmpBar = document.getElementById("abCompareBar");
          const layoutBtn = document.getElementById("btnDiffLayout");
          if (cmpBtn) { cmpBtn.classList.toggle("bg-slate-900", DOC.compareMode); cmpBtn.classList.toggle("text-white", DOC.compareMode); cmpBtn.classList.toggle("bg-white/70", !DOC.compareMode); cmpBtn.classList.toggle("text-slate-900", !DOC.compareMode); }
          if (cmpBar) cmpBar.classList.toggle("hidden", !DOC.compareMode);
          if (layoutBtn) { layoutBtn.classList.toggle("hidden", !DOC.compareMode); if (DOC.compareMode) layoutBtn.textContent = DOC.diffLayout === "side-by-side" ? "Side-by-Side" : "Unified"; }
          if (DOC.compareMode) populateCompareSelects();
          setDocTab(DOC.tab);
          selectCommitIdx(DOC.idx);
          render();
        }
      });
    </script>
  </body>
</html>
