{"id":"bd-10d","title":"Write B-tree property-based tests","description":"## Context\nB-tree correctness requires more than unit tests; property tests catch structural bugs.\n\n## Goal\nWrite property-based tests for B-tree invariants:\n- ordering\n- structural constraints (fanout bounds)\n- cursor traversal correctness\n\n## Acceptance Criteria\n- Tests generate random operation sequences (insert/delete/search) and validate invariants.\n- Shrinking produces small counterexamples.\n- Runs fast enough for `cargo test`.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:04.063494440Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-10d","depends_on_id":"bd-1me","type":"blocks","created_at":"2026-02-07T04:42:47.074353138Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10d","depends_on_id":"bd-3av","type":"parent-child","created_at":"2026-02-07T04:54:55.187729196Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-10d","depends_on_id":"bd-3us","type":"blocks","created_at":"2026-02-07T04:42:47.155309725Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-117","title":"Implement UnixVfs with asupersync I/O","description":"## Context\nUnixVfs is required for real workloads. It must implement SQLite-like VFS semantics, including locking, and must use asupersync for all I/O.\n\n## Goal\nImplement UnixVfs using asupersync:\n- open/read/write/truncate/sync\n- file size\n- locking/unlocking (scope must be explicit)\n\n## Acceptance Criteria\n- UnixVfs passes the same basic correctness tests as MemoryVfs.\n- Locking behavior is explicitly documented and tested (threads vs processes).\n- I/O uses asupersync primitives (no tokio).","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:03:24.477955861Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-117","depends_on_id":"bd-266","type":"parent-child","created_at":"2026-02-07T04:54:55.605073351Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-117","depends_on_id":"bd-266.1","type":"blocks","created_at":"2026-02-07T04:59:13.680792317Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-125","title":"Build full conformance test suite","description":"## Context\nOnce harness infrastructure exists, we need breadth: run large corpora and chase failures to zero.\n\n## Goal\nBuild and maintain a comprehensive conformance suite (primarily SLT + selected SQLite tests) that measures parity against the oracle.\n\n## Acceptance Criteria\n- The suite is runnable locally in a reasonable time (with a smoke mode).\n- The suite produces a pass/fail/skip report.\n- The target is 100% for the selected surface; skips require explicit issues.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:00:16.472882317Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-125","depends_on_id":"bd-26i","type":"blocks","created_at":"2026-02-07T04:43:20.659037257Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-125","depends_on_id":"bd-3v7","type":"parent-child","created_at":"2026-02-07T04:54:54.848584295Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-125","depends_on_id":"bd-3v7.3","type":"blocks","created_at":"2026-02-07T05:00:16.382898818Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-125","depends_on_id":"bd-3v7.4","type":"blocks","created_at":"2026-02-07T05:00:16.472842583Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-125","depends_on_id":"bd-yfs","type":"blocks","created_at":"2026-02-07T04:43:20.738257442Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-12u","title":"Write end-to-end integration tests","description":"## Context\nUnit tests validate components; end-to-end tests validate integration and prevent regressions.\n\n## Goal\nWrite end-to-end integration tests:\n- open DB\n- create schema\n- insert/update/delete\n- query\n- close/reopen\n\n## Acceptance Criteria\n- Tests run on MemoryVfs and UnixVfs (where applicable).\n- Tests cover at least one crash/recovery path (once WAL exists).\n- Tests are included in CI-like local runs.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:03:18.541165074Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-12u","depends_on_id":"bd-221","type":"parent-child","created_at":"2026-02-07T04:54:55.524127502Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-12u","depends_on_id":"bd-qvw","type":"blocks","created_at":"2026-02-07T04:42:54.431961219Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-14h","title":"Implement B-tree cell format parsing","description":"## Context\nCell format defines keys, payload, and overflow behavior. Correctness here is critical.\n\n## Goal\nImplement cell parsing/serialization:\n- intkey table cells\n- blobkey index cells\n- payload size computations\n\n## Acceptance Criteria\n- Unit tests cover boundary sizes (local payload threshold, overflow cutoff).\n- Round-trip: serialize(parse(cell)) == original.\n- Cross-check with oracle for known fixtures.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:03.532945962Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-14h","depends_on_id":"bd-3av","type":"parent-child","created_at":"2026-02-07T04:54:55.181841948Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-14h","depends_on_id":"bd-3sl","type":"blocks","created_at":"2026-02-07T04:42:46.591682364Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-15s","title":"Decide/implement harness-level sheaf consistency checker referenced in spec 4.6","status":"open","priority":3,"issue_type":"question","created_at":"2026-02-07T06:58:47.494562915Z","created_by":"ubuntu","updated_at":"2026-02-07T06:58:47.494562915Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-16k","title":"Implement B-tree cursor traversal","description":"## Context\nCursors implement ordered traversal and are the foundation for query execution.\n\n## Goal\nImplement cursor traversal:\n- seek/move-to-key\n- next/prev\n- stack of pages for interior traversal\n\n## Acceptance Criteria\n- Traversal returns keys in correct order across page boundaries.\n- Cursor stays valid across common mutations where SQLite defines behavior.\n- Property tests cover random insert sequences + traversal invariants.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:03.622662812Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-16k","depends_on_id":"bd-14h","type":"blocks","created_at":"2026-02-07T04:42:46.669696299Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-16k","depends_on_id":"bd-3av","type":"parent-child","created_at":"2026-02-07T04:54:55.183000889Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1a5","title":"[EPIC] CLI shell (frankentui)","description":"## Context\nWe want a drop-in replacement for the sqlite3 shell, but we also want to leverage /dp/frankentui for rendering and UX.\n\n## Goal\nBuild an interactive shell that matches sqlite3 behaviors (dot-commands, output modes) while using frankentui for a modern terminal experience.\n\n## Non-Negotiables\n- No ad-hoc terminal rendering libraries outside frankentui.\n- CLI behavior is conformance-tested where practical.\n\n## Deliverables\n- REPL + history\n- Output modes (.mode) + formatting\n- Dot-commands subset parity (.tables, .schema, .dump, .read, etc.)\n- Rendering via frankentui components\n\n## Success Criteria\n- For common workflows, output matches sqlite3 (or is strictly better but still script-friendly).","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-07T04:53:49.561001345Z","created_by":"ubuntu","updated_at":"2026-02-07T04:53:49.561001345Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["cli","frankentui"]}
{"id":"bd-1a5.1","title":"Add frankentui path dependency (workspace)","description":"## Context\nDocs require frankentui for all terminal rendering, but the workspace does not currently include it as a dependency.\n\n## Goal\nAdd frankentui as a workspace dependency via path `/dp/frankentui`, and plumb it into `fsqlite-cli`.\n\n## Acceptance Criteria\n- Root `Cargo.toml` includes `frankentui = { path = \"/dp/frankentui\" }` (or equivalent).\n- `fsqlite-cli` depends on frankentui and compiles.\n- No alternate terminal rendering crate is introduced.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:59:08.946125418Z","created_by":"ubuntu","updated_at":"2026-02-07T04:59:08.946125418Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["build","frankentui"],"dependencies":[{"issue_id":"bd-1a5.1","depends_on_id":"bd-1a5","type":"parent-child","created_at":"2026-02-07T04:59:08.946125418Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1cj","title":"[EPIC] WAL + crash recovery (formal crash model)","description":"## Context\nWAL and crash recovery are the correctness foundation. With MVCC and concurrent writers, we must be explicit about:\n- what constitutes a committed transaction on disk\n- what crash models we handle (torn writes, reordering, metadata durability)\n- how recovery reconstructs a consistent state\n\n## Goal\nImplement a crash model and a WAL/recovery design that is both:\n- compatible with SQLite file formats at rest\n- correct under the defined crash+fsync assumptions\n\n## Non-Negotiables\n- Crash model written down as a precise contract (not prose-only).\n- Recovery correctness is validated by tests that inject partial writes/corruption.\n- MVCC and WAL interaction is specified (visibility vs durability).\n\n## Deliverables\n- Formal crash model spec\n- WAL frame format + WAL index format (or compatibility story)\n- Checkpoint algorithm + invariants\n- Recovery algorithm + tests (kill -9 simulation, torn-write simulation)\n\n## Success Criteria\n- For the declared crash model, we can prove and test: after restart, DB is consistent and reflects exactly the set of committed transactions.\n- Recovery is deterministic and idempotent.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-07T04:53:05.949443684Z","created_by":"ubuntu","updated_at":"2026-02-07T04:53:05.949443684Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["crash-recovery","durability","wal"]}
{"id":"bd-1d2","title":"Implement MVCC-aware WAL frame format","description":"## Context\nWith MVCC and concurrent writers, we need a durable representation of committed changes and a way for readers/recovery to locate the latest committed page images.\n\n## Goal\nImplement the WAL frame format and commit semantics for our engine:\n- define what constitutes a committed transaction on disk\n- encode the data needed for recovery and checkpoint\n\n## Acceptance Criteria\n- Format and commit rules are consistent with `bd-1ud` (crash model).\n- Round-trip tests: write frames and parse them back.\n- Recovery can distinguish committed vs uncommitted records.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:01:38.139931473Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1d2","depends_on_id":"bd-1cj","type":"parent-child","created_at":"2026-02-07T04:54:55.097797214Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d2","depends_on_id":"bd-1ud","type":"blocks","created_at":"2026-02-07T04:43:03.047145187Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1d2","depends_on_id":"bd-ya9","type":"blocks","created_at":"2026-02-07T04:43:02.969614296Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1fr","title":"Write conflict rate analysis documentation","description":"## Context\nMVCC page-level conflict behavior depends on workload. We need a quantitative model to explain and to design benchmarks.\n\n## Goal\nWrite a short analysis that estimates conflict probability as a function of:\n- table size (pages)\n- pages written per transaction\n- concurrent writers\n\nAnd use it to motivate benchmark scenarios (low conflict vs high conflict).\n\n## Acceptance Criteria\n- Document includes the model and at least 2 numerical worked examples.\n- Benchmark suite references the scenarios derived from this analysis.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:00:37.244980319Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1fr","depends_on_id":"bd-3gk","type":"parent-child","created_at":"2026-02-07T04:54:54.932247264Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1fr","depends_on_id":"bd-ya9","type":"blocks","created_at":"2026-02-07T04:43:03.687656849Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1fv","title":"Sync CODEX spec with corrected main spec facts (profiles, asupersync API, RaptorQ claims)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T06:58:47.439936383Z","created_by":"ubuntu","updated_at":"2026-02-07T06:58:47.439936383Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1it","title":"[EPIC] MVCC-native storage engine","description":"## Context\nThis project’s primary differentiator is concurrent writers. If MVCC is bolted on later, we will bake single-writer assumptions into pager/cache/WAL APIs and rewrite them.\n\n## Goal\nImplement the storage foundation as MVCC-native from day one: snapshot-aware page reads, page-level version chains, eager conflict detection, bounded version growth via GC, and a cache architecture that is policy-pluggable.\n\n## Non-Negotiables\n- Safe Rust only (workspace forbids unsafe code).\n- Pager APIs are snapshot/txn-aware (no hidden globals).\n- Conflict detection is fail-fast (Busy), not wait-based.\n\n## Deliverables\n- MVCC core types (TxnId, Snapshot, Version metadata)\n- Visibility engine + conflict detection\n- MVCC-aware pager interface\n- Version GC with formal reclaimability predicate\n- Cache policy abstraction (so ARC/TinyLFU/etc can be evaluated, not hard-coded)\n\n## Success Criteria\n- Concurrent writers modifying disjoint pages can commit without blocking each other.\n- Version chain length is bounded under sustained load (GC keeps memory stable).\n- Property tests cover key invariants (visibility, no lost updates, deadlock freedom-by-construction).","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-07T04:52:43.084679120Z","created_by":"ubuntu","updated_at":"2026-02-07T04:52:56.429362309Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["mvcc","pager","storage"]}
{"id":"bd-1j9","title":"Implement core VDBE opcodes (50+)","description":"## Context\nWe need a minimal opcode set to execute basic SQL end-to-end and to bootstrap conformance.\n\n## Goal\nImplement a core subset of VDBE opcodes (50+) with correct semantics.\n\n## Acceptance Criteria\n- Each implemented opcode has at least one targeted test.\n- End-to-end: simple SELECT/INSERT/UPDATE/DELETE execute correctly on small DB.\n- Conformance smoke suite passes for queries covered by the implemented opcode subset.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:03:02.420162490Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1j9","depends_on_id":"bd-16k","type":"blocks","created_at":"2026-02-07T04:42:53.954523686Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1j9","depends_on_id":"bd-21e","type":"blocks","created_at":"2026-02-07T04:42:53.871041087Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1j9","depends_on_id":"bd-3t3","type":"parent-child","created_at":"2026-02-07T04:54:55.437778602Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1jq","title":"Implement rollback journal modes","description":"## Context\nSQLite has multiple journal modes beyond WAL. Even if we prioritize WAL+MVCC, journaling modes may be required for parity and for specific environments.\n\n## Goal\nImplement rollback journal modes required for parity (as scoped by the plan):\n- journal file creation\n- commit/rollback semantics\n- interaction with MVCC model\n\n## Acceptance Criteria\n- Supported journal modes are explicitly listed.\n- For supported modes, crash recovery tests pass.\n- Conformance suite includes at least a smoke set for journal-mode-specific behavior.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:01:38.492484345Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1jq","depends_on_id":"bd-1cj","type":"parent-child","created_at":"2026-02-07T04:54:55.100928349Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1jq","depends_on_id":"bd-ya9","type":"blocks","created_at":"2026-02-07T04:43:03.366349118Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1me","title":"Implement B-tree insert and page splitting","description":"## Context\nInsert path exercises balancing/splitting and is a major correctness+performance hotspot.\n\n## Goal\nImplement B-tree insert + page splitting:\n- insert into leaf\n- split when full\n- propagate split up the tree\n\n## Acceptance Criteria\n- Inserts maintain invariants (ordering, min/max fanout).\n- After random inserts, full traversal yields sorted keys and all records present.\n- Conformance fixtures cover overflow + split scenarios.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:03.713110652Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1me","depends_on_id":"bd-16k","type":"blocks","created_at":"2026-02-07T04:42:46.747298924Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1me","depends_on_id":"bd-3av","type":"parent-child","created_at":"2026-02-07T04:54:55.183950578Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1oc","title":"Spec: define CheckpointChunk + SnapshotManifest object formats and bootstrap pointers","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T17:33:54.490779244Z","created_by":"ubuntu","updated_at":"2026-02-07T17:33:54.490779244Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-1qo","title":"Implement miscellaneous extensions","description":"## Context\nSQLite ships a grab-bag of useful misc extensions.\n\n## Goal\nImplement the scoped set of misc extensions (generate_series, dbstat, dbpage, etc.).\n\n## Acceptance Criteria\n- Each extension has at least one targeted conformance fixture.\n- Feature flags are documented.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:03:48.715675398Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1qo","depends_on_id":"bd-2gj","type":"parent-child","created_at":"2026-02-07T04:54:55.763830606Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1qo","depends_on_id":"bd-qvw","type":"blocks","created_at":"2026-02-07T04:43:20.422353522Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ry","title":"Complete all 190+ VDBE opcodes","description":"## Context\nTrue parity requires implementing the full opcode surface used by SQLite 3.52.0.\n\n## Goal\nImplement the remaining VDBE opcodes with spec-driven semantics and conformance coverage.\n\n## Acceptance Criteria\n- Opcode coverage reaches 100% for the targeted version surface.\n- Conformance suite drives semantics (golden outputs match oracle).\n- Performance: dispatch loop and hotspots are benchmarked/profiler-visible.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:03:02.505350477Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1ry","depends_on_id":"bd-1j9","type":"blocks","created_at":"2026-02-07T04:43:11.352518588Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1ry","depends_on_id":"bd-3t3","type":"parent-child","created_at":"2026-02-07T04:54:55.438728462Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1ud","title":"Write formal crash model specification","description":"## Context\nCrash recovery correctness depends on explicit assumptions about what the OS/filesystem can do (reordering, torn writes, metadata durability).\n\n## Goal\nWrite a formal crash model spec for FrankenSQLite:\n- when writes become durable (fsync semantics)\n- what can be partially applied (sector/torn write model)\n- what can be reordered\n- what metadata operations require extra fsync (dir entry)\n\n## Acceptance Criteria\n- The crash model is a numbered list of assumptions that tests can simulate.\n- WAL/journal protocol invariants are stated relative to this model.\n- Recovery tests reference this model explicitly.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:01:38.048251926Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1ud","depends_on_id":"bd-1cj","type":"parent-child","created_at":"2026-02-07T04:54:55.096802761Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-1zb","title":"Implement advanced WHERE clause optimization","description":"## Context\nAdvanced WHERE optimization is a major SQLite performance and behavior area. Some optimizations can affect which indexes are chosen.\n\n## Goal\nImplement advanced WHERE clause optimizations as required by conformance and measured performance.\n\n## Acceptance Criteria\n- Conformance tests that depend on planner behavior pass.\n- Benchmarks show improvement on targeted workloads.\n- Any behavior change is justified and documented (or matches oracle).","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:02:23.952356762Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-1zb","depends_on_id":"bd-3so","type":"parent-child","created_at":"2026-02-07T04:54:55.274603962Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-1zb","depends_on_id":"bd-xza","type":"blocks","created_at":"2026-02-07T04:43:11.274944937Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-21e","title":"Implement VDBE register file and Mem type","description":"## Context\nThe VDBE register file and Mem representation define SQLite’s runtime value semantics.\n\n## Goal\nImplement:\n- Mem type (NULL/int/real/text/blob + flags)\n- register file storage\n- value conversions and comparisons (as required)\n\n## Acceptance Criteria\n- Unit tests cover numeric conversion edge cases, text encoding, blob handling.\n- Behavior matches oracle for core value operations used by opcodes.\n- No hidden allocations on hot paths (benchmarked).","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:03:02.333786599Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-21e","depends_on_id":"bd-3t3","type":"parent-child","created_at":"2026-02-07T04:54:55.436614421Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-221","title":"[EPIC] Public API + engine integration","description":"## Context\nA port is only real once the external API behaves identically. Internals can be better, but the contract must match: connection lifecycle, prepared statements, error codes, transactions, pragmas, and extension exposure.\n\n## Goal\nWire the subsystems into a coherent engine and expose a stable, SQLite-compatible API surface.\n\n## Non-Negotiables\n- Matching API surface for the selected target (including error codes and edge cases).\n- Conformance harness drives behavior (no guesswork).\n\n## Deliverables\n- Connection + statement pipeline\n- Transaction semantics (BEGIN/COMMIT/ROLLBACK, savepoints)\n- PRAGMA behaviors\n- Extension registration\n- CLI integration points (shell uses API crate)\n\n## Success Criteria\n- The harness can run end-to-end workloads using only the public API.\n- The CLI is a thin layer over the public API.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-07T04:53:35.991483486Z","created_by":"ubuntu","updated_at":"2026-02-07T04:53:35.991483486Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["api","core","integration"]}
{"id":"bd-22f","title":"Implement SQL recursive descent parser","description":"## Context\nParser must match SQLite’s accepted syntax and error behavior for conformance.\n\n## Goal\nImplement a recursive descent parser (Pratt for expressions) that builds the typed AST.\n\n## Acceptance Criteria\n- Parser covers full grammar surface (not \"95%\").\n- Error messages/codes are stable enough for conformance (normalized where needed).\n- Snapshot tests cover representative parse trees.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:23.771253193Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-22f","depends_on_id":"bd-2fc","type":"blocks","created_at":"2026-02-07T04:42:46.353462071Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22f","depends_on_id":"bd-3so","type":"parent-child","created_at":"2026-02-07T04:54:55.272720854Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22f","depends_on_id":"bd-bd5","type":"blocks","created_at":"2026-02-07T04:42:46.431435220Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-22r","title":"Implement freelist management","description":"## Context\nFreelist management controls page allocation/reuse and affects file size and performance.\n\n## Goal\nImplement freelist management compatible with SQLite format:\n- trunk/leaf freelist pages\n- allocate/free pages\n- maintain counts\n\n## Acceptance Criteria\n- Alloc/free sequences match oracle behavior on fixtures.\n- Freelist structure remains consistent after crashes/recovery (when applicable).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:03.975553036Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-22r","depends_on_id":"bd-3av","type":"parent-child","created_at":"2026-02-07T04:54:55.186967529Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-22r","depends_on_id":"bd-3sl","type":"blocks","created_at":"2026-02-07T04:42:46.992569893Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-24w","title":"Implement overflow page chain management","description":"## Context\nLarge payloads spill into overflow pages. Overflow chains must be correct and efficient.\n\n## Goal\nImplement overflow page chain management:\n- allocate overflow pages\n- read/write payload across chain\n- free overflow chain on delete\n\n## Acceptance Criteria\n- Round-trip tests for payload sizes spanning 0, 1, many overflow pages.\n- No leaks: freeing a record frees its overflow chain.\n- Oracle cross-check on overflow-heavy fixtures.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:03.889382118Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-24w","depends_on_id":"bd-14h","type":"blocks","created_at":"2026-02-07T04:42:46.913468300Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-24w","depends_on_id":"bd-3av","type":"parent-child","created_at":"2026-02-07T04:54:55.186021457Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-266","title":"[EPIC] VFS + I/O (asupersync)","description":"## Context\nSQLite correctness and portability depend on the VFS contract (open/read/write/sync/locking). In FrankenSQLite, VFS must also support the concurrency model (threads vs processes) and must use /dp/asupersync for all blocking/async I/O.\n\n## Goal\nImplement a robust VFS layer:\n- MemoryVfs (test/ephemeral)\n- UnixVfs (production) using asupersync primitives\n- Correct locking semantics (or explicit scope limits)\n\n## Non-Negotiables\n- No tokio. All async/IO patterns come from asupersync.\n- VFS semantics are conformance-tested (including tricky error cases).\n\n## Success Criteria\n- We can run core tests + conformance on MemoryVfs and UnixVfs.\n- File locking behavior is explicitly specified and tested.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-07T04:54:43.541171342Z","created_by":"ubuntu","updated_at":"2026-02-07T04:54:43.541171342Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["asupersync","io","vfs"]}
{"id":"bd-266.1","title":"Add asupersync path dependency (workspace)","description":"## Context\nDocs require asupersync for all I/O and async patterns, but the workspace does not currently include it as a dependency.\n\n## Goal\nAdd asupersync as a workspace dependency via path `/dp/asupersync`, and plumb it into the crates that need it (starting with UnixVfs).\n\n## Acceptance Criteria\n- Root `Cargo.toml` includes `asupersync = { path = \"/dp/asupersync\" }` (or equivalent workspace-relative path strategy).\n- `cargo check --workspace` succeeds.\n- No tokio dependency appears in the workspace.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:59:08.862856968Z","created_by":"ubuntu","updated_at":"2026-02-07T04:59:08.862856968Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["asupersync","build"],"dependencies":[{"issue_id":"bd-266.1","depends_on_id":"bd-266","type":"parent-child","created_at":"2026-02-07T04:59:08.862856968Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-26i","title":"Set up conformance golden file infrastructure","description":"## Context\nWe need an always-on conformance loop, not a late-stage audit.\n\n## Goal\nSet up the conformance harness plumbing:\n- golden storage layout under `conformance/`\n- test runner integration (fsqlite-harness)\n- normalization utilities (stable output, error normalization)\n\n## Acceptance Criteria\n- Running the harness produces a deterministic artifact directory (goldens + report).\n- Harness can run a small smoke corpus quickly.\n- `bd-125` (full suite) can build on this without redesign.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:00:16.036399700Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-26i","depends_on_id":"bd-3v7","type":"parent-child","created_at":"2026-02-07T04:54:54.847300630Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26i","depends_on_id":"bd-3v7.1","type":"blocks","created_at":"2026-02-07T05:00:15.950052543Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-26i","depends_on_id":"bd-3v7.2","type":"blocks","created_at":"2026-02-07T05:00:16.036361789Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-28o","title":"Implement conflict detection","description":"## Context\nTo allow concurrent writers safely, we need write-write conflict detection.\n\n## Goal\nImplement page-level conflict detection (fail-fast):\n- eager page lock acquisition on first write\n- Busy error returned immediately on conflict\n- no waiting while holding locks (deadlock freedom-by-construction)\n\n## Acceptance Criteria\n- Concurrent write attempts to the same page: one succeeds, one returns Busy.\n- Transactions writing disjoint pages do not block each other.\n- Property tests include random interleavings that never deadlock.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:01:17.556886310Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-28o","depends_on_id":"bd-1it","type":"parent-child","created_at":"2026-02-07T04:54:55.015621351Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-28o","depends_on_id":"bd-35w","type":"blocks","created_at":"2026-02-07T04:42:36.808428477Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29h","title":"[EPIC] RaptorQ: FEC-backed durability, repair, and replication","description":"## Context\nSQLite durability relies on strict fsync ordering and a sequential WAL. Real systems see:\n- power loss / crashes\n- torn writes / partial sector persistence\n- bit-rot / silent corruption (especially on cheap flash)\n- networked / remote filesystems that violate assumptions\n\nWe already have RaptorQ in /dp/asupersync. Fountain codes are a superpower: they turn \"missing/corrupt pieces\" into a solvable erasure problem, and they compose with streaming and replication.\n\n## Goal\nExploit RaptorQ in ways that preserve SQLite file-format compatibility at rest, but dramatically improve robustness and open the door to high-throughput multi-writer and replication modes.\n\n## Key Ideas (to be made concrete)\n1. WAL FEC sidecar\nMaintain a `.rq` parity sidecar for WAL frames (or page-diff bundles). If the WAL has erasures/corruption, recovery can decode missing frames without losing the transaction.\n\n2. Commit-as-codeword\nTreat each commit’s durable record as a codeword: persist enough parity symbols so that recovery succeeds even with bounded torn writes / missing tail blocks.\n\n3. Fountain replication\nStream WAL/commit capsules over lossy links; replicas can join mid-stream, collect symbols opportunistically, and decode when they have enough.\n\n4. Optional multi-writer capsule log\nAllow writers to emit independent \"commit capsules\" (encoded page-diff bundles) that can be merged/checkpointed deterministically, reducing hot serialization points.\n\n## Non-Negotiables\n- Default mode remains strict SQLite compatibility.\n- Any RaptorQ feature is opt-in via PRAGMA and/or Cargo feature flags.\n- Formal crash/erasure model + correctness obligations must be explicit and testable.\n\n## Success Criteria\n- A concrete crash+erasure model (sector size, max lost blocks, corruption model).\n- Simulated erasure/corruption tests that prove recovery succeeds when redundancy thresholds are met.\n- End-to-end demo: replicate a write workload over an intentionally lossy channel and converge.","acceptance_criteria":"## Success Criteria\n- We define a concrete crash+erasure model (sector size, max lost blocks, corruption model).\n- We can simulate erasures/corruption and prove recovery succeeds when redundancy thresholds are met.\n- We can run an end-to-end demo: replicate a write workload over an intentionally lossy channel and converge.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-07T04:51:10.781089416Z","created_by":"ubuntu","updated_at":"2026-02-07T04:52:21.374103266Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["asupersync","durability","raptorq","replication"]}
{"id":"bd-29h.1","title":"Define crash+erasure model for FEC (RaptorQ)","description":"## Context\nRaptorQ only helps if we define what failures we are correcting: erasures vs corruption, maximum lost blocks, sector/torn-write model.\n\n## Goal\nWrite a concrete model for:\n- what can be lost/corrupted in the WAL/page-diff stream\n- what redundancy we commit to persisting\n- what recovery guarantees we claim\n\n## Acceptance Criteria\n- Model parameters are explicit (max lost blocks, corruption model, symbol sizing).\n- Model is consistent with the core crash model (`bd-1ud`).\n- The model directly informs redundancy defaults (documented).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:57:16.112797363Z","created_by":"ubuntu","updated_at":"2026-02-07T05:04:05.170441214Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["correctness","raptorq"],"dependencies":[{"issue_id":"bd-29h.1","depends_on_id":"bd-29h","type":"parent-child","created_at":"2026-02-07T04:57:16.112797363Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29h.2","title":"Design WAL parity sidecar format (.rq)","description":"## Context\nWe want strict SQLite compatibility at rest. Changing WAL frame format would break that.\n\n## Goal\nDesign a sidecar file that stores RaptorQ symbols keyed by:\n- commit id / WAL segment id\n- frame ranges or page identifiers\n- hashes for integrity\n\n## Acceptance Criteria\n- Sidecar format is versioned and documented.\n- Sidecar can be written and read independently of engine state.\n- Integrity checks detect corrupted symbols cleanly.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:57:16.203611869Z","created_by":"ubuntu","updated_at":"2026-02-07T05:04:42.818357559Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["raptorq","wal"],"dependencies":[{"issue_id":"bd-29h.2","depends_on_id":"bd-29h","type":"parent-child","created_at":"2026-02-07T04:57:16.203611869Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29h.2","depends_on_id":"bd-29h.1","type":"blocks","created_at":"2026-02-07T05:04:42.818303418Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29h.3","title":"Implement FEC encode/decode pipeline (WAL repair)","description":"## Context\nWith a sidecar format, we need actual encoding and recovery integration.\n\n## Goal\nImplement:\n- encoding symbols on commit/checkpoint\n- decoding missing/corrupt frames during recovery\n- metrics and limits (avoid pathological CPU use)\n\n## Acceptance Criteria\n- Recovery can succeed with bounded erasures according to the model.\n- When recovery cannot succeed, failure mode is explicit and does not corrupt DB.\n- Encode/decode cost is benchmarked.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:57:16.292077534Z","created_by":"ubuntu","updated_at":"2026-02-07T05:04:42.925326900Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["raptorq","recovery","wal"],"dependencies":[{"issue_id":"bd-29h.3","depends_on_id":"bd-29h","type":"parent-child","created_at":"2026-02-07T04:57:16.292077534Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29h.3","depends_on_id":"bd-29h.2","type":"blocks","created_at":"2026-02-07T05:04:42.925262389Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29h.4","title":"FEC corruption/erasure injection tests","description":"## Context\nWe need proof that FEC actually repairs realistic damage.\n\n## Goal\nCreate tests that:\n- intentionally drop/corrupt a bounded number of WAL blocks/frames\n- verify recovery succeeds when redundancy threshold is met\n- verify recovery fails cleanly when it is not met\n\n## Acceptance Criteria\n- Tests simulate both erasure and corruption.\n- Tests cover boundary conditions at the decoding threshold.\n- Tests run deterministically (seeded).","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:57:16.382833130Z","created_by":"ubuntu","updated_at":"2026-02-07T05:04:43.083052524Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["raptorq","tests"],"dependencies":[{"issue_id":"bd-29h.4","depends_on_id":"bd-29h","type":"parent-child","created_at":"2026-02-07T04:57:16.382833130Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29h.4","depends_on_id":"bd-29h.3","type":"blocks","created_at":"2026-02-07T05:04:43.083004203Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29h.5","title":"Fountain replication prototype (WAL stream over lossy link)","description":"## Context\nRaptorQ shines over lossy networks. This can enable lightweight read replicas and backup shipping.\n\n## Goal\nPrototype a replicator that:\n- streams commit capsules/WAL frames as RaptorQ symbols\n- supports late joiners\n- converges on the same state\n\n## Acceptance Criteria\n- Demo can converge a replica over an intentionally lossy channel.\n- Replica correctness is validated via conformance queries.\n- Implementation uses asupersync networking/IO primitives.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:57:16.472974484Z","created_by":"ubuntu","updated_at":"2026-02-07T05:04:43.418372442Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["asupersync","raptorq","replication"],"dependencies":[{"issue_id":"bd-29h.5","depends_on_id":"bd-29h","type":"parent-child","created_at":"2026-02-07T04:57:16.472974484Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29h.5","depends_on_id":"bd-29h.1","type":"blocks","created_at":"2026-02-07T05:04:43.418334071Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29h.6","title":"Commit-as-codeword: FEC-backed commit records","description":"## Context\nStandard durability requires fsyncing a complete set of frames. If we treat a commit record as a codeword, we can trade extra parity for tolerance to partial persistence (crash erasures) and potentially reduce serialization.\n\n## Goal\nPrototype a commit encoding scheme:\n- source blocks = transaction frame group (or page-diff bundle)\n- generate RaptorQ symbols with enough redundancy to recover under the crash+erasure model\n- persist symbols + a minimal commit marker\n\n## Acceptance Criteria\n- A concrete parameterization exists (symbol size, redundancy factor) with a justification from the erasure model.\n- Recovery can decode and apply commits when up to X blocks are missing.\n- When decoding fails, engine fails closed (no partial apply).","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-07T05:04:35.877900904Z","created_by":"ubuntu","updated_at":"2026-02-07T05:04:44.179894754Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["durability","raptorq"],"dependencies":[{"issue_id":"bd-29h.6","depends_on_id":"bd-29h","type":"parent-child","created_at":"2026-02-07T05:04:35.877900904Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29h.6","depends_on_id":"bd-29h.1","type":"blocks","created_at":"2026-02-07T05:04:43.881811151Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29h.6","depends_on_id":"bd-29h.2","type":"blocks","created_at":"2026-02-07T05:04:44.179846002Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29h.7","title":"Multi-writer capsule log prototype (RaptorQ commit capsules)","description":"## Context\nEven with MVCC, a sequential WAL append can become a bottleneck at very high writer counts. A capsule log explores a more parallel durability pipeline.\n\n## Goal\nPrototype a \"commit capsule\" design:\n- each transaction writes its own capsule (page-diff bundle) encoded with RaptorQ\n- capsules are content-addressed and append-only\n- a deterministic merge/checkpoint compacts capsules into the canonical DB/WAL format\n\n## Acceptance Criteria\n- Capsule format is documented (versioned) and validated by tests.\n- Deterministic merge order is defined (TxnId ordering) and tested.\n- Prototype demonstrates higher writer throughput under low conflict workloads.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-07T05:04:35.970373847Z","created_by":"ubuntu","updated_at":"2026-02-07T05:04:44.494672921Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["concurrency","raptorq"],"dependencies":[{"issue_id":"bd-29h.7","depends_on_id":"bd-29h","type":"parent-child","created_at":"2026-02-07T05:04:35.970373847Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29h.7","depends_on_id":"bd-29h.1","type":"blocks","created_at":"2026-02-07T05:04:44.350899785Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29h.7","depends_on_id":"bd-ya9","type":"blocks","created_at":"2026-02-07T05:04:44.494624981Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-29h.8","title":"RaptorQ-coded incremental backup/snapshot shipping","description":"## Context\nBackups and replicas often suffer from packet loss and partial transfers. Fountain coding makes snapshot delivery robust and resumable.\n\n## Goal\nImplement an incremental backup/snapshot shipping path:\n- encode pages or WAL ranges as RaptorQ symbols\n- receiver decodes once threshold reached\n- supports resume without re-sending identical data\n\n## Acceptance Criteria\n- Backup can be streamed over an intentionally lossy channel and complete successfully.\n- Restored snapshot passes basic conformance queries.","status":"open","priority":3,"issue_type":"task","created_at":"2026-02-07T05:04:36.064412844Z","created_by":"ubuntu","updated_at":"2026-02-07T05:04:44.577802250Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["asupersync","backup","raptorq"],"dependencies":[{"issue_id":"bd-29h.8","depends_on_id":"bd-29h","type":"parent-child","created_at":"2026-02-07T05:04:36.064412844Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-29h.8","depends_on_id":"bd-29h.5","type":"blocks","created_at":"2026-02-07T05:04:44.577764139Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2b9","title":"Write proptest MVCC invariant tests","description":"## Context\nConcurrency bugs hide in interleavings. Property tests are how we make \"alien artifact\" guarantees real.\n\n## Goal\nWrite proptest-based MVCC invariant tests:\n- visibility invariants\n- conflict detection invariants\n- deadlock freedom-by-construction (no wait cycles)\n\n## Acceptance Criteria\n- At least 3 core invariants from `bd-w2k` are encoded as property tests.\n- Tests run fast enough to include in normal `cargo test`.\n- Failures produce minimal shrinking counterexamples.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:01:17.815577336Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2b9","depends_on_id":"bd-1it","type":"parent-child","created_at":"2026-02-07T04:54:55.017814540Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2b9","depends_on_id":"bd-ya9","type":"blocks","created_at":"2026-02-07T04:42:37.604174945Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2e1","title":"Implement MVCC garbage collector","description":"## Context\nPage-level MVCC creates version chains. Without GC, memory grows without bound.\n\n## Goal\nImplement MVCC garbage collection:\n- reclaimability predicate based on active snapshots\n- safe reclamation of old versions\n- bounded chain length strategy (policy)\n\n## Acceptance Criteria\n- GC never reclaims a version that could still be visible to any active snapshot.\n- Stress test: sustained writes do not cause unbounded memory growth.\n- Microbench quantifies GC cost and chain-length distribution.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:01:17.726758188Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2e1","depends_on_id":"bd-1it","type":"parent-child","created_at":"2026-02-07T04:54:55.017104700Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2e1","depends_on_id":"bd-rmd","type":"blocks","created_at":"2026-02-07T04:42:37.370798053Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2e1","depends_on_id":"bd-ya9","type":"blocks","created_at":"2026-02-07T04:42:37.448518990Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2fc","title":"Define AST node types for SQL statements","description":"## Context\nAST types are the typed contract between parser and planner.\n\n## Goal\nDefine AST node types for SQLite SQL statements and expressions:\n- statements (SELECT/INSERT/UPDATE/DELETE/CREATE/etc.)\n- expressions (operators, functions, literals)\n- identifiers and qualified names\n\n## Acceptance Criteria\n- AST covers the full SQLite 3.52.0 grammar surface we target.\n- Unit tests validate construction and basic traversal/visitation.\n- Parser tasks reference these types (no ad-hoc parse trees).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:23.590210839Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2fc","depends_on_id":"bd-3so","type":"parent-child","created_at":"2026-02-07T04:54:55.270739673Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2gj","title":"[EPIC] Extensions (FTS, R-Tree, JSON1, Session, ICU, misc)","description":"## Context\nSQLite ships key extensions that users rely on. To claim parity, we must implement them (or explicitly scope them out with a hard rationale). The plan currently assumes they are compiled-in, feature-gated.\n\n## Goal\nImplement the major SQLite extensions with conformance and performance coverage.\n\n## Non-Negotiables\n- Extensions are tested via the same harness (oracle comparison) as core.\n- Feature flags must be explicit and documented.\n\n## Deliverables\n- JSON1\n- FTS3/4/5\n- R-Tree + geopoly\n- Session/changeset\n- ICU collation\n- Misc (generate_series, dbstat, dbpage, etc.)\n\n## Success Criteria\n- Extension-specific test corpus passes against oracle where applicable.","status":"open","priority":2,"issue_type":"epic","created_at":"2026-02-07T04:53:42.546921315Z","created_by":"ubuntu","updated_at":"2026-02-07T04:53:42.546921315Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["extensions"]}
{"id":"bd-2t4","title":"Implement full transaction semantics","description":"## Context\nTransaction semantics are user-visible: BEGIN/COMMIT/ROLLBACK, savepoints, isolation rules.\n\n## Goal\nImplement full transaction semantics:\n- BEGIN variants\n- COMMIT/ROLLBACK\n- savepoints\n- integration with MVCC and WAL\n\n## Acceptance Criteria\n- Transaction behavior matches oracle for core cases.\n- Conformance covers savepoint edge cases.\n- Busy/conflict behavior is consistent with the declared isolation model.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:03:18.353061212Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2t4","depends_on_id":"bd-221","type":"parent-child","created_at":"2026-02-07T04:54:55.523387646Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2t4","depends_on_id":"bd-3rt","type":"blocks","created_at":"2026-02-07T04:43:03.605449239Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2t5","title":"Implement foreign key enforcement","description":"## Context\nForeign keys are a major correctness feature and interact with transactions and triggers.\n\n## Goal\nImplement foreign key enforcement (immediate + deferred) with SQLite semantics.\n\n## Acceptance Criteria\n- Conformance tests for FK enforcement pass.\n- Deferred constraints behave correctly across transactions/savepoints.\n- Error codes/messages are normalized and match oracle.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:02:48.343316009Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2t5","depends_on_id":"bd-1ry","type":"blocks","created_at":"2026-02-07T04:43:11.670175662Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2t5","depends_on_id":"bd-3p7","type":"parent-child","created_at":"2026-02-07T04:54:55.355259577Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2vt","title":"Implement code generation for SQL statements","description":"## Context\nPlanner output must be turned into executable bytecode. This boundary is where many semantic bugs appear.\n\n## Goal\nImplement code generation from plans/AST into VDBE bytecode:\n- emit opcodes for scans, filters, projections\n- emit control flow for joins and subqueries (as phased)\n\n## Acceptance Criteria\n- Generated bytecode can be dumped and compared against oracle in simple cases.\n- End-to-end queries run through prepare -> step -> rows.\n- Conformance smoke tests pass for supported query subset.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:03:02.593747484Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2vt","depends_on_id":"bd-1j9","type":"blocks","created_at":"2026-02-07T04:42:54.115178583Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2vt","depends_on_id":"bd-3t3","type":"parent-child","created_at":"2026-02-07T04:54:55.439674354Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2vt","depends_on_id":"bd-xza","type":"blocks","created_at":"2026-02-07T04:42:54.194523712Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2x0","title":"Implement Bloom filter for MVCC version store","description":"## Context\nIf most pages have no MVCC versions, we want a fast path that avoids version-store lookup overhead.\n\n## Goal\nAdd a Bloom filter (or similar membership prefilter) to quickly decide:\n- this page definitely has no MVCC versions (skip)\n- this page might have versions (lookup)\n\n## Acceptance Criteria\n- Prefilter has a measured false positive rate and bounded memory cost.\n- Hot path avoids allocations.\n- Microbench demonstrates benefit on read-mostly workloads.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:01:17.990799888Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-2x0","depends_on_id":"bd-1it","type":"parent-child","created_at":"2026-02-07T04:54:55.019268082Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x0","depends_on_id":"bd-35w","type":"blocks","created_at":"2026-02-07T04:42:36.968978688Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-2x0","depends_on_id":"bd-rmd","type":"blocks","created_at":"2026-02-07T04:59:24.811452272Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-2yx","title":"Implement crate-level Cargo feature flags for extensions + RaptorQ integration (spec 8.5)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T06:58:47.463051536Z","created_by":"ubuntu","updated_at":"2026-02-07T06:58:47.463051536Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-35w","title":"Implement MVCC core types in fsqlite-mvcc","description":"## Context\nEverything above the storage stack depends on correct MVCC primitives. We also need types strong enough to prevent whole classes of bugs.\n\n## Goal\nImplement the MVCC core types and bookkeeping in `fsqlite-mvcc`:\n- TxnId allocation\n- Snapshot representation\n- PageVersion metadata representation (without committing to a final storage layout prematurely)\n\n## Acceptance Criteria\n- Types are documented and derived from `bd-w2k` (formal spec).\n- Unit tests cover snapshot visibility edge cases (in-flight, committed, boundary ids).\n- Public APIs are minimal and do not bake in single-writer assumptions.","status":"in_progress","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:01:17.378954352Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-35w","depends_on_id":"bd-1it","type":"parent-child","created_at":"2026-02-07T04:54:55.014027004Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-35w","depends_on_id":"bd-w2k","type":"blocks","created_at":"2026-02-07T04:42:36.653442647Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-372","title":"Implement window functions","description":"## Context\nWindow functions are required for modern SQL compatibility and have tricky semantics (frames, partitions).\n\n## Goal\nImplement SQLite window functions (built-ins + execution semantics) with correct behavior.\n\n## Acceptance Criteria\n- Conformance tests for window functions pass against oracle.\n- Edge cases covered: empty partitions, NULL handling, frame boundaries.\n- Performance: basic benchmark for common window patterns exists.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:02:48.063604404Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-372","depends_on_id":"bd-1ry","type":"blocks","created_at":"2026-02-07T04:43:11.431944859Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-372","depends_on_id":"bd-3p7","type":"parent-child","created_at":"2026-02-07T04:54:55.351845200Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-38t","title":"Implement WAL checkpoint","description":"## Context\nCheckpointing moves committed data from WAL into the main database file and bounds WAL growth.\n\n## Goal\nImplement WAL checkpoint:\n- select frames to apply\n- write pages to DB file\n- truncate/reset WAL safely\n\n## Acceptance Criteria\n- After checkpoint, DB file reflects all committed transactions up to a point.\n- WAL size is bounded over time under sustained writes.\n- Crash tests cover checkpoint interruptions.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:01:38.312963321Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-38t","depends_on_id":"bd-1cj","type":"parent-child","created_at":"2026-02-07T04:54:55.099350684Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-38t","depends_on_id":"bd-3l0","type":"blocks","created_at":"2026-02-07T04:43:03.205531987Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-397","title":"Implement FTS3/FTS4 extension","description":"## Context\nFTS3/4 are legacy but still used.\n\n## Goal\nImplement FTS3/4 with SQLite semantics.\n\n## Acceptance Criteria\n- FTS3/4 tests pass against oracle.\n- Back-compat behaviors required by tests are implemented.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:03:48.344076897Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-397","depends_on_id":"bd-2gj","type":"parent-child","created_at":"2026-02-07T04:54:55.760805970Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-397","depends_on_id":"bd-9n8","type":"blocks","created_at":"2026-02-07T04:43:20.499349401Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3av","title":"[EPIC] B-tree storage engine","description":"## Context\nSQLite’s on-disk format and performance are dominated by the B-tree layer. We must implement from the extracted spec (page formats, cell formats, invariants), not from C function names.\n\n## Goal\nImplement a spec-driven B-tree engine compatible with SQLite 3.52.0 file format:\n- table btrees (intkey)\n- index btrees (blobkey)\n- cursor traversal\n- insert/delete balancing\n- overflow chains\n- freelist management\n\n## Non-Negotiables\n- Implementation is derived from invariants + spec, not line-by-line C translation.\n- Property tests validate invariants over random operation sequences.\n\n## Deliverables\n- Page/cell parsing + serialization\n- Cursor APIs\n- Insert + split + balance\n- Delete + merge\n- Overflow + freelist\n\n## Success Criteria\n- Round-trip: pages written by us are readable by C SQLite and vice versa.\n- B-tree invariants hold (ordering, fanout bounds, correct overflow) under proptest.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-07T04:53:13.850991386Z","created_by":"ubuntu","updated_at":"2026-02-07T04:53:13.850991386Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["btree","storage"]}
{"id":"bd-3cw","title":"Implement R-Tree/Geopoly extension","description":"## Context\nR-Tree + geopoly provide spatial indexing features.\n\n## Goal\nImplement R-Tree/Geopoly extension.\n\n## Acceptance Criteria\n- Extension tests pass against oracle.\n- Query correctness matches oracle for spatial constraints.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:03:48.439789489Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3cw","depends_on_id":"bd-2gj","type":"parent-child","created_at":"2026-02-07T04:54:55.761554152Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3cw","depends_on_id":"bd-qvw","type":"blocks","created_at":"2026-02-07T04:43:20.178795923Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3dp","title":"Implement connection and prepare pipeline","description":"## Context\nTo be a real SQLite replacement, we need the prepare/step/finalize pipeline to work end-to-end.\n\n## Goal\nImplement the engine pipeline wiring:\n- Connection open/close\n- prepare: SQL -> AST -> plan -> bytecode\n- step: execute bytecode and return rows\n- finalize/reset\n\n## Acceptance Criteria\n- Basic lifecycle works: open -> execute -> query -> close.\n- Errors are mapped to SQLite-like error codes.\n- Harness can run at least a smoke corpus through the public API.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:03:18.172245522Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3dp","depends_on_id":"bd-221","type":"parent-child","created_at":"2026-02-07T04:54:55.521064665Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3dp","depends_on_id":"bd-2vt","type":"blocks","created_at":"2026-02-07T04:42:54.276908504Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3gk","title":"[EPIC] Perf baselines + profiling discipline (extreme optimization)","description":"## Context\\nWe are explicitly trying to beat SQLite, especially for concurrent writers. If we postpone baselines/benchmarks, we will accidentally ship regressions, and we will optimize the wrong things. Extreme-optimization rules apply: baseline first, profile, prove behavior unchanged, one lever at a time.\\n\\n## Goal\\nInstall performance measurement as infrastructure, not an afterthought. Every core hot path gets a criterion microbench plus a realistic macrobench, with baselines captured early and tracked over time.\\n\\n## Deliverables\\n- Criterion benches for: page fetch path, MVCC visibility scan, version-chain insertion, conflict detection, WAL append, checkpoint, btree insert/search/delete, vdbe dispatch loop\\n- Macrobenches: ycsb-like mixes, write-skew/conflict microbenches, read-mostly + scan workloads, multi-writer scaling curves\\n- Profiling playbook + scripts: flamegraph targets, allocation profiling, syscall profiling\\n- Golden-output/isomorphism protocol: perf changes must keep conformance green\\n- Budget docs: p50/p95 latency and throughput budgets per subsystem\\n\\n## Non-Negotiables\\n- Benchmarks start before MVCC lands (otherwise we cannot quantify overhead)\\n- Performance tests must separate: CPU vs IO vs contention\\n\\n## Success Criteria\\n- We can answer: \"What is the p95 page-read latency under snapshot visibility with K versions?\"\\n- We can reproduce a multi-writer throughput graph locally (N writers vs ops/sec).\\n- Perf regressions are detectable automatically (at least locally in CI-like runs).\\n","acceptance_criteria":"## Success Criteria\n- We have reproducible baseline numbers for the core hot paths (pager read/write, WAL append, MVCC visibility, btree ops, vdbe dispatch).\n- We can profile the system (CPU + allocs + syscalls) and identify top-5 hotspots.\n- Optimization changes include an isomorphism proof: conformance green + golden outputs unchanged.\n- We can plot multi-writer scaling curves locally (N writers vs ops/sec).","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-07T04:50:55.629051440Z","created_by":"ubuntu","updated_at":"2026-02-07T04:51:45.804797794Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["bench","perf","profiling"]}
{"id":"bd-3gk.1","title":"Define perf baselines + budgets (p50/p95, throughput, memory)","description":"## Context\nWithout baselines and budgets, optimization work degenerates into vibes and regressions slip in unnoticed.\n\n## Goal\nDefine the initial performance model and budgets:\n- baseline numbers for key ops (page read, btree insert, vdbe dispatch)\n- multi-writer scaling targets\n- memory growth budgets (MVCC version chains, cache)\n\n## Acceptance Criteria\n- A short doc (in-repo) lists the baseline commands and the first recorded numbers.\n- Each budget maps to a specific benchmark in the suite.\n- Budgets include both uncontended and contended (multi-writer) cases.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:56:57.972010564Z","created_by":"ubuntu","updated_at":"2026-02-07T05:00:36.793641441Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["budgets","perf"],"dependencies":[{"issue_id":"bd-3gk.1","depends_on_id":"bd-3gk","type":"parent-child","created_at":"2026-02-07T04:56:57.972010564Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3gk.2","title":"Profiling playbook (flamegraph, allocs, syscalls)","description":"## Context\nExtreme optimization requires fast iteration: profile, change one lever, re-measure.\n\n## Goal\nDocument and automate profiling runs:\n- CPU flamegraphs for key benches\n- allocation profiling for hot paths\n- syscall profiling for VFS/WAL paths\n\n## Acceptance Criteria\n- There is a reproducible set of commands to generate profiles for at least 3 hot paths.\n- The playbook includes how to interpret results and how to validate isomorphism (conformance green).","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:56:58.085883105Z","created_by":"ubuntu","updated_at":"2026-02-07T05:00:36.879044519Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["perf","profiling"],"dependencies":[{"issue_id":"bd-3gk.2","depends_on_id":"bd-3gk","type":"parent-child","created_at":"2026-02-07T04:56:58.085883105Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3gk.3","title":"Multi-writer scaling macrobench","description":"## Context\nThe main promise is better concurrent write throughput. We need an explicit benchmark that measures it.\n\n## Goal\nCreate a macrobench that:\n- runs N writer threads (and later, optionally multi-process)\n- controls conflict rate via workload design\n- reports ops/sec and tail latencies\n\n## Acceptance Criteria\n- Benchmark reports throughput vs N writers (at least N=1..8).\n- Benchmark has a parameter that changes conflict rate (hot pages vs wide writes).\n- Results are reproducible (seeded data generation).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:56:58.191169932Z","created_by":"ubuntu","updated_at":"2026-02-07T05:00:36.969412278Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["concurrency","mvcc","perf"],"dependencies":[{"issue_id":"bd-3gk.3","depends_on_id":"bd-3gk","type":"parent-child","created_at":"2026-02-07T04:56:58.191169932Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3j1","title":"Implement triggers","description":"## Context\nTriggers affect correctness of DML and have complex recursion/ordering rules.\n\n## Goal\nImplement triggers (BEFORE/AFTER, row-level) with SQLite semantics.\n\n## Acceptance Criteria\n- Conformance tests for triggers pass.\n- Recursion and trigger program ordering match oracle.\n- Error behavior (e.g., recursive trigger limits) is validated.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:02:48.246943210Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3j1","depends_on_id":"bd-1ry","type":"blocks","created_at":"2026-02-07T04:43:11.589894139Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3j1","depends_on_id":"bd-3p7","type":"parent-child","created_at":"2026-02-07T04:54:55.354211683Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3j6","title":"Implement CLI shell with frankentui","description":"## Context\nWe want an interactive shell like sqlite3, but rendered via /dp/frankentui.\n\n## Goal\nImplement the CLI shell:\n- REPL loop\n- dot-commands subset\n- output modes\n- rendering via frankentui\n\n## Acceptance Criteria\n- Shell can open a DB and run basic SQL.\n- Dot-command smoke set works (.tables, .schema, .mode, .headers, .quit at minimum).\n- No alternate terminal rendering crate is introduced.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:03:34.314359915Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3j6","depends_on_id":"bd-1a5","type":"parent-child","created_at":"2026-02-07T04:54:55.680898988Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3j6","depends_on_id":"bd-1a5.1","type":"blocks","created_at":"2026-02-07T04:59:13.759923666Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3j6","depends_on_id":"bd-qvw","type":"blocks","created_at":"2026-02-07T04:43:20.579435818Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3l0","title":"Implement WAL index","description":"## Context\nEfficient reads require an index from (page number) to the latest committed frame location.\n\n## Goal\nImplement the WAL index:\n- lookup for latest committed frame for a page\n- efficient append/update path\n- correctness under concurrent writers (with defined serialization points)\n\n## Acceptance Criteria\n- Lookup returns the same result as scanning the WAL sequentially (tested).\n- Index rebuild on startup works.\n- Stress test: concurrent commits do not corrupt the index.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:01:38.225178290Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3l0","depends_on_id":"bd-1cj","type":"parent-child","created_at":"2026-02-07T04:54:55.098639532Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3l0","depends_on_id":"bd-1d2","type":"blocks","created_at":"2026-02-07T04:43:03.127730088Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3lb","title":"Spec: make index segment tips/bootstrap explicit (ManifestSegment/ObjectLocatorSegment discovery)","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T17:33:58.537837668Z","created_by":"ubuntu","updated_at":"2026-02-07T17:33:58.537837668Z","source_repo":".","compaction_level":0,"original_size":0}
{"id":"bd-3p7","title":"[EPIC] Advanced SQL features (CTE, triggers, FK, window, DDL)","description":"## Context\nFeature parity is not just parsing; it’s semantic execution. SQLite’s higher-level SQL features touch parser, planner, VDBE, schema, and storage.\n\n## Goal\nImplement advanced SQL features with conformance coverage:\n- CTEs\n- window functions\n- triggers\n- foreign keys\n- views\n- DDL maintenance commands (ALTER TABLE, VACUUM, REINDEX)\n\n## Non-Negotiables\n- Each feature lands with conformance fixtures.\n- Behavior matches C SQLite 3.52.0 for the selected surface.\n\n## Success Criteria\n- SQLLogicTest and targeted SQLite test cases for these features pass.\n- Error codes and edge cases (cycles, recursion, deferred FK, trigger recursion) are validated.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-07T04:54:24.590276280Z","created_by":"ubuntu","updated_at":"2026-02-07T04:54:24.590276280Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["features","sql"]}
{"id":"bd-3rt","title":"Implement crash recovery","description":"## Context\nRecovery is the final arbiter of durability: after a crash, the DB must be consistent and include exactly the committed transactions.\n\n## Goal\nImplement crash recovery:\n- detect WAL/journal state\n- replay committed changes\n- repair/ignore partial writes according to crash model\n\n## Acceptance Criteria\n- Kill/crash simulation tests pass (various interruption points).\n- Recovery is deterministic and idempotent.\n- Recovered DB opens and passes sanity checks.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:01:38.403211466Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3rt","depends_on_id":"bd-1cj","type":"parent-child","created_at":"2026-02-07T04:54:55.100183935Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3rt","depends_on_id":"bd-38t","type":"blocks","created_at":"2026-02-07T04:43:03.287175290Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3sl","title":"Implement B-tree page format parsing from spec","description":"## Context\nB-tree page parsing/serialization must match SQLite’s on-disk format exactly for file compatibility.\n\n## Goal\nImplement B-tree page header + cell pointer array parsing/serialization from the spec:\n- leaf/interior pages\n- table vs index page types\n- freeblock handling\n\n## Acceptance Criteria\n- Round-trip tests: parse then serialize yields identical bytes for valid pages.\n- Pages produced by us are accepted by oracle (smoke conformance).\n- Parsing rejects invalid pages with correct error classification.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:03.446946485Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3sl","depends_on_id":"bd-3av","type":"parent-child","created_at":"2026-02-07T04:54:55.180650967Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3sl","depends_on_id":"bd-ya9","type":"blocks","created_at":"2026-02-07T04:42:46.511584876Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3so","title":"[EPIC] SQL frontend (lexer/parser/AST/planner)","description":"## Context\nThe SQL layer defines external behavior. Parser and planner must match SQLite’s accepted syntax, error messages/codes (within reason), and planning heuristics where behavior is observable.\n\n## Goal\nImplement lexer/parser/AST and a planner sufficient to compile SQLite SQL into VDBE bytecode with matching semantics.\n\n## Non-Negotiables\n- Parser coverage is not \"95%\"; target is complete grammar coverage for SQLite 3.52.0 surface.\n- Error behavior is conformance-tested (not just happy path).\n\n## Deliverables\n- Tokenizer/lexer\n- Recursive descent parser (with Pratt for expressions)\n- AST types and visitors\n- Name resolution + basic planning\n- WHERE and join optimizations as required by conformance\n\n## Success Criteria\n- SQLLogicTest parse corpus runs clean (or explicit documented exceptions).\n- Planner choices that affect results (e.g. row order without ORDER BY is undefined) are validated via conformance harness.","status":"open","priority":1,"issue_type":"epic","created_at":"2026-02-07T04:53:21.529032656Z","created_by":"ubuntu","updated_at":"2026-02-07T04:53:21.529032656Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["parser","planner","sql"]}
{"id":"bd-3t3","title":"[EPIC] VDBE bytecode engine + execution","description":"## Context\nSQLite’s observable semantics are largely implemented in the VDBE. To achieve true parity, we must implement the opcode set and memory/register model precisely enough to satisfy the conformance corpus.\n\n## Goal\nImplement the VDBE:\n- register file + Mem representation\n- opcode dispatch loop\n- opcode semantics (including edge cases)\n- statement lifecycle (prepare/step/finalize)\n\n## Non-Negotiables\n- Spec-driven: opcodes are implemented from extracted semantics + conformance, not by translating C control flow.\n- Deterministic behavior where SQLite is deterministic.\n\n## Deliverables\n- Mem/register layer\n- Core opcode set\n- Full opcode coverage for 3.52.0 surface\n- Bytecode emitter (from planner)\n\n## Success Criteria\n- We can execute non-trivial SQLLogicTest suites end-to-end.\n- Golden outputs match C SQLite for supported tests.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-07T04:53:28.968869940Z","created_by":"ubuntu","updated_at":"2026-02-07T04:53:28.968869940Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["execution","vdbe"]}
{"id":"bd-3t4","title":"Implement ALTER TABLE, VACUUM, REINDEX","description":"## Context\nALTER TABLE/VACUUM/REINDEX are important for schema evolution and database maintenance.\n\n## Goal\nImplement:\n- ALTER TABLE behaviors\n- VACUUM\n- REINDEX\n\n## Acceptance Criteria\n- Conformance tests for each command pass.\n- File-format invariants remain valid (especially VACUUM).\n- Performance: VACUUM completes on medium DB sizes without pathological blowups.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:02:48.436162112Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3t4","depends_on_id":"bd-1ry","type":"blocks","created_at":"2026-02-07T04:43:11.750124592Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3t4","depends_on_id":"bd-3p7","type":"parent-child","created_at":"2026-02-07T04:54:55.356316116Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3us","title":"Implement B-tree delete and page merging","description":"## Context\nDeletes trigger underflow and page merging, which is tricky and easy to get wrong.\n\n## Goal\nImplement delete + page merge/rebalance:\n- delete cell\n- rebalance/merge when below minimum\n- update parent pointers\n\n## Acceptance Criteria\n- After random insert/delete sequences, invariants still hold (proptest).\n- Free space is reclaimed correctly (freeblocks/freelist).\n- Conformance covers delete edge cases.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:03.801617365Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3us","depends_on_id":"bd-1me","type":"blocks","created_at":"2026-02-07T04:42:46.828627909Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3us","depends_on_id":"bd-3av","type":"parent-child","created_at":"2026-02-07T04:54:55.184968615Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3uu","title":"Implement views","description":"## Context\nViews are part of SQL schema functionality and must interact correctly with name resolution and triggers.\n\n## Goal\nImplement CREATE VIEW / DROP VIEW and view expansion semantics.\n\n## Acceptance Criteria\n- Conformance tests for views pass.\n- Name resolution and column mapping match oracle.\n- Errors for invalid view definitions match oracle (normalized).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:02:48.532691123Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3uu","depends_on_id":"bd-3j1","type":"blocks","created_at":"2026-02-07T04:43:11.830068221Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3uu","depends_on_id":"bd-3p7","type":"parent-child","created_at":"2026-02-07T04:54:55.357329094Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3v2","title":"Build comprehensive performance benchmark suite","description":"## Context\nMicrobenches catch regressions in hot loops; macrobenches catch system-level contention and IO effects.\n\n## Goal\nBuild a comprehensive benchmark suite (micro + macro) for FrankenSQLite.\n\n## Acceptance Criteria\n- Suite includes: read-mostly, write-heavy, mixed, scan-heavy workloads.\n- Suite includes a multi-writer scaling benchmark.\n- Bench results can be generated locally in a single command.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:00:37.152264692Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-3v2","depends_on_id":"bd-12u","type":"blocks","created_at":"2026-02-07T04:43:20.895798397Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3v2","depends_on_id":"bd-3gk","type":"parent-child","created_at":"2026-02-07T04:54:54.931437847Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3v2","depends_on_id":"bd-zi6","type":"blocks","created_at":"2026-02-07T04:43:20.816616795Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3v7","title":"[EPIC] 100% parity + conformance harness (C sqlite oracle)","description":"## Context\\nThe current docs target \"95%+\" compatibility and defer conformance late. That creates two failure modes:\\n- We discover behavioral drift only after thousands of lines of implementation.\\n- We optimize/architect in the dark (no golden outputs to prove isomorphism).\\n\\n## Goal\\nMake conformance a first-class, always-on feedback loop. Every implemented feature must be proven against an oracle (C SQLite 3.52.0) with fixtures and reproducible golden outputs. Target is 100% feature/functionality coverage for the chosen SQLite version surface and matching API/behavior (except explicitly documented intentional divergences, which must be near-zero and justified).\\n\\n## Deliverables\\n- C SQLite reference build + runner integrated into fsqlite-harness\\n- Golden-output capture format + storage layout\\n- Continuous SQLLogicTest (SLT) ingestion + execution\\n- Feature-parity matrix + reporting (pass/fail/skip with reasons)\\n- Conformance gating for each subsystem as it lands (parser, btree, vdbe, wal, pragmas, extensions)\\n\\n## Non-Negotiables\\n- Deterministic test runs (seed control, stable ordering)\\n- Tests runnable locally without network\\n- No tokio; use asupersync for any async/IO plumbing\\n\\n## Success Criteria\\n- We can run a single command that executes the harness against both engines and produces a machine-readable report.\\n- New functionality lands with conformance tests in the same PR/commit.\\n- We can point at concrete fixtures for any behavioral claim.\\n","acceptance_criteria":"## Success Criteria\n- `fsqlite-harness` can run the oracle (C SQLite 3.52.0) and FrankenSQLite on the same test corpus and emit a machine-readable report.\n- SQLLogicTest (SLT) ingestion works and runs continuously.\n- Feature-parity reporting is versioned and reproducible.\n- Target is 100% for the selected surface; any skip must have an explicit issue and rationale.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-07T04:50:42.659156504Z","created_by":"ubuntu","updated_at":"2026-02-07T04:51:45.717368830Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["conformance","correctness"]}
{"id":"bd-3v7.1","title":"Build C SQLite 3.52.0 oracle runner","description":"## Context\nTo get deterministic, undeniable parity signals, we need an oracle: the C SQLite behavior for the exact target version (3.52.0).\n\n## Goal\nAdd an oracle runner that can execute tests/queries via C SQLite and capture:\n- result rows\n- error codes/messages\n- stdout/stderr where relevant\n\n## Acceptance Criteria\n- Oracle can be built from `legacy_sqlite_code` (or an equivalent vendored tree) without network.\n- Harness can run a single SQL script against oracle and capture structured output.\n- Output capture includes: rows, column types, error code, and a normalized error message.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:56:46.441948044Z","created_by":"ubuntu","updated_at":"2026-02-07T05:00:09.211120277Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["conformance","oracle"],"dependencies":[{"issue_id":"bd-3v7.1","depends_on_id":"bd-3v7","type":"parent-child","created_at":"2026-02-07T04:56:46.441948044Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3v7.2","title":"Implement golden output format + diffing","description":"## Context\nGolden outputs are the proof layer for both correctness and optimization isomorphism.\n\n## Goal\nDefine a stable fixture format for:\n- SQL input\n- output rows\n- SQLite result codes\n- stderr/stdout\n- nondeterminism controls (seed, pragmas)\n\nAnd implement diff tooling that highlights behavioral differences.\n\n## Acceptance Criteria\n- Fixture schema is versioned and documented.\n- Golden outputs are deterministic (stable ordering where defined; explicit nondeterminism policy).\n- Diff output is actionable (shows first mismatch with context).","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:56:46.528363118Z","created_by":"ubuntu","updated_at":"2026-02-07T05:00:09.299752454Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["conformance","fixtures"],"dependencies":[{"issue_id":"bd-3v7.2","depends_on_id":"bd-3v7","type":"parent-child","created_at":"2026-02-07T04:56:46.528363118Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3v7.3","title":"Integrate SQLLogicTest (SLT) corpus","description":"## Context\nSQLLogicTest provides wide SQL coverage and is a standard comparison corpus.\n\n## Goal\nIngest SLT tests into `fsqlite-harness` and run them against both engines with reproducible reporting.\n\n## Acceptance Criteria\n- SLT files are discoverable under `conformance/` (vendored or pinned submodule).\n- Harness can execute a subset (smoke) and the full suite.\n- Failures are reported with test id, SQL, and diff.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:56:46.618100667Z","created_by":"ubuntu","updated_at":"2026-02-07T05:00:16.295365118Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["conformance","slt"],"dependencies":[{"issue_id":"bd-3v7.3","depends_on_id":"bd-26i","type":"blocks","created_at":"2026-02-07T05:00:16.123796984Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3v7.3","depends_on_id":"bd-3v7","type":"parent-child","created_at":"2026-02-07T04:56:46.618100667Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3v7.3","depends_on_id":"bd-3v7.1","type":"blocks","created_at":"2026-02-07T05:00:16.211634312Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-3v7.3","depends_on_id":"bd-3v7.2","type":"blocks","created_at":"2026-02-07T05:00:16.295322749Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-3v7.4","title":"Generate feature-parity matrix + reporting","description":"## Context\nWe need a single source of truth for what is implemented, what is failing, and why.\n\n## Goal\nProduce machine-readable reports (JSON) and a human summary that includes:\n- pass/fail/skip per test\n- failure classification\n- links to owning issues\n\n## Acceptance Criteria\n- Report format is stable and versioned.\n- Skips require an explicit issue reference and rationale.\n- Summary includes top regressions and trend over time (at least locally).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:56:46.705643343Z","created_by":"ubuntu","updated_at":"2026-02-07T05:00:09.479072991Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["conformance","reporting"],"dependencies":[{"issue_id":"bd-3v7.4","depends_on_id":"bd-3v7","type":"parent-child","created_at":"2026-02-07T04:56:46.705643343Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-72t","title":"Implement Session/Changeset extension","description":"## Context\nSession/changeset enables change tracking and replication use cases.\n\n## Goal\nImplement Session/Changeset extension.\n\n## Acceptance Criteria\n- Changeset generation/application matches oracle on fixtures.\n- Conflicts are detected and reported correctly.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:03:48.530695798Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-72t","depends_on_id":"bd-2gj","type":"parent-child","created_at":"2026-02-07T04:54:55.762286554Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-72t","depends_on_id":"bd-qvw","type":"blocks","created_at":"2026-02-07T04:43:20.262116698Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-7vf","title":"Implement ARC cache eviction policy","description":"## Context\nPlain LRU is scan-hostile for database workloads. ARC is a strong adaptive policy candidate, but we should be measurement-driven.\n\n## Goal\nImplement ARC (Adaptive Replacement Cache) as a cache policy option and compare it against a simple baseline (e.g., LRU) using criterion benches.\n\n## Acceptance Criteria\n- ARC is implemented behind a cache-policy trait (no pager rewrite).\n- Bench suite includes a scan-heavy workload demonstrating ARC advantage (if present).\n- Correctness: eviction never drops a dirty, uncheckpointed version.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:01:17.904356199Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-7vf","depends_on_id":"bd-1it","type":"parent-child","created_at":"2026-02-07T04:54:55.018565657Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-7vf","depends_on_id":"bd-ya9","type":"blocks","created_at":"2026-02-07T04:59:24.729583765Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-9n8","title":"Implement FTS5 extension","description":"## Context\nFTS5 is the modern full-text search extension.\n\n## Goal\nImplement FTS5 with core features needed for parity.\n\n## Acceptance Criteria\n- FTS5 test corpus (or subset) passes against oracle.\n- Basic performance benchmark exists (tokenization + query).","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:03:48.256906176Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-9n8","depends_on_id":"bd-2gj","type":"parent-child","created_at":"2026-02-07T04:54:55.760035867Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-9n8","depends_on_id":"bd-qvw","type":"blocks","created_at":"2026-02-07T04:43:20.096722414Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-bd5","title":"Implement SQL lexer/tokenizer","description":"## Context\nLexing defines token boundaries, keywords, and literal parsing. Parser correctness depends on it.\n\n## Goal\nImplement SQL lexer/tokenizer compatible with SQLite:\n- keywords vs identifiers\n- numeric, string, blob literals\n- comments and whitespace\n\n## Acceptance Criteria\n- Tokenization matches SQLite for tricky edge cases (quotes, escaped strings, unicode identifiers where relevant).\n- Lexer error cases are conformance-tested (invalid tokens).","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:13.356120220Z","updated_at":"2026-02-07T05:02:23.679379953Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-bd5","depends_on_id":"bd-3so","type":"parent-child","created_at":"2026-02-07T04:54:55.271769221Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-edd","title":"[EPIC] MVCC-first storage + plan reset","description":"## Context\nOur current plan stages MVCC as a late bolt-on (Phase 6). That creates a build-then-refactor trap: pager/WAL/cache APIs get shaped around SQLite's single-writer architecture, then rewritten.\n\n## Goal\nRestructure the port plan so MVCC is the native storage model from the beginning: page access APIs carry snapshot/txn context, the version store is the page cache, and commit coordination/WAL appends are designed around concurrent writers.\n\n## Non-Negotiables\n- Clean-room reimplementation: consult C SQLite only as behavioral spec\n- File format compatibility with SQLite 3.52.0 is preserved\n- No tokio: use /dp/asupersync for async + I/O, /dp/frankentui for CLI UI\n\n## Deliverables\n- Updated PLAN_TO_PORT_SQLITE_TO_RUST.md phases and acceptance gates\n- Updated PROPOSED_ARCHITECTURE.md storage APIs and invariants\n- Explicit statement of isolation level + multi-process scope (with rationale)\n\n## Success Criteria\n- MVCC shapes storage APIs from day one (no pager/WAL refactor later).\n- Docs stop promising \"95%+\" and instead target 100% for the selected surface, with explicit, rare, justified exceptions.\n- Concurrency semantics are stated precisely enough to test.","acceptance_criteria":"## Success Criteria\n- `PLAN_TO_PORT_SQLITE_TO_RUST.md` no longer defers MVCC to a late phase; MVCC shapes pager/WAL APIs from day one.\n- `PROPOSED_ARCHITECTURE.md` storage interfaces are snapshot/txn-aware (no later refactor required).\n- Docs explicitly declare:\n  - isolation level goal (and how it maps to SQLite semantics)\n  - concurrency scope (threads vs multi-process) with rationale\n- Any remaining intentional deviations from C SQLite are explicitly listed and justified.","status":"open","priority":0,"issue_type":"epic","created_at":"2026-02-07T04:43:38.523592010Z","created_by":"ubuntu","updated_at":"2026-02-07T04:52:06.380157229Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["architecture","mvcc","plan"]}
{"id":"bd-edd.1","title":"Rewrite plan phases to MVCC-first + early conformance/perf","description":"## Context\n`PLAN_TO_PORT_SQLITE_TO_RUST.md` currently defers MVCC to Phase 6 and defers serious conformance/perf gating. That creates a build-then-refactor trap and makes performance/correctness drift inevitable.\n\n## Goal\nRewrite the phases so that:\n- MVCC shapes pager/WAL APIs from Phase 2.\n- Conformance harness + perf baselines start immediately (Phase 1/2), not at the end.\n- Every phase has explicit gates that prevent drift.\n\n## Acceptance Criteria\n- The plan no longer contains a Phase dedicated to \"bolt MVCC on later\"; MVCC is foundational.\n- Benchmarks and conformance harness appear as early infrastructure, not as late polish.\n- Each phase has objective exit criteria (tests passing, harness coverage, perf baselines captured).","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:56:30.645649934Z","created_by":"ubuntu","updated_at":"2026-02-07T04:59:49.756141528Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["architecture","docs","plan"],"dependencies":[{"issue_id":"bd-edd.1","depends_on_id":"bd-edd","type":"parent-child","created_at":"2026-02-07T04:56:30.645649934Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-edd.2","title":"Align docs: 95%+ -> 100% parity target","description":"## Context\nREADME and plan docs repeatedly state \"95%+\" compatibility targets. Your stated goal is 100% feature/functionality coverage with matching API/behavior for the chosen SQLite surface.\n\n## Goal\nUpdate README/plan/architecture docs to:\n- target 100% parity for the chosen surface\n- replace \"95%\" language\n- require an explicit issue + rationale for any skip/deviation\n\n## Acceptance Criteria\n- No remaining \"95%\" targets in README/PLAN/ARCH docs.\n- Any explicit exclusions/intentional divergences are listed in one place with rationale.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:56:30.730426068Z","created_by":"ubuntu","updated_at":"2026-02-07T04:59:49.848670756Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["conformance","docs"],"dependencies":[{"issue_id":"bd-edd.2","depends_on_id":"bd-edd","type":"parent-child","created_at":"2026-02-07T04:56:30.730426068Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-edd.3","title":"Decide isolation level + multi-process scope (formal)","description":"## Context\nDocs say \"snapshot isolation + first-committer-wins\" and sometimes imply multi-process concurrency, but do not state the exact isolation contract or supported concurrency scope.\n\n## Goal\nWrite down precise, testable decisions:\n- Isolation: Snapshot Isolation (SI) vs Serializable Snapshot Isolation (SSI) vs strict 2PL variant\n- Scope: threads only (in-process) vs multi-process on the same DB file\n- Mapping to SQLite expectations (what changes, what stays identical)\n\n## Acceptance Criteria\n- The decision is documented as explicit definitions/predicates (visibility + conflict + commit rules).\n- There is a test plan for the decision (property tests + harness tests).\n- Any deviation from C SQLite concurrency semantics is explicitly described.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:56:30.814380011Z","created_by":"ubuntu","updated_at":"2026-02-07T04:59:49.941185197Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["architecture","correctness","mvcc"],"dependencies":[{"issue_id":"bd-edd.3","depends_on_id":"bd-edd","type":"parent-child","created_at":"2026-02-07T04:56:30.814380011Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-edd.4","title":"Update PROPOSED_ARCHITECTURE: MVCC-first pager API + cache policy hook","description":"## Context\n`PROPOSED_ARCHITECTURE.md` currently documents a versioned page cache keyed by (pgno, txn_id) with LRU, and the plan still contains C-function-name-driven design language.\n\n## Goal\nUpdate the architecture doc so the core storage traits:\n- make snapshot/txn context explicit (e.g. get_page(pgno, snapshot))\n- decouple cache policy from pager logic (policy-pluggable)\n- remove C-function-name-driven design language\n\n## Acceptance Criteria\n- Architecture doc shows snapshot/txn-aware storage API signatures.\n- Cache policy is described as a plug-in (LRU/ARC/TinyLFU are implementations, not assumptions).\n- B-tree section is framed in terms of invariants/spec, not C function names.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:56:30.894797340Z","created_by":"ubuntu","updated_at":"2026-02-07T04:59:50.027275963Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["architecture","docs","storage"],"dependencies":[{"issue_id":"bd-edd.4","depends_on_id":"bd-edd","type":"parent-child","created_at":"2026-02-07T04:56:30.894797340Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-phn","title":"Write-merge hardening: forbid raw byte-XOR merges on SQLite pages","description":"Implement spec v1.13 write-merge rules:\\n- No raw byte-disjoint XOR merge for SQLite structured pages (B-tree/overflow/freelist/pointer-map).\\n- Add PRAGMA fsqlite.write_merge = OFF|SAFE|LAB_UNSAFE (SAFE default for BEGIN CONCURRENT).\\n- Commit-time conflict resolution uses SAFE ladder: deterministic rebase (intent replay) then structured page patch merge keyed by cell_key_digest; abort/retry otherwise.\\n- LAB_UNSAFE may allow raw_xor_ranges only for explicitly-opaque engine pages; never for SQLite structured pages.\\n- Add regression test for B-tree lost-update counterexample: cell move/defrag vs update at old offset must never silently lose writes.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T17:44:39.367655263Z","created_by":"ubuntu","updated_at":"2026-02-07T17:44:39.367655263Z","source_repo":".","compaction_level":0,"original_size":0,"labels":["correctness","mvcc"]}
{"id":"bd-qh0","title":"Implement JSON1 extension","description":"## Context\nJSON1 is a widely used SQLite extension.\n\n## Goal\nImplement JSON1 functions with SQLite semantics.\n\n## Acceptance Criteria\n- JSON1 conformance tests pass against oracle.\n- Edge cases (invalid JSON, path errors, NULL handling) match oracle.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:03:48.163699959Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-qh0","depends_on_id":"bd-2gj","type":"parent-child","created_at":"2026-02-07T04:54:55.759080267Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-qh0","depends_on_id":"bd-qvw","type":"blocks","created_at":"2026-02-07T04:43:20.015267373Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-qvw","title":"Implement public API facade","description":"## Context\nWe need a user-facing crate with a stable API that matches SQLite expectations.\n\n## Goal\nImplement the public API facade (`fsqlite`):\n- Connection\n- Statement\n- execute/query convenience APIs\n- error mapping\n\n## Acceptance Criteria\n- Public API supports basic use cases without reaching into internal crates.\n- API behavior is tested via harness (not only unit tests).\n- Documentation states supported parity surface and feature flags.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:03:18.263107948Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-qvw","depends_on_id":"bd-221","type":"parent-child","created_at":"2026-02-07T04:54:55.521902835Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-qvw","depends_on_id":"bd-3dp","type":"blocks","created_at":"2026-02-07T04:42:54.354850154Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-rmd","title":"Implement visibility engine","description":"## Context\nVisibility is the heart of MVCC reads. If this is wrong, results are wrong.\n\n## Goal\nImplement the visibility engine:\n- visible(version, snapshot) predicate\n- selecting the newest visible version for a page\n- rules for TxnId::ZERO / base-file versions\n\n## Acceptance Criteria\n- Implementation matches the predicates in `bd-w2k`.\n- Unit tests cover: reading your own writes, seeing committed versions, not seeing in-flight versions.\n- Worst-case version scan cost is measured in a microbench (so we can bound it via GC).","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:01:17.466699339Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-rmd","depends_on_id":"bd-1it","type":"parent-child","created_at":"2026-02-07T04:54:55.014862570Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-rmd","depends_on_id":"bd-35w","type":"blocks","created_at":"2026-02-07T04:42:36.729921478Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-s4i","title":"Implement ICU collation extension","description":"## Context\nICU collation affects string comparison and ordering.\n\n## Goal\nImplement ICU-based collation support (as scoped).\n\n## Acceptance Criteria\n- Collation behavior matches oracle where ICU is available.\n- Feature flag behavior is documented.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:03:48.623321948Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-s4i","depends_on_id":"bd-2gj","type":"parent-child","created_at":"2026-02-07T04:54:55.763039624Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-s4i","depends_on_id":"bd-qvw","type":"blocks","created_at":"2026-02-07T04:43:20.341691988Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-tsx","title":"Implement Common Table Expressions (CTEs)","description":"## Context\nCTEs (WITH / WITH RECURSIVE) are core SQLite features and impact parsing, planning, and execution.\n\n## Goal\nImplement CTEs, including recursive CTE semantics.\n\n## Acceptance Criteria\n- Conformance tests for CTEs pass (including recursion).\n- Cycle/termination behavior matches oracle.\n- Explain/plan output shows CTE handling for debugging.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:24.652847747Z","updated_at":"2026-02-07T05:02:48.157154605Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-tsx","depends_on_id":"bd-1ry","type":"blocks","created_at":"2026-02-07T04:43:11.510715763Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-tsx","depends_on_id":"bd-3p7","type":"parent-child","created_at":"2026-02-07T04:54:55.353078731Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-w2k","title":"Write formal MVCC specification document","description":"## Context\nOur MVCC design must be precise and testable. Prose descriptions (even in PROPOSED_ARCHITECTURE) are not enough to prevent subtle correctness bugs.\n\n## Goal\nWrite a formal MVCC specification for FrankenSQLite that defines:\n- snapshots and visibility\n- read/write rules\n- conflict detection predicate\n- commit/abort semantics\n- garbage collection safety (reclaimability)\n\nThe spec should be written so every rule can be turned into a unit test or property test.\n\n## Acceptance Criteria\n- The spec contains explicit definitions (as predicates) for visibility and conflict.\n- At least one theorem/invariant is stated with a proof sketch (e.g. deadlock freedom-by-construction).\n- The spec includes a mapping section: how this compares to SQLite’s effective isolation under single-writer.\n- The proptest suite references the spec invariants.\n\n## References\n- MVCC spec draft: `MVCC_SPECIFICATION.md`","acceptance_criteria":"## Acceptance Criteria\n- The spec contains explicit definitions (as predicates) for visibility and conflict.\n- At least one theorem/invariant is stated with a proof sketch (e.g. deadlock freedom-by-construction).\n- The spec includes a mapping section: how this compares to SQLite’s effective isolation under single-writer.\n- The proptest suite references the spec invariants.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:06:18.078807588Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-w2k","depends_on_id":"bd-1it","type":"parent-child","created_at":"2026-02-07T04:57:25.334058939Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-xza","title":"Implement basic query planner","description":"## Context\nPlanner translates AST into an executable strategy and ultimately VDBE bytecode. Observable behavior can depend on planner decisions.\n\n## Goal\nImplement a basic query planner:\n- name resolution\n- WHERE clause analysis\n- join planning (initial)\n- index selection (initial)\n\n## Acceptance Criteria\n- Planner generates plans that pass conformance for supported queries.\n- Plan output can be dumped for debugging and snapshot-tested.","status":"open","priority":1,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:02:23.863928987Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-xza","depends_on_id":"bd-22f","type":"blocks","created_at":"2026-02-07T04:42:54.034401031Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-xza","depends_on_id":"bd-3so","type":"parent-child","created_at":"2026-02-07T04:54:55.273672166Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-ya9","title":"Implement MVCC-native pager","description":"## Context\nThe pager is where page I/O, caching, and MVCC meet. If we design it single-writer-first, we will refactor.\n\n## Goal\nImplement an MVCC-native pager interface:\n- snapshot-aware page reads\n- transaction-context-aware page writes (copy-on-write to create versions)\n- pluggable cache policy hook\n\n## Acceptance Criteria\n- Pager read API takes explicit snapshot/txn context (no hidden global state).\n- Writers create new page versions without blocking readers.\n- Disjoint-page concurrent writers can make progress.\n- Tests cover: read snapshot stability, write visibility rules, and conflict behavior.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:01:17.641256925Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-ya9","depends_on_id":"bd-1it","type":"parent-child","created_at":"2026-02-07T04:54:55.016383328Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ya9","depends_on_id":"bd-28o","type":"blocks","created_at":"2026-02-07T04:42:37.131190852Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-ya9","depends_on_id":"bd-rmd","type":"blocks","created_at":"2026-02-07T04:42:37.048754083Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-yfs","title":"Implement file format round-trip compatibility","description":"## Context\nFile format compatibility is a core promise: databases should round-trip between FrankenSQLite and C SQLite.\n\n## Goal\nImplement file format round-trip compatibility for:\n- database header\n- btree pages\n- WAL/journal as scoped\n\n## Acceptance Criteria\n- A DB created by FrankenSQLite can be opened by C SQLite and returns the same results.\n- A DB created by C SQLite can be opened by FrankenSQLite and returns the same results.\n- Conformance harness includes explicit round-trip tests.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:17.294442003Z","updated_at":"2026-02-07T05:03:18.444605796Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-yfs","depends_on_id":"bd-12u","type":"blocks","created_at":"2026-02-07T04:43:03.527099144Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-yfs","depends_on_id":"bd-221","type":"parent-child","created_at":"2026-02-07T04:54:55.522658911Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-yfs","depends_on_id":"bd-3rt","type":"blocks","created_at":"2026-02-07T04:43:03.447856417Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
{"id":"bd-zi6","title":"Set up criterion benchmark infrastructure","description":"## Context\nWe need criterion benches early so we can quantify MVCC overhead and avoid optimizing blindly.\n\n## Goal\nSet up criterion benchmarking infrastructure for the workspace:\n- standard harness crate layout\n- baseline benches for pager read path and MVCC visibility\n- CI-friendly bench subset (smoke)\n\n## Acceptance Criteria\n- `cargo bench` runs at least one microbench.\n- Bench targets include at least: pager get_page, visibility scan, WAL append (stub ok initially).\n- Bench output is stable enough to compare runs locally.","status":"open","priority":0,"issue_type":"task","created_at":"2026-02-07T04:42:09.188941773Z","updated_at":"2026-02-07T05:00:37.062664270Z","source_repo":".","compaction_level":0,"original_size":0,"dependencies":[{"issue_id":"bd-zi6","depends_on_id":"bd-3gk","type":"parent-child","created_at":"2026-02-07T04:54:54.930455487Z","created_by":"ubuntu","metadata":"{}","thread_id":""},{"issue_id":"bd-zi6","depends_on_id":"bd-ya9","type":"blocks","created_at":"2026-02-07T04:42:37.527533860Z","created_by":"ubuntu","metadata":"{}","thread_id":""}]}
